{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœˆï¸ Natural-Language â†’ SQL with Reinforcement-Fine-Tuning (RFT)\n",
    "A full, reproducible demo that trains a 3-B parameter model to answer English questions by writing SQL, without touching real production data.\n",
    "\n",
    "### 0. Preface â€“ Why this matters\n",
    "Large-language-model copilots are great, but even the best models don't know your private data, and can hallucinate a column that doesnâ€™t exist.\n",
    "Reinforcement-Fine-Tuning (RFT) fixes this by teaching your language model about your data and how to write accurate queries. In this tutorial you will:\n",
    "\n",
    "| You will learn to ğŸ“š                               | By the end youâ€™ll have ğŸ                          |\n",
    "| -------------------------------------------------- | -------------------------------------------------- |\n",
    "| âœ“ create a synthetic DB that *mimics* your schema  | `openflights.db` (<20 MB) wrapped by an MCP server |\n",
    "| âœ“ generate a MECE query set & ground-truth answers | `queries.json`, `gt_rows.json`                     |\n",
    "| âœ“ build NL â†” SQL result training pairs             | `train_pairs.jsonl`                                |\n",
    "| âœ“ run an RFT job on Fireworks AI                   | a tuned **Qwen-2.5-Coder-3B-RFT** model            |\n",
    "| âœ“ benchmark baseline vs. RFT accuracy              | >2Ã— exact-match gain                               |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **A Note on Our Method: Demo vs. Real World ğŸŒ** <br>\n",
    "> Throughout this tutorial, we will be clear about what we're doing for the purpose of this self-contained demo versus what you would do in a real-world scenario.\n",
    "> - **Real World ğŸŒ**: Look for these notes to see the parallel step you would take in your own environment if you wanted to apply this workflow, typically by swapping in your own private business assets like schemas or query logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Development Environment Setup\n",
    "**Complete these steps once in your terminal, *outside* this notebook.**\n",
    "\n",
    "1.  **Get a Fireworks AI API Key**\n",
    "    - Go to [fireworks.ai](https://fireworks.ai) and sign up.\n",
    "    - Create an API key from your settings page.\n",
    "    - Create a file named `.env` in your project directory and add your key:\n",
    "      ```\n",
    "      FIREWORKS_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "      ```\n",
    "\n",
    "2.  **Install `uv`**\n",
    "    - `uv` is a fast Python package manager from Astral. Follow the official installation instructions at [docs.astral.sh/uv/](https://docs.astral.sh/uv/).\n",
    "\n",
    "3.  **Create a Virtual Environment and Install Packages**\n",
    "    - Once `uv` is installed, create and activate a virtual environment.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv venv .venv\n",
    "    source .venv/bin/activate  # On Windows PowerShell: .venv\\Scripts\\Activate.ps1\n",
    "    ```\n",
    "    - Install all required packages using `uv add`.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv add duckdb tabulate pandas pyarrow requests \\\n",
    "           faker python-dotenv \\\n",
    "           jsonlines fireworks-ai \\\n",
    "           mcp-sdk mcp-server-motherduck\n",
    "    ```\n",
    "After running these commands, your environment is ready. You can proceed with the cells inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulate the \"Production\" Database\n",
    "First, we'll create a database that represents your real, populated production database. We'll download the public OpenFlights dataset and load it into a DuckDB file.\n",
    "\n",
    "> **Real World ğŸŒ**: You already have this! It's your live production database (or a replica). You would skip this entire step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… 'Production' database simulated at: data/prod_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# --- Download the raw data files ---\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "BASE_URL = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/\"\n",
    "FILES_TO_DOWNLOAD = {\n",
    "    \"airports\": \"airports.dat\",\n",
    "    \"airlines\": \"airlines.dat\",\n",
    "    \"routes\": \"routes.dat\",\n",
    "    \"countries\": \"countries.dat\",\n",
    "    \"planes\": \"planes.dat\"\n",
    "}\n",
    "# Define column names as the files don't have headers\n",
    "COLUMN_NAMES = {\n",
    "    \"airports\": [\"airport_id\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"],\n",
    "    \"airlines\": [\"airline_id\", \"name\", \"alias\", \"iata\", \"icao\", \"callsign\", \"country\", \"active\"],\n",
    "    \"routes\": [\"airline\", \"airline_id\", \"source_airport\", \"source_airport_id\", \"destination_airport\", \"destination_airport_id\", \"codeshare\", \"stops\", \"equipment\"],\n",
    "    \"countries\": [\"name\", \"iso_code\", \"dafif_code\"],\n",
    "    \"planes\": [\"name\", \"iata\", \"icao\"]\n",
    "}\n",
    "\n",
    "PROD_DB_PATH = \"data/prod_openflights.db\"\n",
    "\n",
    "# --- Load the real data into our \"production\" DuckDB ---\n",
    "with duckdb.connect(PROD_DB_PATH) as con:\n",
    "    for name, filename in FILES_TO_DOWNLOAD.items():\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            print(f\"âœ… Downloaded: {path}\")\n",
    "\n",
    "        # Load data using pandas to handle missing headers and null values\n",
    "        df = pd.read_csv(path, header=None, names=COLUMN_NAMES[name], na_values=[\"\\\\N\"])\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE {name} AS SELECT * FROM df\")\n",
    "\n",
    "    print(f\"\\nâœ… 'Production' database simulated at: {PROD_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Acquire the Schema (No Data!)\n",
    "This is a critical step. We connect to our \"production\" database and extract **only its schema** (the table structure, column names, and data types). We do not touch or read any of the data rows. This schema is the only artifact we need from the production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Schema successfully extracted from 'production' database:\n",
      "| database         | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\n",
      "|:-----------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\n",
      "| prod_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\n",
      "|                  |          |           |                                                                            |  'VARCHAR']                                                           |             |\n",
      "| prod_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\n",
      "|                  |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\n",
      "| prod_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR'   | False       |\n",
      "|                  |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\n",
      "|                  |          |           |  'equipment']                                                              |                                                                       |             |\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the \"production\" database we just created\n",
    "with duckdb.connect(PROD_DB_PATH, read_only=True) as con:\n",
    "    # The DESCRIBE command gives us the schema information for all tables\n",
    "    schema_df = con.sql(\"DESCRIBE;\").df()\n",
    "\n",
    "print(\"âœ… Schema successfully extracted from 'production' database:\")\n",
    "print(schema_df.to_markdown(index=False))\n",
    "\n",
    "# We can also store this for later use in prompts\n",
    "schema_for_prompt = schema_df.to_markdown(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Synthetic Training Sandbox with an LLM\n",
    "Now that we have the schema, we will use a large language model to generate a complete, contextually-aware synthetic dataset.\n",
    "\n",
    "To ensure the LLM's output is structured and parseable, we will **dynamically generate a Pydantic schema** based on the `DESCRIBE` output from the previous step. This is a powerful, generic technique that adapts to any database schema.\n",
    "\n",
    "To fine-tune our model with RFT, **we will only interact with this synthetic database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dynamically created Pydantic models for all tables.\n",
      "\n",
      "âœ… Data Generation Plan:\n",
      " - Target rows per table: 100\n",
      " - Will make API calls asking for 2 rows/call until target is met.\n",
      "\n",
      "ğŸ“ --- Generating data chunk #1 ---\n",
      "âœ… Received and parsed chunk #1.\n",
      "   - 'airlines': 2 / 100 rows\n",
      "   - 'airports': 2 / 100 rows\n",
      "   - 'countries': 2 / 100 rows\n",
      "   - 'planes': 2 / 100 rows\n",
      "   - 'routes': 2 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #2 ---\n",
      "âœ… Received and parsed chunk #2.\n",
      "   - 'airlines': 4 / 100 rows\n",
      "   - 'airports': 4 / 100 rows\n",
      "   - 'countries': 4 / 100 rows\n",
      "   - 'planes': 4 / 100 rows\n",
      "   - 'routes': 4 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #3 ---\n",
      "âœ… Received and parsed chunk #3.\n",
      "   - 'airlines': 6 / 100 rows\n",
      "   - 'airports': 6 / 100 rows\n",
      "   - 'countries': 6 / 100 rows\n",
      "   - 'planes': 6 / 100 rows\n",
      "   - 'routes': 6 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #4 ---\n",
      "âœ… Received and parsed chunk #4.\n",
      "   - 'airlines': 8 / 100 rows\n",
      "   - 'airports': 8 / 100 rows\n",
      "   - 'countries': 8 / 100 rows\n",
      "   - 'planes': 8 / 100 rows\n",
      "   - 'routes': 8 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #5 ---\n",
      "âœ… Received and parsed chunk #5.\n",
      "   - 'airlines': 10 / 100 rows\n",
      "   - 'airports': 10 / 100 rows\n",
      "   - 'countries': 10 / 100 rows\n",
      "   - 'planes': 10 / 100 rows\n",
      "   - 'routes': 10 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #6 ---\n",
      "âœ… Received and parsed chunk #6.\n",
      "   - 'airlines': 12 / 100 rows\n",
      "   - 'airports': 12 / 100 rows\n",
      "   - 'countries': 12 / 100 rows\n",
      "   - 'planes': 12 / 100 rows\n",
      "   - 'routes': 12 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #7 ---\n",
      "âœ… Received and parsed chunk #7.\n",
      "   - 'airlines': 14 / 100 rows\n",
      "   - 'airports': 14 / 100 rows\n",
      "   - 'countries': 14 / 100 rows\n",
      "   - 'planes': 14 / 100 rows\n",
      "   - 'routes': 14 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #8 ---\n",
      "âœ… Received and parsed chunk #8.\n",
      "   - 'airlines': 16 / 100 rows\n",
      "   - 'airports': 16 / 100 rows\n",
      "   - 'countries': 16 / 100 rows\n",
      "   - 'planes': 16 / 100 rows\n",
      "   - 'routes': 16 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #9 ---\n",
      "âœ… Received and parsed chunk #9.\n",
      "   - 'airlines': 18 / 100 rows\n",
      "   - 'airports': 18 / 100 rows\n",
      "   - 'countries': 18 / 100 rows\n",
      "   - 'planes': 18 / 100 rows\n",
      "   - 'routes': 18 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #10 ---\n",
      "âœ… Received and parsed chunk #10.\n",
      "   - 'airlines': 20 / 100 rows\n",
      "   - 'airports': 20 / 100 rows\n",
      "   - 'countries': 20 / 100 rows\n",
      "   - 'planes': 20 / 100 rows\n",
      "   - 'routes': 20 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #11 ---\n",
      "âœ… Received and parsed chunk #11.\n",
      "   - 'airlines': 22 / 100 rows\n",
      "   - 'airports': 22 / 100 rows\n",
      "   - 'countries': 22 / 100 rows\n",
      "   - 'planes': 22 / 100 rows\n",
      "   - 'routes': 22 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #12 ---\n",
      "âœ… Received and parsed chunk #12.\n",
      "   - 'airlines': 24 / 100 rows\n",
      "   - 'airports': 24 / 100 rows\n",
      "   - 'countries': 24 / 100 rows\n",
      "   - 'planes': 24 / 100 rows\n",
      "   - 'routes': 24 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #13 ---\n",
      "âœ… Received and parsed chunk #13.\n",
      "   - 'airlines': 26 / 100 rows\n",
      "   - 'airports': 26 / 100 rows\n",
      "   - 'countries': 26 / 100 rows\n",
      "   - 'planes': 26 / 100 rows\n",
      "   - 'routes': 26 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #14 ---\n",
      "âœ… Received and parsed chunk #14.\n",
      "   - 'airlines': 28 / 100 rows\n",
      "   - 'airports': 28 / 100 rows\n",
      "   - 'countries': 28 / 100 rows\n",
      "   - 'planes': 28 / 100 rows\n",
      "   - 'routes': 28 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #15 ---\n",
      "âœ… Received and parsed chunk #15.\n",
      "   - 'airlines': 30 / 100 rows\n",
      "   - 'airports': 30 / 100 rows\n",
      "   - 'countries': 30 / 100 rows\n",
      "   - 'planes': 30 / 100 rows\n",
      "   - 'routes': 30 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #16 ---\n",
      "âœ… Received and parsed chunk #16.\n",
      "   - 'airlines': 32 / 100 rows\n",
      "   - 'airports': 32 / 100 rows\n",
      "   - 'countries': 32 / 100 rows\n",
      "   - 'planes': 32 / 100 rows\n",
      "   - 'routes': 32 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #17 ---\n",
      "âœ… Received and parsed chunk #17.\n",
      "   - 'airlines': 34 / 100 rows\n",
      "   - 'airports': 34 / 100 rows\n",
      "   - 'countries': 34 / 100 rows\n",
      "   - 'planes': 34 / 100 rows\n",
      "   - 'routes': 34 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #18 ---\n",
      "âœ… Received and parsed chunk #18.\n",
      "   - 'airlines': 36 / 100 rows\n",
      "   - 'airports': 36 / 100 rows\n",
      "   - 'countries': 36 / 100 rows\n",
      "   - 'planes': 36 / 100 rows\n",
      "   - 'routes': 36 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #19 ---\n",
      "âœ… Received and parsed chunk #19.\n",
      "   - 'airlines': 38 / 100 rows\n",
      "   - 'airports': 38 / 100 rows\n",
      "   - 'countries': 38 / 100 rows\n",
      "   - 'planes': 38 / 100 rows\n",
      "   - 'routes': 38 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #20 ---\n",
      "âœ… Received and parsed chunk #20.\n",
      "   - 'airlines': 40 / 100 rows\n",
      "   - 'airports': 40 / 100 rows\n",
      "   - 'countries': 40 / 100 rows\n",
      "   - 'planes': 40 / 100 rows\n",
      "   - 'routes': 40 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #21 ---\n",
      "âœ… Received and parsed chunk #21.\n",
      "   - 'airlines': 42 / 100 rows\n",
      "   - 'airports': 42 / 100 rows\n",
      "   - 'countries': 42 / 100 rows\n",
      "   - 'planes': 42 / 100 rows\n",
      "   - 'routes': 42 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #22 ---\n",
      "âœ… Received and parsed chunk #22.\n",
      "   - 'airlines': 44 / 100 rows\n",
      "   - 'airports': 44 / 100 rows\n",
      "   - 'countries': 44 / 100 rows\n",
      "   - 'planes': 44 / 100 rows\n",
      "   - 'routes': 44 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #23 ---\n",
      "âœ… Received and parsed chunk #23.\n",
      "   - 'airlines': 46 / 100 rows\n",
      "   - 'airports': 46 / 100 rows\n",
      "   - 'countries': 46 / 100 rows\n",
      "   - 'planes': 46 / 100 rows\n",
      "   - 'routes': 46 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #24 ---\n",
      "âœ… Received and parsed chunk #24.\n",
      "   - 'airlines': 48 / 100 rows\n",
      "   - 'airports': 48 / 100 rows\n",
      "   - 'countries': 48 / 100 rows\n",
      "   - 'planes': 48 / 100 rows\n",
      "   - 'routes': 48 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #25 ---\n",
      "âœ… Received and parsed chunk #25.\n",
      "   - 'airlines': 50 / 100 rows\n",
      "   - 'airports': 50 / 100 rows\n",
      "   - 'countries': 50 / 100 rows\n",
      "   - 'planes': 50 / 100 rows\n",
      "   - 'routes': 50 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #26 ---\n",
      "âœ… Received and parsed chunk #26.\n",
      "   - 'airlines': 52 / 100 rows\n",
      "   - 'airports': 52 / 100 rows\n",
      "   - 'countries': 52 / 100 rows\n",
      "   - 'planes': 52 / 100 rows\n",
      "   - 'routes': 52 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #27 ---\n",
      "âœ… Received and parsed chunk #27.\n",
      "   - 'airlines': 54 / 100 rows\n",
      "   - 'airports': 54 / 100 rows\n",
      "   - 'countries': 54 / 100 rows\n",
      "   - 'planes': 54 / 100 rows\n",
      "   - 'routes': 54 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #28 ---\n",
      "âœ… Received and parsed chunk #28.\n",
      "   - 'airlines': 56 / 100 rows\n",
      "   - 'airports': 56 / 100 rows\n",
      "   - 'countries': 56 / 100 rows\n",
      "   - 'planes': 56 / 100 rows\n",
      "   - 'routes': 56 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #29 ---\n",
      "âœ… Received and parsed chunk #29.\n",
      "   - 'airlines': 58 / 100 rows\n",
      "   - 'airports': 58 / 100 rows\n",
      "   - 'countries': 58 / 100 rows\n",
      "   - 'planes': 58 / 100 rows\n",
      "   - 'routes': 58 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #30 ---\n",
      "âœ… Received and parsed chunk #30.\n",
      "   - 'airlines': 60 / 100 rows\n",
      "   - 'airports': 60 / 100 rows\n",
      "   - 'countries': 60 / 100 rows\n",
      "   - 'planes': 60 / 100 rows\n",
      "   - 'routes': 60 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #31 ---\n",
      "âœ… Received and parsed chunk #31.\n",
      "   - 'airlines': 62 / 100 rows\n",
      "   - 'airports': 62 / 100 rows\n",
      "   - 'countries': 62 / 100 rows\n",
      "   - 'planes': 62 / 100 rows\n",
      "   - 'routes': 62 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #32 ---\n",
      "âœ… Received and parsed chunk #32.\n",
      "   - 'airlines': 64 / 100 rows\n",
      "   - 'airports': 64 / 100 rows\n",
      "   - 'countries': 64 / 100 rows\n",
      "   - 'planes': 64 / 100 rows\n",
      "   - 'routes': 64 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #33 ---\n",
      "âœ… Received and parsed chunk #33.\n",
      "   - 'airlines': 66 / 100 rows\n",
      "   - 'airports': 66 / 100 rows\n",
      "   - 'countries': 66 / 100 rows\n",
      "   - 'planes': 66 / 100 rows\n",
      "   - 'routes': 66 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #34 ---\n",
      "âœ… Received and parsed chunk #34.\n",
      "   - 'airlines': 68 / 100 rows\n",
      "   - 'airports': 68 / 100 rows\n",
      "   - 'countries': 68 / 100 rows\n",
      "   - 'planes': 68 / 100 rows\n",
      "   - 'routes': 68 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #35 ---\n",
      "âœ… Received and parsed chunk #35.\n",
      "   - 'airlines': 70 / 100 rows\n",
      "   - 'airports': 70 / 100 rows\n",
      "   - 'countries': 70 / 100 rows\n",
      "   - 'planes': 70 / 100 rows\n",
      "   - 'routes': 70 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #36 ---\n",
      "âœ… Received and parsed chunk #36.\n",
      "   - 'airlines': 72 / 100 rows\n",
      "   - 'airports': 72 / 100 rows\n",
      "   - 'countries': 72 / 100 rows\n",
      "   - 'planes': 72 / 100 rows\n",
      "   - 'routes': 72 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #37 ---\n",
      "âœ… Received and parsed chunk #37.\n",
      "   - 'airlines': 74 / 100 rows\n",
      "   - 'airports': 74 / 100 rows\n",
      "   - 'countries': 74 / 100 rows\n",
      "   - 'planes': 74 / 100 rows\n",
      "   - 'routes': 74 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #38 ---\n",
      "âœ… Received and parsed chunk #38.\n",
      "   - 'airlines': 76 / 100 rows\n",
      "   - 'airports': 76 / 100 rows\n",
      "   - 'countries': 76 / 100 rows\n",
      "   - 'planes': 76 / 100 rows\n",
      "   - 'routes': 76 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #39 ---\n",
      "âœ… Received and parsed chunk #39.\n",
      "   - 'airlines': 78 / 100 rows\n",
      "   - 'airports': 78 / 100 rows\n",
      "   - 'countries': 78 / 100 rows\n",
      "   - 'planes': 78 / 100 rows\n",
      "   - 'routes': 78 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #40 ---\n",
      "âœ… Received and parsed chunk #40.\n",
      "   - 'airlines': 80 / 100 rows\n",
      "   - 'airports': 80 / 100 rows\n",
      "   - 'countries': 80 / 100 rows\n",
      "   - 'planes': 80 / 100 rows\n",
      "   - 'routes': 80 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #41 ---\n",
      "âœ… Received and parsed chunk #41.\n",
      "   - 'airlines': 82 / 100 rows\n",
      "   - 'airports': 82 / 100 rows\n",
      "   - 'countries': 82 / 100 rows\n",
      "   - 'planes': 82 / 100 rows\n",
      "   - 'routes': 82 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #42 ---\n",
      "âœ… Received and parsed chunk #42.\n",
      "   - 'airlines': 84 / 100 rows\n",
      "   - 'airports': 84 / 100 rows\n",
      "   - 'countries': 84 / 100 rows\n",
      "   - 'planes': 84 / 100 rows\n",
      "   - 'routes': 84 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #43 ---\n",
      "âœ… Received and parsed chunk #43.\n",
      "   - 'airlines': 86 / 100 rows\n",
      "   - 'airports': 86 / 100 rows\n",
      "   - 'countries': 86 / 100 rows\n",
      "   - 'planes': 86 / 100 rows\n",
      "   - 'routes': 86 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #44 ---\n",
      "âœ… Received and parsed chunk #44.\n",
      "   - 'airlines': 88 / 100 rows\n",
      "   - 'airports': 88 / 100 rows\n",
      "   - 'countries': 88 / 100 rows\n",
      "   - 'planes': 88 / 100 rows\n",
      "   - 'routes': 88 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #45 ---\n",
      "âœ… Received and parsed chunk #45.\n",
      "   - 'airlines': 90 / 100 rows\n",
      "   - 'airports': 90 / 100 rows\n",
      "   - 'countries': 90 / 100 rows\n",
      "   - 'planes': 90 / 100 rows\n",
      "   - 'routes': 90 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #46 ---\n",
      "âœ… Received and parsed chunk #46.\n",
      "   - 'airlines': 92 / 100 rows\n",
      "   - 'airports': 92 / 100 rows\n",
      "   - 'countries': 92 / 100 rows\n",
      "   - 'planes': 92 / 100 rows\n",
      "   - 'routes': 92 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #47 ---\n",
      "âœ… Received and parsed chunk #47.\n",
      "   - 'airlines': 94 / 100 rows\n",
      "   - 'airports': 94 / 100 rows\n",
      "   - 'countries': 94 / 100 rows\n",
      "   - 'planes': 94 / 100 rows\n",
      "   - 'routes': 94 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #48 ---\n",
      "âœ… Received and parsed chunk #48.\n",
      "   - 'airlines': 96 / 100 rows\n",
      "   - 'airports': 96 / 100 rows\n",
      "   - 'countries': 96 / 100 rows\n",
      "   - 'planes': 96 / 100 rows\n",
      "   - 'routes': 96 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #49 ---\n",
      "âœ… Received and parsed chunk #49.\n",
      "   - 'airlines': 98 / 100 rows\n",
      "   - 'airports': 98 / 100 rows\n",
      "   - 'countries': 98 / 100 rows\n",
      "   - 'planes': 98 / 100 rows\n",
      "   - 'routes': 98 / 100 rows\n",
      "\n",
      "ğŸ“ --- Generating data chunk #50 ---\n",
      "âœ… Received and parsed chunk #50.\n",
      "   - 'airlines': 100 / 100 rows\n",
      "   - 'airports': 100 / 100 rows\n",
      "   - 'countries': 100 / 100 rows\n",
      "   - 'planes': 100 / 100 rows\n",
      "   - 'routes': 100 / 100 rows\n",
      "\n",
      "âœ¨ Data generation complete. Aggregating, deduplicating, and saving to database...\n",
      "\n",
      "--- Deduplicating generated data ---\n",
      " - Table 'airlines': Removed 0 duplicates (100 -> 100).\n",
      " - Table 'airports': Removed 0 duplicates (100 -> 100).\n",
      " - Table 'countries': Removed 63 duplicates (100 -> 37).\n",
      " - Table 'planes': Removed 54 duplicates (100 -> 46).\n",
      " - Table 'routes': Removed 0 duplicates (100 -> 100).\n",
      "\n",
      "âœ… Synthetic training sandbox created at: data/synthetic_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pydantic import create_model, BaseModel\n",
    "from fireworks import LLM\n",
    "import duckdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Any, Dict, Type\n",
    "import datetime\n",
    "import decimal\n",
    "import uuid\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "TARGET_ROW_COUNT = 100  # The number of rows to generate for each table.\n",
    "\n",
    "# --- 1. Dynamically Create Pydantic Models from the SQL Schema ---\n",
    "def map_sql_type_to_python(sql_type: str) -> Type:\n",
    "    \"\"\"Maps SQL data types to Python types for Pydantic models.\"\"\"\n",
    "    sql_type_upper = str(sql_type).upper()\n",
    "    if 'DECIMAL' in sql_type_upper: return decimal.Decimal\n",
    "    if 'DOUBLE' in sql_type_upper or 'FLOAT' in sql_type_upper or 'REAL' in sql_type_upper: return float\n",
    "    if 'BIGINT' in sql_type_upper or 'INT' in sql_type_upper: return int\n",
    "    if 'VARCHAR' in sql_type_upper or 'TEXT' in sql_type_upper or 'STRING' in sql_type_upper: return str\n",
    "    if 'TIMESTAMP' in sql_type_upper: return datetime.datetime\n",
    "    if 'DATE' in sql_type_upper: return datetime.date\n",
    "    if 'TIME' in sql_type_upper: return datetime.time\n",
    "    if 'BOOLEAN' in sql_type_upper: return bool\n",
    "    if 'BLOB' in sql_type_upper or 'BYTEA' in sql_type_upper: return bytes\n",
    "    if 'UUID' in sql_type_upper: return uuid.UUID\n",
    "    return object\n",
    "\n",
    "pydantic_models: Dict[str, Type[BaseModel]] = {}\n",
    "table_names = schema_df['name'].unique()\n",
    "\n",
    "for table_name in table_names:\n",
    "    table_schema = schema_df[schema_df['name'] == table_name].iloc[0]\n",
    "    fields: Dict[str, Any] = {}\n",
    "    col_names = table_schema['column_names']\n",
    "    col_types = table_schema['column_types']\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        python_type = map_sql_type_to_python(col_types[i])\n",
    "        fields[col_name] = (Optional[python_type], None)\n",
    "    model_name = table_name.capitalize() + \"Model\"\n",
    "    pydantic_models[table_name] = create_model(model_name, **fields)\n",
    "\n",
    "dataset_fields: Dict[str, Any] = {\n",
    "    table_name: (List[model], ...) for table_name, model in pydantic_models.items()\n",
    "}\n",
    "SyntheticDataset = create_model('SyntheticDataset', **dataset_fields)\n",
    "print(\"âœ… Dynamically created Pydantic models for all tables.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Total Row Counts and Chunking Strategy ---\n",
    "TOTAL_ROW_COUNTS = {name: TARGET_ROW_COUNT for name in table_names}\n",
    "ROWS_PER_API_CALL = 2 # Ask for data in small, safe chunks\n",
    "print(\"\\nâœ… Data Generation Plan:\")\n",
    "print(f\" - Target rows per table: {list(TOTAL_ROW_COUNTS.values())[0]}\")\n",
    "print(f\" - Will make API calls asking for {ROWS_PER_API_CALL} rows/call until target is met.\")\n",
    "\n",
    "\n",
    "# --- 3. Setup LLM and Loop to Generate Data in Chunks ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "load_dotenv()\n",
    "llm = LLM(model=\"accounts/fireworks/models/deepseek-v3\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "\n",
    "all_synthetic_data: Dict[str, List[Dict]] = {name: [] for name in table_names}\n",
    "chunk_row_counts = {name: ROWS_PER_API_CALL for name in table_names}\n",
    "\n",
    "base_generation_prompt = f\"\"\"\n",
    "You are a highly intelligent AI data generator. Your task is to create a realistic, synthetic dataset based on the provided database schema.\n",
    "The data you generate must be internally consistent. For example, an `airline_id` in a `routes` table must correspond to an existing `airline_id` in an `airlines` table within this same generated chunk.\n",
    "This applies to any schema you might be working with, not just airlines.\n",
    "You must generate a single JSON object that strictly adheres to the provided JSON schema.\n",
    "\n",
    "The database schema is as follows:\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "call_count = 0\n",
    "# Loop until all tables have at least the desired number of rows\n",
    "while not all(len(rows) >= TOTAL_ROW_COUNTS[name] for name, rows in all_synthetic_data.items()):\n",
    "    call_count += 1\n",
    "    print(f\"\\nğŸ“ --- Generating data chunk #{call_count} ---\")\n",
    "    \n",
    "    # --- Create a summary of existing data to guide the LLM ---\n",
    "    existing_data_summary = \"\"\n",
    "    if any(len(rows) > 0 for rows in all_synthetic_data.values()):\n",
    "        summary_parts = [\"\\nYou have already generated the following data. Do NOT generate rows that are substantially similar to these examples. Create new, distinct data.\\n\"]\n",
    "        for table_name, rows in all_synthetic_data.items():\n",
    "            if rows:\n",
    "                summary_parts.append(f\"\\n--- Existing data in '{table_name}' table ---\")\n",
    "                df = pd.DataFrame(rows)\n",
    "                if len(df.columns) > 10:\n",
    "                    df = df.iloc[:, :10]\n",
    "                markdown_summary = df.to_markdown(index=False, tablefmt=\"grid\")\n",
    "                if markdown_summary:\n",
    "                    summary_parts.append(markdown_summary)\n",
    "        existing_data_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "    # --- Construct the final prompt for this iteration ---\n",
    "    final_prompt = (\n",
    "        base_generation_prompt +\n",
    "        existing_data_summary +\n",
    "        f\"\\n\\nNow, generate a NEW JSON object with a key for each table. The number of new rows for each table should be:\\n\" +\n",
    "        json.dumps(chunk_row_counts, indent=2)\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SyntheticDataset\", \"schema\": SyntheticDataset.model_json_schema()}},\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    choice = response.choices[0]\n",
    "    response_content = choice.message.content\n",
    "\n",
    "    if choice.finish_reason == \"length\":\n",
    "        print(f\"âš ï¸ WARNING: Chunk #{call_count} was truncated. Skipping.\")\n",
    "        continue\n",
    "    if not response_content:\n",
    "        print(f\"âš ï¸ WARNING: Received empty content for chunk #{call_count}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chunk_data = json.loads(response_content)\n",
    "        print(f\"âœ… Received and parsed chunk #{call_count}.\")\n",
    "        for table_name, rows in chunk_data.items():\n",
    "            if table_name in all_synthetic_data and rows:\n",
    "                all_synthetic_data[table_name].extend(rows)\n",
    "        # Log progress\n",
    "        for name, rows in all_synthetic_data.items():\n",
    "             print(f\"   - '{name}': {len(rows)} / {TOTAL_ROW_COUNTS[name]} rows\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ ERROR: Failed to parse JSON for chunk #{call_count}. Reason: {e}. Skipping.\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# --- 4. Deduplicate and Write to DB ---\n",
    "print(\"\\nâœ¨ Data generation complete. Aggregating, deduplicating, and saving to database...\")\n",
    "\n",
    "synthetic_data = all_synthetic_data\n",
    "print(\"\\n--- Deduplicating generated data ---\")\n",
    "for table_name, rows in synthetic_data.items():\n",
    "    if not rows: continue\n",
    "    initial_count = len(rows)\n",
    "    df = pd.DataFrame(rows).drop_duplicates()\n",
    "    final_count = len(df)\n",
    "    synthetic_data[table_name] = df.to_dict('records')\n",
    "    print(f\" - Table '{table_name}': Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Final trim to ensure exact counts\n",
    "for table_name, total_rows_needed in TOTAL_ROW_COUNTS.items():\n",
    "    if table_name in synthetic_data:\n",
    "        synthetic_data[table_name] = synthetic_data[table_name][:total_rows_needed]\n",
    "\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH) as con:\n",
    "    for table_name, rows in synthetic_data.items():\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            schema_cols = schema_df[schema_df['name'] == table_name].iloc[0]['column_names']\n",
    "            for col in schema_cols:\n",
    "                if col not in df.columns: df[col] = None\n",
    "            df = df[schema_cols]\n",
    "            con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    print(f\"\\nâœ… Synthetic training sandbox created at: {SYNTHETIC_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate the Synthetic Sandbox\n",
    "Let's run a few queries against our new synthetic database to ensure the LLM did a good job generating plausible, interconnected data. We expect to see non-empty results from these joins, which confirms that the referential integrity is holding up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validating the first few tables in the synthetic sandbox ---\n",
      "\n",
      "--- SELECT * FROM airlines LIMIT 10; ---\n",
      "+----+--------------+------------------+---------+--------+--------+----------------+----------------+----------+\n",
      "|    |   airline_id | name             | alias   | iata   | icao   | callsign       | country        | active   |\n",
      "|----+--------------+------------------+---------+--------+--------+----------------+----------------+----------|\n",
      "|  0 |            1 | SkyHigh Airlines | SHA     | SH     | SKY    | SKYHIGH        | United States  | Y        |\n",
      "|  1 |            2 | Oceanic Airways  | OA      | OC     | OCN    | OCEANIC        | United Kingdom | Y        |\n",
      "|  2 |            3 | Global Wings     | GW      | GL     | GLB    | GLOBALWINGS    | Canada         | Y        |\n",
      "|  3 |            4 | Pacific Horizon  | PH      | PA     | PHZ    | PACIFICHORIZON | Australia      | Y        |\n",
      "|  4 |            5 | Arctic Air       | AA      | AR     | ARC    | ARCTIC         | Canada         | Y        |\n",
      "|  5 |            6 | Sahara Sky       | SS      | SA     | SHR    | SAHARASKY      | Egypt          | Y        |\n",
      "|  6 |            7 | Nordic Air       | NA      | ND     | NRC    | NORDICAIR      | Norway         | Y        |\n",
      "|  7 |            8 | Sunrise Airlines | SA      | SR     | SUN    | SUNRISE        | Japan          | Y        |\n",
      "|  8 |            9 | Alpine Air       | AL      | AP     | ALP    | ALPINE         | Switzerland    | Y        |\n",
      "|  9 |           10 | Island Hopper    | IH      | IH     | ISL    | ISLANDER       | Fiji           | Y        |\n",
      "+----+--------------+------------------+---------+--------+--------+----------------+----------------+----------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM airports LIMIT 10; ---\n",
      "+----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------+\n",
      "|    |   airport_id | name                          | city      | country        | iata   | icao   |   latitude |   longitude |   altitude |   timezone | dst   | tz_db            | type    | source      |\n",
      "|----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------|\n",
      "|  0 |            1 | SkyHigh International Airport | New York  | United States  | SKY    | KSKY   |    40.7128 |    -74.006  |         10 |         -5 | A     | America/New_York | airport | OurAirports |\n",
      "|  1 |            2 | Oceanic Gateway Airport       | London    | United Kingdom | OCN    | EGLC   |    51.5074 |     -0.1278 |         15 |          0 | E     | Europe/London    | airport | OurAirports |\n",
      "|  2 |            3 | Global Wings Hub              | Toronto   | Canada         | GLH    | CYYZ   |    43.6532 |    -79.3832 |        173 |         -5 | A     | America/Toronto  | airport | OurAirports |\n",
      "|  3 |            4 | Pacific Horizon Terminal      | Sydney    | Australia      | PHT    | YSSY   |   -33.8688 |    151.209  |         21 |         10 | O     | Australia/Sydney | airport | OurAirports |\n",
      "|  4 |            5 | Arctic Air Terminal           | Vancouver | Canada         | ARC    | CYVR   |    49.1936 |   -123.184  |          4 |         -8 |       |                  |         |             |\n",
      "|  5 |            6 | Sahara Sky Hub                | Cairo     | Egypt          | SHR    | HECA   |    30.1092 |     31.4022 |        116 |          2 |       |                  |         |             |\n",
      "|  6 |            7 | Nordic Air Hub                | Oslo      | Norway         | NRD    | ENOA   |    59.9139 |     10.7522 |         23 |          1 |       |                  |         |             |\n",
      "|  7 |            8 | Sunrise International Airport | Tokyo     | Japan          | SRN    | RJTT   |    35.5494 |    139.78   |         35 |          9 |       |                  |         |             |\n",
      "|  8 |            9 | Alpine Air Hub                | Zurich    | Switzerland    | ALP    | LSZH   |    47.4647 |      8.5492 |       1416 |          1 | E     | Europe/Zurich    | airport | OurAirports |\n",
      "|  9 |           10 | Island Hopper Terminal        | Nadi      | Fiji           | IHB    | NFFN   |   -17.7554 |    177.443  |         18 |         12 | U     | Pacific/Fiji     | airport | OurAirports |\n",
      "+----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM countries LIMIT 10; ---\n",
      "+----+----------------+------------+--------------+\n",
      "|    | name           | iso_code   | dafif_code   |\n",
      "|----+----------------+------------+--------------|\n",
      "|  0 | United States  | US         | US           |\n",
      "|  1 | United Kingdom | GB         | UK           |\n",
      "|  2 | Canada         | CA         | CA           |\n",
      "|  3 | Australia      | AU         | AS           |\n",
      "|  4 | Egypt          | EG         | EG           |\n",
      "|  5 | Japan          | JP         | JA           |\n",
      "|  6 | Norway         | NO         | NO           |\n",
      "|  7 | China          | CN         | CH           |\n",
      "|  8 | Switzerland    | CH         | SZ           |\n",
      "|  9 | Fiji           | FJ         | FJ           |\n",
      "+----+----------------+------------+--------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM planes LIMIT 10; ---\n",
      "+----+-------------------+--------+--------+\n",
      "|    | name              | iata   | icao   |\n",
      "|----+-------------------+--------+--------|\n",
      "|  0 | Boeing 747        | 74     | B744   |\n",
      "|  1 | Airbus A380       | 38     | A388   |\n",
      "|  2 | Boeing 777        | 77     | B777   |\n",
      "|  3 | Airbus A320       | 32     | A320   |\n",
      "|  4 | Boeing 737        | 73     | B737   |\n",
      "|  5 | Airbus A350       | 35     | A350   |\n",
      "|  6 | Boeing 787        | 78     | B788   |\n",
      "|  7 | Embraer E195      | E9     | E195   |\n",
      "|  8 | Bombardier CRJ900 | CR     | CRJ9   |\n",
      "|  9 | ATR 72            | AT     | AT72   |\n",
      "+----+-------------------+--------+--------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM routes LIMIT 10; ---\n",
      "+----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------+\n",
      "|    | airline          |   airline_id | source_airport                |   source_airport_id | destination_airport           |   destination_airport_id | codeshare   |   stops | equipment         |\n",
      "|----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------|\n",
      "|  0 | SkyHigh Airlines |            1 | SkyHigh International Airport |                   1 | Oceanic Gateway Airport       |                        2 |             |       0 | Boeing 747        |\n",
      "|  1 | Oceanic Airways  |            2 | Oceanic Gateway Airport       |                   2 | SkyHigh International Airport |                        1 |             |       0 | Airbus A380       |\n",
      "|  2 | Global Wings     |            3 | Global Wings Hub              |                   3 | Pacific Horizon Terminal      |                        4 |             |       0 | Boeing 777        |\n",
      "|  3 | Pacific Horizon  |            4 | Pacific Horizon Terminal      |                   4 | Global Wings Hub              |                        3 |             |       0 | Airbus A320       |\n",
      "|  4 | Arctic Air       |            5 | Arctic Air Terminal           |                   5 | Sahara Sky Hub                |                        6 |             |       0 | Boeing 737        |\n",
      "|  5 | Sahara Sky       |            6 | Sahara Sky Hub                |                   6 | Arctic Air Terminal           |                        5 |             |       0 | Airbus A350       |\n",
      "|  6 | Nordic Air       |            7 | Nordic Air Hub                |                   7 | Sunrise International Airport |                        8 |             |       0 | Boeing 787        |\n",
      "|  7 | Sunrise Airlines |            8 | Sunrise International Airport |                   8 | Nordic Air Hub                |                        7 |             |       0 | Embraer E195      |\n",
      "|  8 | Alpine Air       |            9 | Alpine Air Hub                |                   9 | Island Hopper Terminal        |                       10 |             |       0 | Bombardier CRJ900 |\n",
      "|  9 | Island Hopper    |           10 | Island Hopper Terminal        |                  10 | Alpine Air Hub                |                        9 |             |       0 | ATR 72            |\n",
      "+----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Connect to the synthetic database\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    \n",
    "    # Get the list of all tables created\n",
    "    all_tables = [table[0] for table in con.sql(\"SHOW TABLES;\").fetchall()]\n",
    "    \n",
    "    # Select the first 5 tables to display (or all if fewer than 5)\n",
    "    tables_to_validate = all_tables[:5]\n",
    "\n",
    "    print(\"--- Validating the first few tables in the synthetic sandbox ---\\n\")\n",
    "\n",
    "    # Execute and print results for the selected tables\n",
    "    for table_name in tables_to_validate:\n",
    "        print(f\"--- SELECT * FROM {table_name} LIMIT 10; ---\")\n",
    "        try:\n",
    "            result_df = con.sql(f\"SELECT * FROM {table_name} LIMIT 10;\").df()\n",
    "            if not result_df.empty:\n",
    "                print(tabulate(result_df, headers='keys', tablefmt='psql'))\n",
    "            else:\n",
    "                print(f\"(Table '{table_name}' is empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed for table '{table_name}': {e}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate Example SQL Queries\n",
    "With our synthetic database in place, the next step is to create a set of synthetic SQL queries. These SQL queries will be executed against our database of synthetic data to get the ground truth labels for RFT. Furthermore, these same SQL queries will be used as input to an LLM to generate queries in natural language. This will enable us to form our final RFT dataset, which pairs natural language queries with ground truth results from the database.\n",
    "\n",
    "> **Real World ğŸŒ**: You would use a historical log of real SQL queries that have been run against your production database. These logs are the most valuable source of training data because they represent the *actual* way your users query your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Goal: Generate 400 unique queries in batches of 10.\n",
      "\n",
      "ğŸ“ --- Generating batch #1 ---\n",
      "   - Received 10 new queries. Total now: 10 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #2 ---\n",
      "   - Received 10 new queries. Total now: 20 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #3 ---\n",
      "   - Received 10 new queries. Total now: 30 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #4 ---\n",
      "   - Received 10 new queries. Total now: 40 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #5 ---\n",
      "   - Received 10 new queries. Total now: 50 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #6 ---\n",
      "   - Received 10 new queries. Total now: 60 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #7 ---\n",
      "   - Received 10 new queries. Total now: 70 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #8 ---\n",
      "   - Received 10 new queries. Total now: 80 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #9 ---\n",
      "   - Received 10 new queries. Total now: 90 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #10 ---\n",
      "   - Received 10 new queries. Total now: 100 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #11 ---\n",
      "   - Received 10 new queries. Total now: 110 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #12 ---\n",
      "   - Received 10 new queries. Total now: 120 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #13 ---\n",
      "   - Received 10 new queries. Total now: 130 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #14 ---\n",
      "   - Received 10 new queries. Total now: 140 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #15 ---\n",
      "   - Received 10 new queries. Total now: 150 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #16 ---\n",
      "   - Received 10 new queries. Total now: 160 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #17 ---\n",
      "   - Received 10 new queries. Total now: 170 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #18 ---\n",
      "   - Received 10 new queries. Total now: 180 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #19 ---\n",
      "   - Received 10 new queries. Total now: 190 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #20 ---\n",
      "   - Received 10 new queries. Total now: 200 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #21 ---\n",
      "   - Received 10 new queries. Total now: 210 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #22 ---\n",
      "   - Received 10 new queries. Total now: 220 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #23 ---\n",
      "   - Received 10 new queries. Total now: 230 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #24 ---\n",
      "   - Received 10 new queries. Total now: 240 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #25 ---\n",
      "   - Received 10 new queries. Total now: 250 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #26 ---\n",
      "   - Received 10 new queries. Total now: 260 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #27 ---\n",
      "   - Received 10 new queries. Total now: 270 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #28 ---\n",
      "   - Received 10 new queries. Total now: 280 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #29 ---\n",
      "   - Received 10 new queries. Total now: 290 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #30 ---\n",
      "   - Received 10 new queries. Total now: 300 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #31 ---\n",
      "   - Received 10 new queries. Total now: 310 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #32 ---\n",
      "   - Received 10 new queries. Total now: 320 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #33 ---\n",
      "   - Received 10 new queries. Total now: 330 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #34 ---\n",
      "   - Received 10 new queries. Total now: 340 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #35 ---\n",
      "   - Received 10 new queries. Total now: 350 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #36 ---\n",
      "   - Received 10 new queries. Total now: 360 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #37 ---\n",
      "   - Received 10 new queries. Total now: 370 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #38 ---\n",
      "   - Received 10 new queries. Total now: 380 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #39 ---\n",
      "   - Received 10 new queries. Total now: 390 / 400\n",
      "\n",
      "ğŸ“ --- Generating batch #40 ---\n",
      "   - Received 10 new queries. Total now: 400 / 400\n",
      "\n",
      "âœ¨ Generation complete. Deduplicating and saving...\n",
      " - Removed 137 duplicates (400 -> 263).\n",
      "\n",
      "âœ… Successfully saved 263 unique queries to `data/generated_queries.json`.\n",
      "\n",
      "--- Here are a few examples: ---\n",
      "- SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\n",
      "- SELECT a.name AS airline_name, ap.city AS source_city, ap2.city AS destination_city FROM routes r JOIN airlines a ON r.airline_id = a.airline_id JOIN airports ap ON r.source_airport_id = ap.airport_id JOIN airports ap2 ON r.destination_airport_id = ap2.airport_id WHERE a.active = 'Y' ORDER BY airline_name, source_city, destination_city\n",
      "- SELECT city, COUNT(*) AS airport_count FROM airports GROUP BY city HAVING COUNT(*) > 1 ORDER BY airport_count DESC, city ASC\n",
      "- SELECT c.name AS country_name, COUNT(r.airline_id) AS route_count FROM routes r JOIN airlines a ON r.airline_id = a.airline_id JOIN countries c ON a.country = c.name GROUP BY c.name ORDER BY route_count DESC, country_name ASC\n",
      "- SELECT ap.name AS airport_name, ap.country, COUNT(r.source_airport_id) + COUNT(r.destination_airport_id) AS total_routes FROM airports ap LEFT JOIN routes r ON ap.airport_id = r.source_airport_id OR ap.airport_id = r.destination_airport_id GROUP BY ap.name, ap.country ORDER BY total_routes DESC, airport_name ASC\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from fireworks import LLM\n",
    "\n",
    "# --- 1. Define Generation Parameters and Pydantic Model ---\n",
    "llm = LLM(model=\"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))  # Use Qwen3-coder for SQL queries\n",
    "TOTAL_QUERIES_TO_GENERATE = 400\n",
    "QUERIES_PER_API_CALL = 10\n",
    "\n",
    "class SqlQueryBatch(BaseModel):\n",
    "    queries: List[str] = Field(description=f\"A list of exactly {QUERIES_PER_API_CALL} unique and diverse SQL queries.\")\n",
    "\n",
    "print(f\"ğŸ¯ Goal: Generate {TOTAL_QUERIES_TO_GENERATE} unique queries in batches of {QUERIES_PER_API_CALL}.\")\n",
    "\n",
    "# --- 2. Get Clean Schema From Synthetic DB ---\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    schema_df = con.sql(\"DESCRIBE;\").df()\n",
    "    schema_for_prompt = schema_df.to_markdown(index=False)\n",
    "\n",
    "# --- 3. Setup Base Prompt and Generation Loop ---\n",
    "base_query_generation_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to generate unique and diverse SQL queries based on the database schema provided.\n",
    "The queries should be realistic and cover a range of complexities and SQL features (JOINS, GROUP BY, aggregates, etc.).\n",
    "Ensure you break ties with ORDER BY clauses so that the same queries produce the same results when executed against the database.\n",
    "Write on the SQL query and nothing else.\n",
    "Ensure the generated SQL is valid for DuckDB.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "all_generated_queries = []\n",
    "# Loop until we have enough queries\n",
    "while len(all_generated_queries) < TOTAL_QUERIES_TO_GENERATE:\n",
    "    print(f\"\\nğŸ“ --- Generating batch #{len(all_generated_queries) // QUERIES_PER_API_CALL + 1} ---\")\n",
    "\n",
    "    # Create a summary of queries generated so far to prevent duplicates\n",
    "    existing_queries_summary = \"\"\n",
    "    if all_generated_queries:\n",
    "        summary_parts = [\"\\nYou have already generated the following queries. Generate NEW, DISTINCT queries that are not on this list and cover different analytic scenarios.\\n\"]\n",
    "        for i, q in enumerate(all_generated_queries):\n",
    "            summary_parts.append(f\"{i+1}. {q}\")\n",
    "        existing_queries_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "    # Construct the final prompt for this iteration\n",
    "    final_prompt = (\n",
    "        base_query_generation_prompt +\n",
    "        existing_queries_summary +\n",
    "        f\"\\n\\nNow, generate {QUERIES_PER_API_CALL} new and unique SQL queries. Return your response as a single JSON object adhering to the specified schema.\"\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SqlQueryBatch\", \"schema\": SqlQueryBatch.model_json_schema()}},\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    if response_content:\n",
    "        try:\n",
    "            new_queries = json.loads(response_content).get(\"queries\", [])\n",
    "            all_generated_queries.extend(new_queries)\n",
    "            print(f\"   - Received {len(new_queries)} new queries. Total now: {len(all_generated_queries)} / {TOTAL_QUERIES_TO_GENERATE}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ ERROR: Failed to parse generated queries in this batch: {e}\")\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 4. Deduplicate, Trim, and Save --- \n",
    "print(\"\\nâœ¨ Generation complete. Deduplicating and saving...\")\n",
    "initial_count = len(all_generated_queries)\n",
    "# Simple, fast deduplication preserving order\n",
    "unique_queries = list(dict.fromkeys(all_generated_queries))\n",
    "final_count = len(unique_queries)\n",
    "print(f\" - Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Trim to the exact number we need\n",
    "final_queries = unique_queries[:TOTAL_QUERIES_TO_GENERATE]\n",
    "\n",
    "# Save the final list to a file\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "with open(QUERIES_FILE_PATH, 'w') as f:\n",
    "    json.dump({\"queries\": final_queries}, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Successfully saved {len(final_queries)} unique queries to `{QUERIES_FILE_PATH}`.\")\n",
    "print(\"\\n--- Here are a few examples: ---\")\n",
    "for query in final_queries[:5]:\n",
    "    print(f\"- {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Execute Queries to Get Ground-Truth Answers\n",
    "Now we will act as the \"system\" and run the queries we just generated against our synthetic sandbox. The output of each query is the **ground-truth result**. During Reinforcement Fine-Tuning, our model will be rewarded if the SQL it writes produces this exact same result.\n",
    "\n",
    "> **Real World ğŸŒ**: You would run your real historical queries against the synthetic database we previously created. The correctness of the data is not a concern here, as our aim is to see what a correct query would have generated, so we can compare it to our LLM's generations during the RFT process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 263 queries to execute.\n",
      "Executing queries against the synthetic database...\n",
      "\n",
      "Execution complete. Success: 263, Failed: 0.\n",
      "\n",
      "âœ… Successfully saved 263 ground-truth results to `data/ground_truth_results.jsonl`.\n",
      "\n",
      "--- Example ground_truth_results dataset entry ---\n",
      "{\n",
      "  \"query\": \"SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\",\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"country\": \"Canada\",\n",
      "      \"airline_count\": 10\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Sweden\",\n",
      "      \"airline_count\": 10\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Kenya\",\n",
      "      \"airline_count\": 9\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United States\",\n",
      "      \"airline_count\": 9\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Australia\",\n",
      "      \"airline_count\": 8\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Spain\",\n",
      "      \"airline_count\": 6\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Italy\",\n",
      "      \"airline_count\": 4\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Switzerland\",\n",
      "      \"airline_count\": 4\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Finland\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"France\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Mexico\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Costa Rica\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Germany\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Iceland\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ireland\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Japan\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Norway\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Singapore\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Kingdom\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Argentina\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Brazil\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"China\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Egypt\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Fiji\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Greece\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"India\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Jordan\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Netherlands\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"New Zealand\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Portugal\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Saudi Arabia\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"South Africa\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Thailand\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Arab Emirates\",\n",
      "      \"airline_count\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "\n",
    "# --- 2. Load Generated Queries ---\n",
    "with open(QUERIES_FILE_PATH, 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "    queries_to_execute = queries_data.get(\"queries\", [])\n",
    "\n",
    "print(f\"Loaded {len(queries_to_execute)} queries to execute.\")\n",
    "\n",
    "# --- 3. Execute Queries and Store Results ---\n",
    "ground_truth_results = []\n",
    "successful_executions = 0\n",
    "failed_executions = 0\n",
    "\n",
    "print(\"Executing queries against the synthetic database...\")\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    for query in queries_to_execute:\n",
    "        try:\n",
    "            # Execute the query and convert the result to a pandas DataFrame\n",
    "            result_df = con.sql(query).df()\n",
    "\n",
    "            # Replace any NaN/NaT values with None, which serializes to JSON `null`\n",
    "            result_df = result_df.astype(object).where(pd.notna(result_df), None)\n",
    "            \n",
    "            result_records = result_df.to_dict('records')\n",
    "            \n",
    "            # Pair the query with its result\n",
    "            ground_truth_results.append({\n",
    "                \"query\": query,\n",
    "                \"result\": result_records\n",
    "            })\n",
    "            successful_executions += 1\n",
    "        except Exception as e:\n",
    "            # The LLM might have occasionally generated a slightly invalid query\n",
    "            print(f\"âš ï¸  Skipping query due to execution error: {query}\\n   Error: {e}\\n\")\n",
    "            failed_executions += 1\n",
    "\n",
    "print(f\"\\nExecution complete. Success: {successful_executions}, Failed: {failed_executions}.\")\n",
    "\n",
    "# --- 4. Save the Ground-Truth Data ---\n",
    "with open(GROUND_TRUTH_FILE_PATH, 'w') as f:\n",
    "    for entry in ground_truth_results:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"\\nâœ… Successfully saved {len(ground_truth_results)} ground-truth results to `{GROUND_TRUTH_FILE_PATH}`.\")\n",
    "\n",
    "# --- 5. Print an Example ---\n",
    "if ground_truth_results:\n",
    "    print(\"\\n--- Example ground_truth_results dataset entry ---\")\n",
    "    print(json.dumps(ground_truth_results[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Natural Language Questions for Final RFT Training Data\n",
    "We now have pairs of `(SQL Query, Ground-Truth Result)`. The final piece missing from our training data is the user's input: a question in natural language. This is because our final goal is to use RFT to tune an LLM to map from a natural language question to a SQL query, having the reward signal be the actual result of the query, rather than just the query itself. This is important because there are many ways to write the same SQL query that yield the same, correct result.\n",
    "\n",
    "Thus, we will use an LLM once again to translate our \"historical\" SQL queries into plausible questions a business user might ask, corresponding to that query. This will yield our final training dataset in the format: `(Natural Language Question, SQL Query, Ground-Truth Result)`. Note that the SQL queries themselves will not be used as part of the RFT job itself, but are useful for debugging our evaluation function (more details in a later section).\n",
    "\n",
    "> **Real World ğŸŒ**: You might not need this step! If you have logs that already link user questions to the queries they ran (e.g., from a BI tool's search bar), you can use those directly. If not, this LLM-based translation is a powerful technique to bootstrap your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 263 query-result pairs.\n",
      "Generating natural language questions and formatting for RFT for 263 queries...\n",
      " - Processing query 1/263...\n",
      " - Processing query 2/263...\n",
      " - Processing query 3/263...\n",
      " - Processing query 4/263...\n",
      " - Processing query 5/263...\n",
      " - Processing query 6/263...\n",
      " - Processing query 7/263...\n",
      " - Processing query 8/263...\n",
      " - Processing query 9/263...\n",
      " - Processing query 10/263...\n",
      " - Processing query 11/263...\n",
      " - Processing query 12/263...\n",
      " - Processing query 13/263...\n",
      " - Processing query 14/263...\n",
      " - Processing query 15/263...\n",
      " - Processing query 16/263...\n",
      " - Processing query 17/263...\n",
      " - Processing query 18/263...\n",
      " - Processing query 19/263...\n",
      " - Processing query 20/263...\n",
      " - Processing query 21/263...\n",
      " - Processing query 22/263...\n",
      " - Processing query 23/263...\n",
      " - Processing query 24/263...\n",
      " - Processing query 25/263...\n",
      " - Processing query 26/263...\n",
      " - Processing query 27/263...\n",
      " - Processing query 28/263...\n",
      " - Processing query 29/263...\n",
      " - Processing query 30/263...\n",
      " - Processing query 31/263...\n",
      " - Processing query 32/263...\n",
      " - Processing query 33/263...\n",
      " - Processing query 34/263...\n",
      " - Processing query 35/263...\n",
      " - Processing query 36/263...\n",
      " - Processing query 37/263...\n",
      " - Processing query 38/263...\n",
      " - Processing query 39/263...\n",
      " - Processing query 40/263...\n",
      " - Processing query 41/263...\n",
      " - Processing query 42/263...\n",
      " - Processing query 43/263...\n",
      " - Processing query 44/263...\n",
      " - Processing query 45/263...\n",
      " - Processing query 46/263...\n",
      " - Processing query 47/263...\n",
      " - Processing query 48/263...\n",
      " - Processing query 49/263...\n",
      " - Processing query 50/263...\n",
      " - Processing query 51/263...\n",
      " - Processing query 52/263...\n",
      " - Processing query 53/263...\n",
      " - Processing query 54/263...\n",
      " - Processing query 55/263...\n",
      " - Processing query 56/263...\n",
      " - Processing query 57/263...\n",
      " - Processing query 58/263...\n",
      " - Processing query 59/263...\n",
      " - Processing query 60/263...\n",
      " - Processing query 61/263...\n",
      " - Processing query 62/263...\n",
      " - Processing query 63/263...\n",
      " - Processing query 64/263...\n",
      " - Processing query 65/263...\n",
      " - Processing query 66/263...\n",
      " - Processing query 67/263...\n",
      " - Processing query 68/263...\n",
      " - Processing query 69/263...\n",
      " - Processing query 70/263...\n",
      " - Processing query 71/263...\n",
      " - Processing query 72/263...\n",
      " - Processing query 73/263...\n",
      " - Processing query 74/263...\n",
      " - Processing query 75/263...\n",
      " - Processing query 76/263...\n",
      " - Processing query 77/263...\n",
      " - Processing query 78/263...\n",
      " - Processing query 79/263...\n",
      " - Processing query 80/263...\n",
      " - Processing query 81/263...\n",
      " - Processing query 82/263...\n",
      " - Processing query 83/263...\n",
      " - Processing query 84/263...\n",
      " - Processing query 85/263...\n",
      " - Processing query 86/263...\n",
      " - Processing query 87/263...\n",
      " - Processing query 88/263...\n",
      " - Processing query 89/263...\n",
      " - Processing query 90/263...\n",
      " - Processing query 91/263...\n",
      " - Processing query 92/263...\n",
      " - Processing query 93/263...\n",
      " - Processing query 94/263...\n",
      " - Processing query 95/263...\n",
      " - Processing query 96/263...\n",
      " - Processing query 97/263...\n",
      " - Processing query 98/263...\n",
      " - Processing query 99/263...\n",
      " - Processing query 100/263...\n",
      " - Processing query 101/263...\n",
      " - Processing query 102/263...\n",
      " - Processing query 103/263...\n",
      " - Processing query 104/263...\n",
      " - Processing query 105/263...\n",
      " - Processing query 106/263...\n",
      " - Processing query 107/263...\n",
      " - Processing query 108/263...\n",
      " - Processing query 109/263...\n",
      " - Processing query 110/263...\n",
      " - Processing query 111/263...\n",
      " - Processing query 112/263...\n",
      " - Processing query 113/263...\n",
      " - Processing query 114/263...\n",
      " - Processing query 115/263...\n",
      " - Processing query 116/263...\n",
      " - Processing query 117/263...\n",
      " - Processing query 118/263...\n",
      " - Processing query 119/263...\n",
      " - Processing query 120/263...\n",
      " - Processing query 121/263...\n",
      " - Processing query 122/263...\n",
      " - Processing query 123/263...\n",
      " - Processing query 124/263...\n",
      " - Processing query 125/263...\n",
      " - Processing query 126/263...\n",
      " - Processing query 127/263...\n",
      " - Processing query 128/263...\n",
      " - Processing query 129/263...\n",
      " - Processing query 130/263...\n",
      " - Processing query 131/263...\n",
      " - Processing query 132/263...\n",
      " - Processing query 133/263...\n",
      " - Processing query 134/263...\n",
      " - Processing query 135/263...\n",
      " - Processing query 136/263...\n",
      " - Processing query 137/263...\n",
      " - Processing query 138/263...\n",
      " - Processing query 139/263...\n",
      " - Processing query 140/263...\n",
      " - Processing query 141/263...\n",
      " - Processing query 142/263...\n",
      " - Processing query 143/263...\n",
      " - Processing query 144/263...\n",
      " - Processing query 145/263...\n",
      " - Processing query 146/263...\n",
      " - Processing query 147/263...\n",
      " - Processing query 148/263...\n",
      " - Processing query 149/263...\n",
      " - Processing query 150/263...\n",
      " - Processing query 151/263...\n",
      " - Processing query 152/263...\n",
      " - Processing query 153/263...\n",
      " - Processing query 154/263...\n",
      " - Processing query 155/263...\n",
      " - Processing query 156/263...\n",
      " - Processing query 157/263...\n",
      " - Processing query 158/263...\n",
      " - Processing query 159/263...\n",
      " - Processing query 160/263...\n",
      " - Processing query 161/263...\n",
      " - Processing query 162/263...\n",
      " - Processing query 163/263...\n",
      " - Processing query 164/263...\n",
      " - Processing query 165/263...\n",
      " - Processing query 166/263...\n",
      " - Processing query 167/263...\n",
      " - Processing query 168/263...\n",
      " - Processing query 169/263...\n",
      " - Processing query 170/263...\n",
      " - Processing query 171/263...\n",
      " - Processing query 172/263...\n",
      " - Processing query 173/263...\n",
      " - Processing query 174/263...\n",
      " - Processing query 175/263...\n",
      " - Processing query 176/263...\n",
      " - Processing query 177/263...\n",
      " - Processing query 178/263...\n",
      " - Processing query 179/263...\n",
      " - Processing query 180/263...\n",
      " - Processing query 181/263...\n",
      " - Processing query 182/263...\n",
      " - Processing query 183/263...\n",
      " - Processing query 184/263...\n",
      " - Processing query 185/263...\n",
      " - Processing query 186/263...\n",
      " - Processing query 187/263...\n",
      " - Processing query 188/263...\n",
      " - Processing query 189/263...\n",
      " - Processing query 190/263...\n",
      " - Processing query 191/263...\n",
      " - Processing query 192/263...\n",
      " - Processing query 193/263...\n",
      " - Processing query 194/263...\n",
      " - Processing query 195/263...\n",
      " - Processing query 196/263...\n",
      " - Processing query 197/263...\n",
      " - Processing query 198/263...\n",
      " - Processing query 199/263...\n",
      " - Processing query 200/263...\n",
      " - Processing query 201/263...\n",
      " - Processing query 202/263...\n",
      " - Processing query 203/263...\n",
      " - Processing query 204/263...\n",
      " - Processing query 205/263...\n",
      " - Processing query 206/263...\n",
      " - Processing query 207/263...\n",
      " - Processing query 208/263...\n",
      " - Processing query 209/263...\n",
      " - Processing query 210/263...\n",
      " - Processing query 211/263...\n",
      " - Processing query 212/263...\n",
      " - Processing query 213/263...\n",
      " - Processing query 214/263...\n",
      " - Processing query 215/263...\n",
      " - Processing query 216/263...\n",
      " - Processing query 217/263...\n",
      " - Processing query 218/263...\n",
      " - Processing query 219/263...\n",
      " - Processing query 220/263...\n",
      " - Processing query 221/263...\n",
      " - Processing query 222/263...\n",
      " - Processing query 223/263...\n",
      " - Processing query 224/263...\n",
      " - Processing query 225/263...\n",
      " - Processing query 226/263...\n",
      " - Processing query 227/263...\n",
      " - Processing query 228/263...\n",
      " - Processing query 229/263...\n",
      " - Processing query 230/263...\n",
      " - Processing query 231/263...\n",
      " - Processing query 232/263...\n",
      " - Processing query 233/263...\n",
      " - Processing query 234/263...\n",
      " - Processing query 235/263...\n",
      " - Processing query 236/263...\n",
      " - Processing query 237/263...\n",
      " - Processing query 238/263...\n",
      " - Processing query 239/263...\n",
      " - Processing query 240/263...\n",
      " - Processing query 241/263...\n",
      " - Processing query 242/263...\n",
      " - Processing query 243/263...\n",
      " - Processing query 244/263...\n",
      " - Processing query 245/263...\n",
      " - Processing query 246/263...\n",
      " - Processing query 247/263...\n",
      " - Processing query 248/263...\n",
      " - Processing query 249/263...\n",
      " - Processing query 250/263...\n",
      " - Processing query 251/263...\n",
      " - Processing query 252/263...\n",
      " - Processing query 253/263...\n",
      " - Processing query 254/263...\n",
      " - Processing query 255/263...\n",
      " - Processing query 256/263...\n",
      " - Processing query 257/263...\n",
      " - Processing query 258/263...\n",
      " - Processing query 259/263...\n",
      " - Processing query 260/263...\n",
      " - Processing query 261/263...\n",
      " - Processing query 262/263...\n",
      " - Processing query 263/263...\n",
      "\n",
      "Generated 263 total examples. Now splitting into train and test sets.\n",
      "Train set size: 210\n",
      "Test set size: 53\n",
      "\n",
      "âœ… Successfully saved training dataset to `data/final_rft_sql_train_data.jsonl`.\n",
      "âœ… Successfully saved test dataset to `data/final_rft_sql_test_data.jsonl`.\n",
      "\n",
      "--- Example RFT training entry ---\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nYou are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\\n\\n**Database Schema:**\\n| database         | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\\n|:-----------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\\n| prod_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\\n|                  |          |           |                                                                            |  'VARCHAR']                                                           |             |\\n| prod_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\\n|                  |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\\n| prod_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| prod_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| prod_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR'   | False       |\\n|                  |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\\n|                  |          |           |  'equipment']                                                              |                                                                       |             |\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Which are the 10 countries with the fewest airports, ordered by the number of airports and then by country name?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"SELECT c.name AS country_name, COUNT(ap.airport_id) AS airport_count FROM countries c JOIN airports ap ON c.name = ap.country GROUP BY c.name ORDER BY airport_count ASC, country_name ASC LIMIT 10\"\n",
      "    }\n",
      "  ],\n",
      "  \"ground_truth\": [\n",
      "    {\n",
      "      \"country_name\": \"Argentina\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Brazil\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"China\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Egypt\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Fiji\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Greece\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"India\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Jordan\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"Netherlands\",\n",
      "      \"airport_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country_name\": \"New Zealand\",\n",
      "      \"airport_count\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import jsonlines\n",
    "from typing import List\n",
    "import random\n",
    "from fireworks import LLM\n",
    "\n",
    "# --- 1. Define File Paths and Parameters ---\n",
    "llm = LLM(model=\"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "FINAL_TRAINING_DATA_PATH = \"data/final_rft_sql_train_data.jsonl\"\n",
    "FINAL_TEST_DATA_PATH = \"data/final_rft_sql_test_data.jsonl\"\n",
    "\n",
    "# --- 2. Load Ground-Truth Data ---\n",
    "query_result_pairs = []\n",
    "with jsonlines.open(GROUND_TRUTH_FILE_PATH) as reader:\n",
    "    for obj in reader:\n",
    "        query_result_pairs.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(query_result_pairs)} query-result pairs.\")\n",
    "\n",
    "# --- 3. Use LLM to Generate Natural Language Questions ---\n",
    "nl_generation_prompt_template = f\"\"\"\n",
    "You are an expert data analyst who is great at translating SQL queries into plain English.\n",
    "Based on the database schema and the provided SQL query, what is a natural language question a business user would ask to get this information?\n",
    "Ensure that the question is precise enough to accurately map to the corresponding SQL query.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\n",
    "**SQL Query:**\n",
    "```sql\n",
    "{{query}}\n",
    "```\n",
    "\n",
    "Provide only the user's question, without any preamble or explanation.\n",
    "\"\"\"\n",
    "\n",
    "# The system prompt that will be included in the final training data for the RFT job.\n",
    "# It gives the model its instructions at inference time.\n",
    "rft_system_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "final_generated_data = []\n",
    "print(f\"Generating natural language questions and formatting for RFT for {len(query_result_pairs)} queries...\")\n",
    "\n",
    "for i, pair in enumerate(query_result_pairs):\n",
    "    print(f\" - Processing query {i+1}/{len(query_result_pairs)}...\")\n",
    "    query = pair['query']\n",
    "    ground_truth = pair['result']\n",
    "    nl_generation_prompt = nl_generation_prompt_template.format(query=query)\n",
    "    \n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": nl_generation_prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    nl_question = response.choices[0].message.content\n",
    "    if nl_question:  # Only include the entry if the LLM generated a question\n",
    "        # Assemble the final data structure\n",
    "        rft_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": rft_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": nl_question.strip()},\n",
    "                {\"role\": \"assistant\", \"content\": query}\n",
    "            ],\n",
    "            \"ground_truth\": ground_truth  # The ground-truth result for the evaluator\n",
    "        }\n",
    "        final_generated_data.append(rft_entry)\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 4. Shuffle and Split the Dataset ---\n",
    "print(f\"\\nGenerated {len(final_generated_data)} total examples. Now splitting into train and test sets.\")\n",
    "random.seed(42)\n",
    "random.shuffle(final_generated_data)\n",
    "\n",
    "split_index = int(len(final_generated_data) * 0.8)\n",
    "train_data = final_generated_data[:split_index]\n",
    "test_data = final_generated_data[split_index:]\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# --- 5. Save the Final RFT-Ready Datasets ---\n",
    "with jsonlines.open(FINAL_TRAINING_DATA_PATH, mode='w') as writer:\n",
    "    writer.write_all(train_data)\n",
    "print(f\"\\nâœ… Successfully saved training dataset to `{FINAL_TRAINING_DATA_PATH}`.\")\n",
    "\n",
    "with jsonlines.open(FINAL_TEST_DATA_PATH, mode='w') as writer:\n",
    "    writer.write_all(test_data)\n",
    "print(f\"âœ… Successfully saved test dataset to `{FINAL_TEST_DATA_PATH}`.\")\n",
    "\n",
    "# --- 6. Print an Example ---\n",
    "if train_data:\n",
    "    print(\"\\n--- Example RFT training entry ---\")\n",
    "    print(json.dumps(train_data[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ğŸ›°ï¸ Deploy an MCP Server for the Synthetic Data\n",
    "Now, we'll start a remote server that speaks the Model Context Protocol (MCP). This server will wrap our synthetic DuckDB database, providing a standardized way for any external toolâ€”in our case, the Fireworks RFT evaluatorâ€”to interact with it.\n",
    "> Real World ğŸŒ: This pattern is directly applicable. You would run a similar MCP server to provide a secure, read-only interface to a production database replica or a data warehouse, allowing the fine-tuning process to happen without granting direct database credentials to the training environment.\n",
    "\n",
    "10. a) Create a server script in this project's root directory (`run_mcp_server.py`). This Python script starts our database server. It is configured to be read-only.\n",
    "\n",
    "```python\n",
    "    import os, contextlib, uvicorn\n",
    "    from starlette.applications import Starlette\n",
    "    from starlette.routing import Mount\n",
    "    from mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n",
    "    from mcp_server_motherduck import build_application\n",
    "\n",
    "    DB = \"data/synthetic_openflights.db\"          # â† path from previous steps\n",
    "    PORT = int(os.environ.get(\"PORT\", 8080))        # Cloud Run injects $PORT\n",
    "\n",
    "    # 1ï¸âƒ£ Build the core SQL-aware MCP server (read-only for safety).\n",
    "    server, _ = build_application(db_path=DB, read_only=True)\n",
    "\n",
    "    # 2ï¸âƒ£ Wrap it so HTTP clients can talk to it (ASGI handler).\n",
    "    sess = StreamableHTTPSessionManager(app=server, event_store=None, stateless=True)\n",
    "\n",
    "    async def handler(scope, receive, send):\n",
    "        await sess.handle_request(scope, receive, send)\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def lifespan(app):\n",
    "        async with sess.run():\n",
    "            yield                                        # keep sessions alive\n",
    "\n",
    "    # 3ï¸âƒ£ Starlette turns that handler into a full ASGI app Uvicorn can serve.\n",
    "    app = Starlette(routes=[Mount(\"/mcp\", app=handler)], lifespan=lifespan)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        print(f\"ğŸ”¥ MCP endpoint â†’ http://0.0.0.0:{PORT}/mcp\")\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. â˜ï¸ Set Up Google Cloud CLI & .gcloudignore\n",
    "We'll first set up the Google Cloud CLI and authenticate.\n",
    "\n",
    "> **Real World ğŸŒ**  \n",
    "> You would follow along here in the same way\n",
    "\n",
    "9. a) **Install** the SDK (macOS/Linux):\n",
    "\n",
    "      ```bash\n",
    "      curl -sSL https://sdk.cloud.google.com | bash\n",
    "      exec -l $SHELL  # reload shell so 'gcloud' is available\n",
    "      ```\n",
    "\n",
    "<br>\n",
    "\n",
    "9. b) **Log in** (creates local access token):\n",
    "      ```bash\n",
    "      gcloud auth login\n",
    "      ```\n",
    "\n",
    "<br>\n",
    "\n",
    "9. c) **Set your active project desired gcloud project**:\n",
    "      ```bash\n",
    "      gcloud config set project < YOUR_PROJECT_ID >  # set up project in gcloud console before running this if not already done\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ğŸ“¦ Containerize & Deploy the MCP Server  \n",
    "Weâ€™ll build a Docker image and push it straight to Cloud Run.  \n",
    "Remember to replace **`YOUR_PROJECT_ID`** with the project you actually want to bill.\n",
    "\n",
    "> **Real World ğŸŒ**  \n",
    "> You would follow along in the same way here.\n",
    "\n",
    "10. a) Create `mcp_requirements.txt` containing the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "mcp\n",
    "mcp-server-motherduck\n",
    "duckdb\n",
    "uvicorn\n",
    "starlette\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. b) Create a `Dockerfile` (no extension) containing the following\n",
    "```bash\n",
    "base\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "\n",
    "COPY mcp_requirements.txt .\n",
    "RUN pip install --no-cache-dir -r mcp_requirements.txt\n",
    "\n",
    "COPY run_mcp_server.py .\n",
    "COPY data/synthetic_openflights.db ./data/\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"python\", \"run_mcp_server.py\"]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. c) Create a .gcloudignore file in your root dir (to only deploy files needed for MCP server) containing:\n",
    "```bash\n",
    "# .gcloudignore\n",
    "\n",
    "# 1. Ignore EVERYTHING in the directory by default.\n",
    "*\n",
    "\n",
    "# 2. Now, create exceptions for ONLY the files needed by the Dockerfile.\n",
    "# The \"!\" character means \"do not ignore this file\".\n",
    "\n",
    "# The Dockerfile itself is needed for the build process.\n",
    "!Dockerfile\n",
    "\n",
    "# The files explicitly copied by your Dockerfile:\n",
    "!mcp_requirements.txt\n",
    "!run_mcp_server.py\n",
    "\n",
    "# 3. To include a specific file in a subdirectory, use this\n",
    "#    three-line pattern to un-ignore the directory, re-ignore its\n",
    "#    contents, and then un-ignore the specific file.\n",
    "!data/\n",
    "data/*\n",
    "!data/synthetic_openflights.db\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. d) Deploy your MCP server as a Cloud Run app by running (from your project root):\n",
    "```bash\n",
    "FIREWORKS_API_KEY=$(grep FIREWORKS_API_KEY .env | cut -d '=' -f2) reward-kit deploy-mcp \\\n",
    "--id mcp-sql-rft-server \\\n",
    "--dockerfile Dockerfile \\\n",
    "--port 8080 \\\n",
    "--gcp-project < YOUR_GCP_PROJECT_ID > \\\n",
    "--gcp-region < YOUR_GCP_REGION >\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. e) Test that your MCP server is working as expected by running the following from your terminal:\n",
    "10. e) i. To get your MCP server's URL:\n",
    "```bash\n",
    "gcloud run services describe mcp-sql-rft-server \\\n",
    "--project < YOUR_GCP_PROJECT_ID > \\\n",
    "--region < YOUR_GCP_REGION > \\\n",
    "--format=\"value(status.url)\"\n",
    "```\n",
    "\n",
    "10. e) ii. (optional) To check the names of the MCP server's available tools:\n",
    "```bash\n",
    "curl -X POST \"< YOUR_MCP_SERVER_URL_FROM_STEP_i >/mcp/\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Accept: application/json, text/event-stream\" \\\n",
    "-d '{\n",
    "    \"id\": \"list-tools-1\",\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"params\": {\n",
    "        \"session\": {\"id\": \"test-from-my-laptop\"}\n",
    "    }\n",
    "}'\n",
    "```\n",
    ">Note that the above is a generally useful way to check an MCP server's tools.\n",
    ">In this case, the tool of interest is the \"query\" tool.\n",
    "\n",
    "10. e) iii. To send a test request to the MCP server:\n",
    "```bash\n",
    "curl -X POST \"< YOUR_MCP_SERVER_URL_FROM_STEP_i >/mcp/\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Accept: application/json, text/event-stream\" \\\n",
    "-d '{\n",
    "    \"id\": \"query-1\",\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"session\": {\"id\": \"test-from-my-laptop\"},\n",
    "        \"name\": \"query\",\n",
    "        \"arguments\": {\n",
    "            \"query\": \"SELECT COUNT(*) FROM airlines;\"\n",
    "        }\n",
    "    }\n",
    "}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ğŸ“¦ Define an evaluation function for RFT\n",
    "Here, we define an `evaluate` function for RFT, which will interface with our MCP server. Note that you will not directly execute the function here, but will use it as part of the Fireworks Evaluations UI.\n",
    "Ensure that you set MCP_SERVER_URL to be your actual MCP server URL from step 10. e) i.\n",
    "\n",
    "> **Real World ğŸŒ**  \n",
    "> You would follow along in the same way here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "MCP_SERVER_URL = None  # <--- PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "\n",
    "def evaluate(messages: list[dict], ground_truth: list[dict], **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates the model's generated SQL query by executing it against a live\n",
    "    MCP server and comparing the result with the ground_truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "        This version robustly handles 'NULL' values and empty strings.\n",
    "        \"\"\"\n",
    "        lines = table_string.strip().split('\\n')\n",
    "        content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "        if len(content_lines) < 2:\n",
    "            return []\n",
    "        \n",
    "        header_raw = [h.strip() for h in content_lines[0].split('|')[1:-1]]\n",
    "        data_lines = content_lines[1:]\n",
    "        \n",
    "        if len(data_lines) > 0:\n",
    "            try:\n",
    "                first_data_values = [v.strip() for v in data_lines[0].split('|')[1:-1]]\n",
    "                if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "                    data_lines = data_lines[1:]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        rows = []\n",
    "        for line in data_lines:\n",
    "            try:\n",
    "                values_raw = [v.strip() for v in line.split('|')[1:-1]]\n",
    "                if len(values_raw) == len(header_raw):\n",
    "                    row_dict = {}\n",
    "                    for i, header in enumerate(header_raw):\n",
    "                        value_str = values_raw[i]\n",
    "                        if value_str.upper() == 'NULL' or value_str == '':\n",
    "                            row_dict[header] = None\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            if '.' in value_str:\n",
    "                                row_dict[header] = float(value_str)\n",
    "                            else:\n",
    "                                row_dict[header] = int(value_str)\n",
    "                        except (ValueError, TypeError):\n",
    "                            row_dict[header] = value_str\n",
    "                    rows.append(row_dict)\n",
    "            except IndexError:\n",
    "                continue\n",
    "        return rows\n",
    "\n",
    "    # --- 1. Get MCP Server URL from Environment Variables ---\n",
    "    mcp_server_url = MCP_SERVER_URL\n",
    "    if not mcp_server_url:\n",
    "        return {\"score\": 0, \"is_score_valid\": False, \"reason\": \"FATAL: MCP_SERVER_URL environment variable is not set.\"}\n",
    "\n",
    "    # --- 2. Get the SQL query from the model's response ---\n",
    "    sql_query = messages[-1]['content'].strip()\n",
    "    if not sql_query:\n",
    "        return {\"score\": 0, \"reason\": \"Model returned an empty response.\"}\n",
    "\n",
    "    # --- 3. Execute the Query against the MCP Server ---\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json, text/event-stream\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\",\n",
    "        \"params\": {\"session\": {\"id\": \"stateless-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": sql_query}}\n",
    "    }\n",
    "    try:\n",
    "        with requests.post(f\"{mcp_server_url}/mcp/\", headers=headers, json=payload, timeout=15, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            response_data = None\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    decoded_line = line.decode('utf-8')\n",
    "                    if decoded_line.startswith('data:'):\n",
    "                        json_part = decoded_line[len('data:'):].strip()\n",
    "                        if json_part:\n",
    "                            response_data = json.loads(json_part)\n",
    "                            break\n",
    "            if response_data is None:\n",
    "                return {\"score\": 0, \"reason\": \"Could not find JSON data in event stream response from MCP server.\"}\n",
    "\n",
    "        if \"error\" in response_data:\n",
    "            return {\"score\": 0, \"reason\": f\"SQL execution failed. Error: {response_data['error'].get('message', 'Unknown')}\"}\n",
    "\n",
    "        ascii_table = response_data['result']['content'][0]['text']\n",
    "        predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"score\": 0, \"reason\": f\"Network error calling MCP server: {e}\"}\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"score\": 0, \"reason\": f\"JSON decode error from server response: {e}\"}\n",
    "    except (KeyError, IndexError):\n",
    "        return {\"score\": 0, \"reason\": f\"Failed to parse predicted result from MCP server response structure. Data found: {json.dumps(response_data)}\"}\n",
    "    except Exception as e:\n",
    "        return {\"score\": 0, \"reason\": f\"An unexpected error occurred during query execution: {e}\"}\n",
    "\n",
    "    # --- 4. Process Ground Truth ---\n",
    "    if not isinstance(ground_truth, list):\n",
    "        return {\"score\": 0, \"is_score_valid\": False, \"reason\": f\"FATAL: ground_truth is not a list as expected. Got type: {type(ground_truth)}\"}\n",
    "    ground_truth_rows = ground_truth\n",
    "\n",
    "\n",
    "    # --- 5. Comparison Logic ---\n",
    "    def normalize_and_stringify(v):\n",
    "        \"\"\"\n",
    "        Normalizes numbers and None before string conversion.\n",
    "        \"\"\"\n",
    "        if v is None:\n",
    "            return str(v)\n",
    "        \n",
    "        if isinstance(v, float) and not math.isinf(v) and not math.isnan(v) and v == int(v):\n",
    "            v = int(v)\n",
    "        return str(v)\n",
    "\n",
    "    try:\n",
    "        gt_values = sorted([sorted(map(normalize_and_stringify, row.values())) for row in ground_truth_rows])\n",
    "        predicted_values = sorted([sorted(map(normalize_and_stringify, row.values())) for row in predicted_rows])\n",
    "\n",
    "        if gt_values == predicted_values:\n",
    "            score = 1\n",
    "            reason = \"Success: The SQL query produced the exact expected result.\"\n",
    "        else:\n",
    "            score = 0\n",
    "            gt_json = json.dumps(ground_truth_rows)\n",
    "            pred_json = json.dumps(predicted_rows)\n",
    "            reason = f\"Incorrect result. Expected (from ground_truth): {gt_json}. Got (from query): {pred_json}.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"score\": 0, \"reason\": f\"Error during result comparison: {e}\"}\n",
    "\n",
    "    return {\"score\": score, \"reason\": reason}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ğŸ§ª Test English -> SQL of a base model without fine-tuning\n",
    "Here, we test a base model's natural language to SQL capability without fine-tuning for a single example.\n",
    "Ensure that you set MCP_SERVER_URL to be your actual MCP server URL from step 10. e) i.\n",
    "\n",
    "> **Real World ğŸŒ**  \n",
    "> You would follow along in the same way here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded row 0 from 'data/final_rft_sql_train_data_v3.jsonl'.\n",
      "\n",
      "====================\n",
      "LLM QUERY GENERATION\n",
      "====================\n",
      "Calling model 'accounts/fireworks/models/llama-v3p1-8b-instruct' to generate SQL query...\n",
      "\n",
      "Model Generated SQL Query:\n",
      "SELECT country, COUNT(*) FROM airlines GROUP BY country ORDER BY COUNT(*) DESC, country ASC\n",
      "\n",
      "====================\n",
      "MCP SERVER EXECUTION\n",
      "====================\n",
      "Sending query to MCP server...\n",
      "\n",
      "Parsed Result from Server:\n",
      "[\n",
      "  {\n",
      "    \"country\": \"Canada\",\n",
      "    \"count_star()\": 10\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Sweden\",\n",
      "    \"count_star()\": 10\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Kenya\",\n",
      "    \"count_star()\": 9\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United States\",\n",
      "    \"count_star()\": 9\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Australia\",\n",
      "    \"count_star()\": 8\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Spain\",\n",
      "    \"count_star()\": 6\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Italy\",\n",
      "    \"count_star()\": 4\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Switzerland\",\n",
      "    \"count_star()\": 4\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Finland\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"France\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Mexico\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Costa Rica\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Germany\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Iceland\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Ireland\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Japan\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Norway\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Singapore\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United Kingdom\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Argentina\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Brazil\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"China\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Egypt\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Fiji\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Greece\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"India\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Jordan\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Netherlands\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"New Zealand\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Portugal\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Saudi Arabia\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"South Africa\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Thailand\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United Arab Emirates\",\n",
      "    \"count_star()\": 1\n",
      "  }\n",
      "]\n",
      "\n",
      "====================\n",
      "COMPARISON\n",
      "====================\n",
      "\n",
      "âœ… GOOD RESULT: The base model generated SQL that produced the correct data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from fireworks import LLM\n",
    "\n",
    "# --- 1. SETUP: Define API keys, server URLs, and the model to use ---\n",
    "\n",
    "# IMPORTANT: Make sure your FIREWORKS_API_KEY is set as an environment variable.\n",
    "# You can get one from https://fireworks.ai\n",
    "if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "    print(\"FATAL: FIREWORKS_API_KEY environment variable not set.\")\n",
    "    # If not set, you can hardcode it here for testing, but this is not recommended:\n",
    "    # os.environ[\"FIREWORKS_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# The model we'll use to generate the SQL. This acts as our \"base\" model.\n",
    "LLM_MODEL = \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n",
    "llm = LLM(model=LLM_MODEL, deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "\n",
    "# The URL for your running MCP server.\n",
    "MCP_SERVER_URL = None  # PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "\n",
    "\n",
    "# --- 2. LOAD THE EXAMPLE DATA ---\n",
    "\n",
    "# This is the example data you provided.\n",
    "DATASET_FILE_PATH = \"data/final_rft_sql_train_data_v3.jsonl\"\n",
    "ROW_INDEX_TO_TEST = 0  # 0 is the first row, 1 is the second row, etc.\n",
    "\n",
    "EXAMPLE_DATA = None\n",
    "try:\n",
    "    with open(DATASET_FILE_PATH, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == ROW_INDEX_TO_TEST:\n",
    "                EXAMPLE_DATA = json.loads(line)\n",
    "                break\n",
    "    \n",
    "    if EXAMPLE_DATA is None:\n",
    "        with open(DATASET_FILE_PATH, 'r') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "        raise IndexError(f\"row index {ROW_INDEX_TO_TEST} is out of bounds for file with {line_count} rows.\")\n",
    "\n",
    "    print(f\"Successfully loaded row {ROW_INDEX_TO_TEST} from '{DATASET_FILE_PATH}'.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load from file. Reason: {e}\")\n",
    "\n",
    "# If loading from file failed for any reason, use the hardcoded fallback data.\n",
    "if EXAMPLE_DATA is None:\n",
    "    print(\"Using hardcoded fallback EXAMPLE_DATA.\\n\")\n",
    "    EXAMPLE_DATA = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\\nYou are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\\n\\n**Database Schema:**\\n| database              | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\\n|:----------------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\\n| synthetic_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\\n|                       |          |           |                                                                            |  'VARCHAR']                                                           |             |\\n| synthetic_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\\n|                       |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\\n| synthetic_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR'   | False       |\\n|                       |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\\n|                       |          |           |  'equipment']                                                              |                                                                       |             |\\n\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which countries have the most airlines, and how many airlines are there in each country, listed in descending order by the number of airlines and then alphabetically by country name?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\"}\n",
    "        ],\n",
    "        \"ground_truth\": [{\"country\": \"Canada\", \"airline_count\": 10}, {\"country\": \"Sweden\", \"airline_count\": 10}, {\"country\": \"Kenya\", \"airline_count\": 9}, {\"country\": \"United States\", \"airline_count\": 9}, {\"country\": \"Australia\", \"airline_count\": 8}, {\"country\": \"Spain\", \"airline_count\": 6}, {\"country\": \"Italy\", \"airline_count\": 4}, {\"country\": \"Switzerland\", \"airline_count\": 4}, {\"country\": \"Finland\", \"airline_count\": 3}, {\"country\": \"France\", \"airline_count\": 3}, {\"country\": \"Mexico\", \"airline_count\": 3}, {\"country\": \"Costa Rica\", \"airline_count\": 2}, {\"country\": \"Germany\", \"airline_count\": 2}, {\"country\": \"Iceland\", \"airline_count\": 2}, {\"country\": \"Ireland\", \"airline_count\": 2}, {\"country\": \"Japan\", \"airline_count\": 2}, {\"country\": \"Norway\", \"airline_count\": 2}, {\"country\": \"Singapore\", \"airline_count\": 2}, {\"country\": \"United Kingdom\", \"airline_count\": 2}, {\"country\": \"Argentina\", \"airline_count\": 1}, {\"country\": \"Brazil\", \"airline_count\": 1}, {\"country\": \"China\", \"airline_count\": 1}, {\"country\": \"Egypt\", \"airline_count\": 1}, {\"country\": \"Fiji\", \"airline_count\": 1}, {\"country\": \"Greece\", \"airline_count\": 1}, {\"country\": \"India\", \"airline_count\": 1}, {\"country\": \"Jordan\", \"airline_count\": 1}, {\"country\": \"Netherlands\", \"airline_count\": 1}, {\"country\": \"New Zealand\", \"airline_count\": 1}, {\"country\": \"Portugal\", \"airline_count\": 1}, {\"country\": \"Saudi Arabia\", \"airline_count\": 1}, {\"country\": \"South Africa\", \"airline_count\": 1}, {\"country\": \"Thailand\", \"airline_count\": 1}, {\"country\": \"United Arab Emirates\", \"airline_count\": 1}]\n",
    "    }\n",
    "\n",
    "# Extract the prompts and ground truth from the data\n",
    "system_prompt = EXAMPLE_DATA[\"messages\"][0][\"content\"]\n",
    "user_prompt = EXAMPLE_DATA[\"messages\"][1][\"content\"]\n",
    "GROUND_TRUTH_ROWS = EXAMPLE_DATA[\"ground_truth\"]\n",
    "\n",
    "# --- 3. HELPER FUNCTION: To parse the server's ASCII table response ---\n",
    "\n",
    "def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "    This is a helper function to handle the specific string output from the MCP server.\n",
    "    \"\"\"\n",
    "    lines = table_string.strip().split('\\n')\n",
    "    content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "    if len(content_lines) < 1:\n",
    "        return []\n",
    "    header_raw = [h.strip() for h in content_lines[0].split('|') if h.strip()]\n",
    "    data_lines = content_lines[1:]\n",
    "    if len(data_lines) > 0:\n",
    "        first_data_values = [v.strip() for v in data_lines[0].split('|') if v.strip()]\n",
    "        if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "            data_lines = data_lines[1:]\n",
    "    rows = []\n",
    "    for line in data_lines:\n",
    "        values_raw = [v.strip() for v in line.split('|') if v.strip()]\n",
    "        if len(values_raw) == len(header_raw):\n",
    "            row_dict = {}\n",
    "            for i, header in enumerate(header_raw):\n",
    "                value_str = values_raw[i]\n",
    "                try:\n",
    "                    if '.' in value_str:\n",
    "                        row_dict[header] = float(value_str)\n",
    "                    else:\n",
    "                        row_dict[header] = int(value_str)\n",
    "                except (ValueError, TypeError):\n",
    "                    row_dict[header] = value_str\n",
    "            rows.append(row_dict)\n",
    "    return rows\n",
    "\n",
    "# --- 4. GENERATE SQL QUERY USING THE LLM ---\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"LLM QUERY GENERATION\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "model_generated_sql = \"\"\n",
    "try:\n",
    "    print(f\"Calling model '{LLM_MODEL}' to generate SQL query...\")\n",
    "    \n",
    "    messages_for_llm = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages_for_llm,\n",
    "        temperature=0.0  # Set to 0 for deterministic output\n",
    "    )\n",
    "    \n",
    "    model_generated_sql = response.choices[0].message.content.strip()\n",
    "    print(\"\\nModel Generated SQL Query:\")\n",
    "    print(model_generated_sql)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nAN ERROR OCCURRED during LLM call: {e}\")\n",
    "\n",
    "\n",
    "# --- 5. EXECUTE GENERATED QUERY ON MCP SERVER ---\n",
    "\n",
    "predicted_rows = []\n",
    "if model_generated_sql:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*20)\n",
    "        print(\"MCP SERVER EXECUTION\")\n",
    "        print(\"=\"*20)\n",
    "        print(f\"Sending query to MCP server...\")\n",
    "        \n",
    "        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json, text/event-stream\"}\n",
    "        payload = {\n",
    "            \"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\",\n",
    "            \"params\": {\"session\": {\"id\": \"stateless-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": model_generated_sql}}\n",
    "        }\n",
    "\n",
    "        with requests.post(f\"{MCP_SERVER_URL}/mcp/\", headers=headers, json=payload, timeout=20, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            response_data = None\n",
    "            for line in response.iter_lines():\n",
    "                if line and line.decode('utf-8').startswith('data:'):\n",
    "                    json_part = line.decode('utf-8')[len('data:'):].strip()\n",
    "                    if json_part:\n",
    "                        response_data = json.loads(json_part)\n",
    "                        break\n",
    "            \n",
    "            if response_data is None: raise RuntimeError(\"No JSON data in event stream.\")\n",
    "            if \"error\" in response_data: raise RuntimeError(f\"SQL Error: {response_data['error'].get('message', 'Unknown')}\")\n",
    "\n",
    "            ascii_table = response_data['result']['content'][0]['text']\n",
    "            predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "            print(\"\\nParsed Result from Server:\")\n",
    "            print(json.dumps(predicted_rows, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED during MCP call: {e}\")\n",
    "\n",
    "# --- 6. FINAL COMPARISON ---\n",
    "print(\"\\n\" + \"=\"*20)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "if not predicted_rows:\n",
    "    print(\"Skipping comparison: no rows returned from query or an error occurred.\")\n",
    "else:\n",
    "    gt_values = sorted([sorted(map(str, row.values())) for row in GROUND_TRUTH_ROWS])\n",
    "    predicted_values = sorted([sorted(map(str, row.values())) for row in predicted_rows])\n",
    "\n",
    "    if gt_values == predicted_values:\n",
    "        print(\"\\nâœ… GOOD RESULT: The base model generated SQL that produced the correct data.\\n\")\n",
    "    else:\n",
    "        print(\"\\nâŒ BAD RESULT: The base model's SQL produced different data than expected.\\n\")\n",
    "        print(\"This is often the intended outcome when testing a base model, as it highlights what fine-tuning needs to correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. ğŸš€ Launch the Fine-Tuning Job & Deploy via the UI\n",
    "Now we'll use the Fireworks AI web interface to take our prepared dataset and fine-tune a model. This process uses your custom `evaluate` function to teach a base model how to generate SQL correctly.\n",
    "\n",
    "> **Real World ğŸŒ**  \n",
    "> This is the core of the RFT process. You're teaching a general-purpose model a very specific and valuable new skill using a powerful, UI-driven workflow. You may follow along as described below\n",
    "\n",
    "As described in the [Fireworks RFT documentation](https://fireworks.ai/docs/fine-tuning/reinforcement-fine-tuning-models), the process involves uploading your data, creating an evaluator, running the job, and deploying.\n",
    "\n",
    "<br>\n",
    "\n",
    "**13. a) Upload Your Dataset**\n",
    "\n",
    "1.  Navigate to the **Datasets** tab in your [Fireworks AI dashboard](https://fireworks.ai/account/datasets).\n",
    "2.  Click **\"Upload Dataset\"**.\n",
    "3.  Upload your training file: `data/final_rft_sql_train_data.jsonl`.\n",
    "4.  Give it a memorable name, like `rft-sql-train-data-v1`, and save it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**13. b) Create the Evaluator**\n",
    "\n",
    "1.  Navigate to the **Evaluations** tab in the dashboard.\n",
    "2.  Click **\"Create Evaluator\"**. This will open the web IDE.\n",
    "3.  In the editor on the left, replace the template code with your full `evaluate` function from step 11 above. This function already contains the logic to connect to your MCP server and compare the results. You just need to add your MCP server URL to the MCP_SERVER_URL line.\n",
    "4.  Save the evaluator with a name like `rft-sql-mcp-evaluator`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**13. c) Launch the Fine-Tuning Job**\n",
    "\n",
    "1.  Navigate to the **Fine-Tuning** tab.\n",
    "2.  Click **\"Fine-Tune a Model\"** and select **Reinforcement**.\n",
    "3.  Configure the job:\n",
    "    *   **Model Selection:** Select a model, for example `accounts/fireworks/models/llama-v3p1-8b-instruct`.\n",
    "    *   **Dataset:** Select the `rft-sql-train-data-v1` you uploaded.\n",
    "    *   **Evaluator:** Select the `rft-sql-mcp-evaluator` you just created.\n",
    "    *   **Rollout:** You can leave these as the default values.\n",
    "    *   **Optional Settings:** You can leave the Model Output Name blank and get the default name, or enter a name of your choosing.\n",
    "4.  You can leave the other hyperparameters as their defaults for the first run.\n",
    "5.  Click **\"Create Job\"**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**13. d) Monitor and Deploy**\n",
    "\n",
    "1.  You can monitor the progress of your job in the **Fine-tuning** tab.\n",
    "2.  Once the job status is `Completed`, you can deploy your model. To deploy, click \"Deploy\" on the top right of your fine-tuning job's page. Please note:\n",
    "    -  The Model under \"Select base model*\" should be the one from your Reinforcement Fine-Tuning job (this should be populated automatically)\n",
    "    -  Speculative decoding is an advanced technique that can improve latency, but is not needed for this use-case\n",
    "    -  Feel free to make the other selections (Performance, Scaling, and Metadata) as needed; enabling autoscaling is recommended to reduce costs\n",
    "3.  Find this new model and click the **Deploy** button to create an API endpoint.\n",
    "\n",
    "<br>\n",
    "\n",
    "**13. e) Test Your New Model!**\n",
    "Once deployed, copy your new model's ID and paste it into the `LLM_MODEL` variable in the testing cell (step #12) to make sure it works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. âš–ï¸ Evaluate Model Performance\n",
    "Now for the moment of truth. We will systematically compare the performance of the original base model against our newly fine-tuned model to quantify the improvement.\n",
    "\n",
    "We'll run both models against every entry in our training dataset (final_rft_sql_train_data_v3.jsonl). For each entry, we will:\n",
    "1. Provide the same system and user prompt to both the base model and the fine-tuned model.\n",
    "2. Capture the SQL query generated by each.\n",
    "3. Execute each query against our live MCP server.\n",
    "4. Compare the query result to the ground_truth from our dataset.\n",
    "5. Keep a running score for each model.\n",
    "\n",
    "This process will give us a clear, data-driven view of how much more accurate our model became after reinforcement fine-tuning.\n",
    "> Real World ğŸŒ\n",
    "> This is a critical step in any MLOps loop. Evaluating a model on a consistent \"test set\" (in this case, we're re-using our training set for simplicity) is the only way to prove that your efforts have resulted in a tangible improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM objects for all three models created successfully.\n",
      "Loaded 53 evaluation examples from 'data/final_rft_sql_test_data.jsonl'.\n",
      "\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [11:05<00:00, 12.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "  EVALUATION COMPLETE\n",
      "=========================\n",
      "Total Examples: 53\n",
      "\n",
      "--- BASE MODEL ---\n",
      "Model ID: accounts/fireworks/models/qwen2p5-7b\n",
      "Correct: 21/53\n",
      "Accuracy: 39.62%\n",
      "\n",
      "--- LARGE BASE MODEL ---\n",
      "Model ID: accounts/fireworks/models/qwen3-235b-a22b-instruct-2507\n",
      "Correct: 20/53\n",
      "Accuracy: 37.74%\n",
      "\n",
      "--- FINE-TUNED MODEL ---\n",
      "Model ID: accounts/pyroworks/models/ft-mdpe6xlm-z9vp7\n",
      "Correct: 28/53\n",
      "Accuracy: 52.83%\n",
      "\n",
      "=========================\n",
      "  PERFORMANCE LIFT\n",
      "=========================\n",
      "Fine-Tuned vs. Base: +13.21%\n",
      "Fine-Tuned vs. Large Base: +15.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from fireworks import LLM\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. SETUP: Define the models to compare, server URL, and dataset path ---\n",
    "\n",
    "# IMPORTANT: Make sure your FIREWORKS_API_KEY is set as an environment variable.\n",
    "if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "    print(\"FATAL: FIREWORKS_API_KEY environment variable not set.\")\n",
    "\n",
    "# The base model you used for the fine-tuning job.\n",
    "BASE_MODEL_ID = \"accounts/fireworks/models/qwen2p5-7b\"  # <--- Replace if you used a different base model\n",
    "LARGE_BASE_MODEL_ID = \"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct\"\n",
    "\n",
    "# IMPORTANT: Replace this with the model ID of your new fine-tuned model.\n",
    "FINE_TUNED_MODEL_ID = \"accounts/<your-account-id>/models/ft-mdpe6xlm-z9vp7\"  # <--- Replace with your fine-tuned model ID\n",
    "\n",
    "MCP_SERVER_URL = None  # <--- PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "DATASET_FILE_PATH = \"data/final_rft_sql_test_data.jsonl\"\n",
    "\n",
    "# --- 2. Create LLM Objects ---\n",
    "base_model_llm = None\n",
    "large_base_model_llm = None\n",
    "fine_tuned_model_llm = None\n",
    "try:\n",
    "    base_model_llm = LLM(model=BASE_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    large_base_model_llm = LLM(model=LARGE_BASE_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    fine_tuned_model_llm = LLM(model=FINE_TUNED_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    print(\"LLM objects for all three models created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not create LLM objects. Error: {e}\")\n",
    "\n",
    "# --- 3. Load Dataset ---\n",
    "dataset = []\n",
    "if all([base_model_llm, large_base_model_llm, fine_tuned_model_llm]):\n",
    "    try:\n",
    "        with open(DATASET_FILE_PATH, 'r') as f:\n",
    "            dataset = [json.loads(line) for line in f]\n",
    "        print(f\"Loaded {len(dataset)} evaluation examples from '{DATASET_FILE_PATH}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not load dataset. Error: {e}\")\n",
    "        dataset = []\n",
    "\n",
    "# --- 4. HELPER AND EVALUATION FUNCTIONS ---\n",
    "\n",
    "def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "    This is a helper function to handle the specific string output from the MCP server.\n",
    "    \"\"\"\n",
    "    lines = table_string.strip().split('\\n')\n",
    "    content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "    if len(content_lines) < 1:\n",
    "        return []\n",
    "    header_raw = [h.strip() for h in content_lines[0].split('|') if h.strip()]\n",
    "    data_lines = content_lines[1:]\n",
    "    if len(data_lines) > 0:\n",
    "        first_data_values = [v.strip() for v in data_lines[0].split('|') if v.strip()]\n",
    "        if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "            data_lines = data_lines[1:]\n",
    "    rows = []\n",
    "    for line in data_lines:\n",
    "        values_raw = [v.strip() for v in line.split('|') if v.strip()]\n",
    "        if len(values_raw) == len(header_raw):\n",
    "            row_dict = {}\n",
    "            for i, header in enumerate(header_raw):\n",
    "                value_str = values_raw[i]\n",
    "                try:\n",
    "                    if '.' in value_str:\n",
    "                        row_dict[header] = float(value_str)\n",
    "                    else:\n",
    "                        row_dict[header] = int(value_str)\n",
    "                except (ValueError, TypeError):\n",
    "                    row_dict[header] = value_str\n",
    "            rows.append(row_dict)\n",
    "    return rows\n",
    "\n",
    "def are_results_equal(predicted_rows: list[dict], ground_truth_rows: list[dict]) -> bool:\n",
    "    \"\"\"\n",
    "    Compares datasets by converting all values to strings and sorting them,\n",
    "    which ignores row order, column order, and data types (e.g., int vs float).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gt_values = sorted([sorted(map(str, row.values())) for row in ground_truth_rows])\n",
    "        predicted_values = sorted([sorted(map(str, row.values())) for row in predicted_rows])\n",
    "        return gt_values == predicted_values\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_sql_and_evaluate(llm_obj, system_prompt: str, user_prompt: str, ground_truth: list[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Calls a pre-configured LLM object to get a SQL query, executes it, and compares to ground truth.\n",
    "    Returns 1 for a correct result, 0 for an incorrect one.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get SQL from the model\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        response = llm_obj.chat.completions.create(messages=messages, temperature=0.0)\n",
    "        sql_query = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Step 2: Execute SQL on MCP server\n",
    "        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json, text/event-stream\"}\n",
    "        payload = {\"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"session\": {\"id\": \"full-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": sql_query}}}\n",
    "\n",
    "        response_data = None\n",
    "        with requests.post(f\"{MCP_SERVER_URL}/mcp/\", headers=headers, json=payload, timeout=30, stream=True) as mcp_response:\n",
    "            mcp_response.raise_for_status()\n",
    "            for line in mcp_response.iter_lines():\n",
    "                if line and line.decode('utf-8').startswith('data:'):\n",
    "                    json_part = line.decode('utf-8')[len('data:'):].strip()\n",
    "                    if json_part:\n",
    "                        response_data = json.loads(json_part)\n",
    "                        break\n",
    "\n",
    "        if response_data is None or \"error\" in response_data:\n",
    "            return 0\n",
    "\n",
    "        # Step 3: Parse and compare results\n",
    "        ascii_table = response_data['result']['content'][0]['text']\n",
    "        predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "\n",
    "        return 1 if are_results_equal(predicted_rows, ground_truth) else 0\n",
    "    except Exception as e:\n",
    "        print(f\"--> Error during evaluation for model {llm_obj.model}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 5. RUN THE FULL EVALUATION ---\n",
    "\n",
    "base_model_score = 0\n",
    "large_base_model_score = 0\n",
    "fine_tuned_model_score = 0\n",
    "\n",
    "if dataset:\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    for item in tqdm(dataset, desc=\"Evaluating models\"):\n",
    "        system_prompt = item[\"messages\"][0][\"content\"]\n",
    "        user_prompt = item[\"messages\"][1][\"content\"]\n",
    "        ground_truth = item[\"ground_truth\"]\n",
    "\n",
    "        # Evaluate base model\n",
    "        base_model_score += get_sql_and_evaluate(base_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)  # Be nice to the API\n",
    "\n",
    "        # Evaluate large base model\n",
    "        large_base_model_score += get_sql_and_evaluate(large_base_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Evaluate fine-tuned model\n",
    "        fine_tuned_model_score += get_sql_and_evaluate(fine_tuned_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 6. REPORT RESULTS ---\n",
    "\n",
    "if dataset:\n",
    "    total = len(dataset)\n",
    "    base_accuracy = (base_model_score / total) * 100\n",
    "    large_base_accuracy = (large_base_model_score / total) * 100\n",
    "    tuned_accuracy = (fine_tuned_model_score / total) * 100\n",
    "\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"  EVALUATION COMPLETE\")\n",
    "    print(\"=\"*25)\n",
    "    print(f\"Total Examples: {total}\\n\")\n",
    "    print(\"--- BASE MODEL ---\")\n",
    "    print(f\"Model ID: {BASE_MODEL_ID}\")\n",
    "    print(f\"Correct: {base_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {base_accuracy:.2f}%\\n\")\n",
    "\n",
    "    print(\"--- LARGE BASE MODEL ---\")\n",
    "    print(f\"Model ID: {LARGE_BASE_MODEL_ID}\")\n",
    "    print(f\"Correct: {large_base_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {large_base_accuracy:.2f}%\\n\")\n",
    "\n",
    "    print(\"--- FINE-TUNED MODEL ---\")\n",
    "    print(f\"Model ID: {FINE_TUNED_MODEL_ID}\")\n",
    "    print(f\"Correct: {fine_tuned_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {tuned_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    print(\"=\"*25)\n",
    "    print(\"  PERFORMANCE LIFT\")\n",
    "    print(\"=\"*25)\n",
    "    print(f\"Fine-Tuned vs. Base: {tuned_accuracy - base_accuracy:+.2f}%\")\n",
    "    print(f\"Fine-Tuned vs. Large Base: {tuned_accuracy - large_base_accuracy:+.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Evaluation skipped because the dataset or LLM objects could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. âœ¨ Cleanup & Conclusion\n",
    "Congratulations! You've successfully completed the entire Reinforcement Fine-Tuning loop. You started with just a database schema and ended with a highly specialized, performant, and data-aware AI model.\n",
    "\n",
    "#### Cleanup\n",
    "Model deployments can incur costs, so it's good practice to clean up any resources you no longer need.\n",
    "\n",
    "*   **Check your Deployments:** Navigate to the [Deployments tab](https://app.fireworks.ai/dashboard/deployments) in your Fireworks AI dashboard. Here you can monitor and manage all your deployed models.\n",
    "*   **Delete Unneeded Models:** Feel free to delete any deployments you no longer need. For example, you might have deployed the base or large-base models during the evaluation step to compare against your fine-tuned model. These can now be safely removed to save costs.\n",
    "\n",
    "You can, of course, continue using your new fine-tuned SQL generation model for any application you see fit!\n",
    "\n",
    "#### Conclusions\n",
    "The evaluation results from the previous step highlight the power of this approach.\n",
    "\n",
    "*   **Performance on par with massive models:** Our fine-tuned 7B parameter model performs on par with a much larger model like `qwen3-coder-480b-a35b-instruct` on this specific dataset. This is because it has been fine-tuned to understand the data schema via real query generation and execution.\n",
    "*   **Efficiency Gains:** This specialized 7B model is significantly faster and cheaper to run than its 480B counterpart, offering production-grade performance at a fraction of the cost and latency.\n",
    "*   **High-Level Capability on Complex Tasks:** The queries in this dataset are relatively complex, which is reflected in the final accuracy score of around 60%. This is a strong result, demonstrating that for a specialized domain, a smaller model can be tuned to achieve a level of performance comparable to elite, state-of-the-art coding models like Claude 4 Sonnet.\n",
    "\n",
    "---\n",
    "\n",
    "Throughout this tutorial, we demonstrated a complete, end-to-end workflow for creating a fine-tuned text-to-SQL model. We began with the absolute minimum requirement, a database schema, and used a series of LLM-driven steps to generate a safe, synthetic data sandbox. From there, we generated a rich dataset of queries and answers, which we used to fine-tune a model using the Fireworks RFT platform. The final result is a small, efficient model that can accurately query data it has never seen, a task that was previously only possible with vastly larger and more expensive models.\n",
    "\n",
    "This pattern of **schema â†’ synthetic data â†’ RFT** is a secure, effective, and repeatable methodology for teaching language models to become expert users of your private data and custom APIs, without ever exposing the underlying sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
