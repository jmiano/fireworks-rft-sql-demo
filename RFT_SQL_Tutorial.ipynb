{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úàÔ∏è Natural-Language ‚Üí SQL with Reinforcement-Fine-Tuning (RFT)\n",
    "Welcome! This tutorial will show you how to fine-tune a 7B parameter model to answer natural language (NL) questions by writing SQL to execute against your database, without using real production data in fine-tuning the process.\n",
    "\n",
    "### Why this matters  \n",
    "Off-the-shelf LLM copilots often guess column names, ignore schema quirks, or hallucinate tables. **Reinforcement Fine-Tuning (RFT)** fixes this by teaching the model the shape of your data _and_ the patterns in your queries, boosting exact-match accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## In this tutorial you will\n",
    "\n",
    "| You‚Äôll practice ‚Ä¶ | ‚Ä¶ and walk away with |\n",
    "| --- | --- |\n",
    "| ‚úÖ **Generate a synthetic DuckDB** that mirrors your schema | `synthetic_openflights.db` (<20 MB) served via an MCP endpoint |\n",
    "| ‚úÖ **Create a MECE query set** & compute ground-truth rows | `generated_queries.json` & `ground_truth_results.json` |\n",
    "| ‚úÖ **Build NL ‚Üî SQL result pairs** for fine-tuning and eval | `final_rft_sql_train_data.jsonl` & `final_rft_sql_test_data.jsonl` |\n",
    "| ‚úÖ **Run an RFT job on Fireworks AI** | A tuned **Qwen 2.5-7B** checkpoint |\n",
    "| ‚úÖ **Benchmark baseline vs. tuned model** and a larger baseline | > 20% exact-match improvement over baseline and on-par with SoTA base models |\n",
    "\n",
    "<br>\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. üõ†Ô∏è Development Environment Setup\n",
    "2. üóÑÔ∏è Simulate the \"Production\" Database  \n",
    "3. üìã Acquire the Schema (No Real Data!)\n",
    "4. üß™ Create the Synthetic Training Sandbox with an LLM\n",
    "5. ‚úÖ Validate the Sandbox\n",
    "6. üìù Generate Example SQL Queries\n",
    "7. üéØ Execute Queries to Get Ground-Truth Answers\n",
    "8. üí¨ Generate Natural Language Questions for Final RFT Training Data\n",
    "9. üõ∞Ô∏è Deploy an MCP Server for the Synthetic Data\n",
    "10. ‚òÅÔ∏è Set Up Google Cloud CLI & .gcloudignore\n",
    "11. üì¶ Containerize & Deploy the MCP Server\n",
    "12. üîç Define an evaluation function for RFT\n",
    "13. üß™ Test English -> SQL of a base model without fine-tuning\n",
    "14. üöÄ Launch the Fine-Tuning Job & Deploy via the UI\n",
    "15. ‚öñÔ∏è Evaluate Model Performance\n",
    "16. ‚ú® Cleanup & Conclusion\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Demo vs Real World üåç**  \n",
    "> Look for these call-outs to see the difference between the self-contained demo steps in this notebook and the equivalent actions you‚Äôd perform on your own private schema, logs, and query store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. üõ†Ô∏è Development Environment Setup\n",
    "**Complete these steps once in your terminal, *outside* this notebook.**\n",
    "\n",
    "1.  **Get a Fireworks AI API Key**\n",
    "    - Go to [fireworks.ai](https://fireworks.ai) and sign up.\n",
    "    - Create an API key from your settings page.\n",
    "    - Create a file named `.env` in your project directory and add your key:\n",
    "      ```\n",
    "      FIREWORKS_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "      ```\n",
    "\n",
    "2.  **Install `uv`**\n",
    "    - `uv` is a fast Python package manager from Astral. Follow the official installation instructions at [docs.astral.sh/uv/](https://docs.astral.sh/uv/).\n",
    "    - It's significantly faster than pip and handles dependency resolution more reliably.\n",
    "\n",
    "3.  **Create a Virtual Environment and Install Packages**\n",
    "    - Once `uv` is installed, create and activate a virtual environment.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv venv .venv -- python3.12\n",
    "    source .venv/bin/activate  # On Windows PowerShell: .venv\\Scripts\\Activate.ps1\n",
    "    ```\n",
    "    - Install all required packages using `uv add`.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv add duckdb tabulate pandas pyarrow requests \\\n",
    "           pydantic python-dotenv \\\n",
    "           jsonlines fireworks-ai \\\n",
    "           mcp-sdk mcp-server-motherduck\n",
    "    ```\n",
    "After running these commands, your environment is ready. You can proceed with the cells inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. üóÑÔ∏è Simulate the \"Production\" Database\n",
    "First, we'll create a database that represents your real, populated production database. We'll download the public OpenFlights dataset and load it into a DuckDB file.\n",
    "\n",
    "#### What is DuckDB?\n",
    "DuckDB is an in-process SQL OLAP database management system. Think of it as \"SQLite for analytics\". It's perfect for this tutorial because:\n",
    "- It's embedded (no server setup required)\n",
    "- It's fast for analytical queries\n",
    "- It has excellent SQL compatibility\n",
    "- The entire database is just a single file\n",
    "- It has an existing MCP server we can use ([mcp-server-motherduck](https://github.com/motherduckdb/mcp-server-motherduck))\n",
    "\n",
    "> **Real World üåç**: You already have this! It's your live production database (or a replica). You would skip this entire step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 'Production' database simulated at: data/prod_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# --- Download the raw data files ---\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "BASE_URL = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/\"\n",
    "FILES_TO_DOWNLOAD = {\n",
    "    \"airports\": \"airports.dat\",\n",
    "    \"airlines\": \"airlines.dat\",\n",
    "    \"routes\": \"routes.dat\",\n",
    "    \"countries\": \"countries.dat\",\n",
    "    \"planes\": \"planes.dat\"\n",
    "}\n",
    "# Define column names as the files don't have headers\n",
    "COLUMN_NAMES = {\n",
    "    \"airports\": [\"airport_id\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"],\n",
    "    \"airlines\": [\"airline_id\", \"name\", \"alias\", \"iata\", \"icao\", \"callsign\", \"country\", \"active\"],\n",
    "    \"routes\": [\"airline\", \"airline_id\", \"source_airport\", \"source_airport_id\", \"destination_airport\", \"destination_airport_id\", \"codeshare\", \"stops\", \"equipment\"],\n",
    "    \"countries\": [\"name\", \"iso_code\", \"dafif_code\"],\n",
    "    \"planes\": [\"name\", \"iata\", \"icao\"]\n",
    "}\n",
    "\n",
    "PROD_DB_PATH = \"data/prod_openflights.db\"\n",
    "\n",
    "# --- Load the real data into our \"production\" DuckDB ---\n",
    "with duckdb.connect(PROD_DB_PATH) as con:\n",
    "    for name, filename in FILES_TO_DOWNLOAD.items():\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            print(f\"‚úÖ Downloaded: {path}\")\n",
    "\n",
    "        # Load data using pandas to handle missing headers and null values\n",
    "        df = pd.read_csv(path, header=None, names=COLUMN_NAMES[name], na_values=[\"\\\\N\"])\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE {name} AS SELECT * FROM df\")\n",
    "\n",
    "    print(f\"\\n‚úÖ 'Production' database simulated at: {PROD_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. üìã Acquire the Schema (No Real Data!)\n",
    "This is a critical step. We connect to our \"production\" database and extract **only its schema** (the table structure, column names, and data types). We do not touch or read any of the data rows. This schema is the only artifact we need from the production environment.\n",
    "\n",
    "#### Why Schema-Only?\n",
    "This approach is powerful because:\n",
    "- **Privacy**: No actual customer data leaves your production environment\n",
    "- **Security**: No risk of exposing sensitive data during fine-tuning\n",
    "- **Efficiency**: Schema information is tiny compared to actual data\n",
    "\n",
    "The `DESCRIBE` command in DuckDB gives us comprehensive schema information without accessing any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema successfully extracted from 'production' database:\n",
      "| database         | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\n",
      "|:-----------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\n",
      "| prod_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\n",
      "|                  |          |           |                                                                            |  'VARCHAR']                                                           |             |\n",
      "| prod_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\n",
      "|                  |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\n",
      "| prod_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR'   | False       |\n",
      "|                  |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\n",
      "|                  |          |           |  'equipment']                                                              |                                                                       |             |\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the \"production\" database we just created\n",
    "with duckdb.connect(PROD_DB_PATH, read_only=True) as con:\n",
    "    # The DESCRIBE command gives us the schema information for all tables\n",
    "    schema_df = con.sql(\"DESCRIBE;\").df()\n",
    "\n",
    "print(\"‚úÖ Schema successfully extracted from 'production' database:\")\n",
    "print(schema_df.to_markdown(index=False))\n",
    "\n",
    "# We can also store this for later use in prompts\n",
    "schema_for_prompt = schema_df.to_markdown(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. üß™ Create the Synthetic Training Sandbox with an LLM\n",
    "Now that we have the schema, we will use a large language model to generate a complete, contextually-aware synthetic dataset.\n",
    "\n",
    "#### Key Concepts in This Step:\n",
    "\n",
    "**Dynamic Pydantic Model Generation**: We dynamically create Pydantic models based on your database schema. This ensures the LLM's output is structured and parseable, adapting to any database schema automatically.\n",
    "\n",
    "**Chunked Generation Strategy**: Instead of asking for all data at once (which could overwhelm the LLM or hit token limits), we generate data in small chunks of 2 rows per API call. This approach:\n",
    "- Ensures high-quality, coherent data\n",
    "- Avoids token limit issues\n",
    "\n",
    "**Contextual Awareness**: Each generation request includes previously generated data as context, preventing duplicates and ensuring variety.\n",
    "\n",
    "To fine-tune our model with RFT, **we will only interact with this synthetic database.**\n",
    "\n",
    "> **Real World üåç**: This pattern is directly applicable. You would use the same approach with your production schema to generate synthetic data that maintains the statistical properties and relationships of your real data without exposing any actual records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically created Pydantic models for all tables.\n",
      "\n",
      "‚úÖ Data Generation Plan:\n",
      " - Target rows per table: 400\n",
      " - Will make API calls asking for 4 rows/call until target is met.\n",
      "\n",
      "üìû --- Generating data chunk #1 ---\n",
      "‚úÖ Received and parsed chunk #1.\n",
      "   - 'airlines': 4 / 400 rows\n",
      "   - 'airports': 4 / 400 rows\n",
      "   - 'countries': 4 / 400 rows\n",
      "   - 'planes': 4 / 400 rows\n",
      "   - 'routes': 4 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #2 ---\n",
      "‚úÖ Received and parsed chunk #2.\n",
      "   - 'airlines': 8 / 400 rows\n",
      "   - 'airports': 8 / 400 rows\n",
      "   - 'countries': 8 / 400 rows\n",
      "   - 'planes': 8 / 400 rows\n",
      "   - 'routes': 8 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #3 ---\n",
      "‚úÖ Received and parsed chunk #3.\n",
      "   - 'airlines': 12 / 400 rows\n",
      "   - 'airports': 12 / 400 rows\n",
      "   - 'countries': 12 / 400 rows\n",
      "   - 'planes': 12 / 400 rows\n",
      "   - 'routes': 12 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #4 ---\n",
      "‚úÖ Received and parsed chunk #4.\n",
      "   - 'airlines': 16 / 400 rows\n",
      "   - 'airports': 16 / 400 rows\n",
      "   - 'countries': 16 / 400 rows\n",
      "   - 'planes': 16 / 400 rows\n",
      "   - 'routes': 16 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #5 ---\n",
      "‚úÖ Received and parsed chunk #5.\n",
      "   - 'airlines': 20 / 400 rows\n",
      "   - 'airports': 20 / 400 rows\n",
      "   - 'countries': 20 / 400 rows\n",
      "   - 'planes': 20 / 400 rows\n",
      "   - 'routes': 20 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #6 ---\n",
      "‚úÖ Received and parsed chunk #6.\n",
      "   - 'airlines': 24 / 400 rows\n",
      "   - 'airports': 24 / 400 rows\n",
      "   - 'countries': 24 / 400 rows\n",
      "   - 'planes': 24 / 400 rows\n",
      "   - 'routes': 24 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #7 ---\n",
      "‚úÖ Received and parsed chunk #7.\n",
      "   - 'airlines': 28 / 400 rows\n",
      "   - 'airports': 28 / 400 rows\n",
      "   - 'countries': 28 / 400 rows\n",
      "   - 'planes': 28 / 400 rows\n",
      "   - 'routes': 28 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #8 ---\n",
      "‚úÖ Received and parsed chunk #8.\n",
      "   - 'airlines': 32 / 400 rows\n",
      "   - 'airports': 32 / 400 rows\n",
      "   - 'countries': 32 / 400 rows\n",
      "   - 'planes': 32 / 400 rows\n",
      "   - 'routes': 32 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #9 ---\n",
      "‚úÖ Received and parsed chunk #9.\n",
      "   - 'airlines': 36 / 400 rows\n",
      "   - 'airports': 36 / 400 rows\n",
      "   - 'countries': 36 / 400 rows\n",
      "   - 'planes': 36 / 400 rows\n",
      "   - 'routes': 36 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #10 ---\n",
      "‚úÖ Received and parsed chunk #10.\n",
      "   - 'airlines': 40 / 400 rows\n",
      "   - 'airports': 40 / 400 rows\n",
      "   - 'countries': 40 / 400 rows\n",
      "   - 'planes': 40 / 400 rows\n",
      "   - 'routes': 40 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #11 ---\n",
      "‚úÖ Received and parsed chunk #11.\n",
      "   - 'airlines': 44 / 400 rows\n",
      "   - 'airports': 44 / 400 rows\n",
      "   - 'countries': 44 / 400 rows\n",
      "   - 'planes': 44 / 400 rows\n",
      "   - 'routes': 44 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #12 ---\n",
      "‚úÖ Received and parsed chunk #12.\n",
      "   - 'airlines': 48 / 400 rows\n",
      "   - 'airports': 48 / 400 rows\n",
      "   - 'countries': 48 / 400 rows\n",
      "   - 'planes': 48 / 400 rows\n",
      "   - 'routes': 48 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #13 ---\n",
      "‚úÖ Received and parsed chunk #13.\n",
      "   - 'airlines': 52 / 400 rows\n",
      "   - 'airports': 52 / 400 rows\n",
      "   - 'countries': 52 / 400 rows\n",
      "   - 'planes': 52 / 400 rows\n",
      "   - 'routes': 52 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #14 ---\n",
      "‚úÖ Received and parsed chunk #14.\n",
      "   - 'airlines': 56 / 400 rows\n",
      "   - 'airports': 56 / 400 rows\n",
      "   - 'countries': 56 / 400 rows\n",
      "   - 'planes': 56 / 400 rows\n",
      "   - 'routes': 56 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #15 ---\n",
      "‚úÖ Received and parsed chunk #15.\n",
      "   - 'airlines': 60 / 400 rows\n",
      "   - 'airports': 60 / 400 rows\n",
      "   - 'countries': 60 / 400 rows\n",
      "   - 'planes': 60 / 400 rows\n",
      "   - 'routes': 60 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #16 ---\n",
      "‚úÖ Received and parsed chunk #16.\n",
      "   - 'airlines': 64 / 400 rows\n",
      "   - 'airports': 64 / 400 rows\n",
      "   - 'countries': 64 / 400 rows\n",
      "   - 'planes': 64 / 400 rows\n",
      "   - 'routes': 64 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #17 ---\n",
      "‚úÖ Received and parsed chunk #17.\n",
      "   - 'airlines': 68 / 400 rows\n",
      "   - 'airports': 68 / 400 rows\n",
      "   - 'countries': 68 / 400 rows\n",
      "   - 'planes': 68 / 400 rows\n",
      "   - 'routes': 68 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #18 ---\n",
      "‚úÖ Received and parsed chunk #18.\n",
      "   - 'airlines': 72 / 400 rows\n",
      "   - 'airports': 72 / 400 rows\n",
      "   - 'countries': 72 / 400 rows\n",
      "   - 'planes': 72 / 400 rows\n",
      "   - 'routes': 72 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #19 ---\n",
      "‚úÖ Received and parsed chunk #19.\n",
      "   - 'airlines': 76 / 400 rows\n",
      "   - 'airports': 76 / 400 rows\n",
      "   - 'countries': 76 / 400 rows\n",
      "   - 'planes': 76 / 400 rows\n",
      "   - 'routes': 76 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #20 ---\n",
      "‚úÖ Received and parsed chunk #20.\n",
      "   - 'airlines': 80 / 400 rows\n",
      "   - 'airports': 80 / 400 rows\n",
      "   - 'countries': 80 / 400 rows\n",
      "   - 'planes': 80 / 400 rows\n",
      "   - 'routes': 80 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #21 ---\n",
      "‚úÖ Received and parsed chunk #21.\n",
      "   - 'airlines': 84 / 400 rows\n",
      "   - 'airports': 84 / 400 rows\n",
      "   - 'countries': 84 / 400 rows\n",
      "   - 'planes': 84 / 400 rows\n",
      "   - 'routes': 84 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #22 ---\n",
      "‚úÖ Received and parsed chunk #22.\n",
      "   - 'airlines': 88 / 400 rows\n",
      "   - 'airports': 88 / 400 rows\n",
      "   - 'countries': 88 / 400 rows\n",
      "   - 'planes': 88 / 400 rows\n",
      "   - 'routes': 88 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #23 ---\n",
      "‚úÖ Received and parsed chunk #23.\n",
      "   - 'airlines': 92 / 400 rows\n",
      "   - 'airports': 92 / 400 rows\n",
      "   - 'countries': 92 / 400 rows\n",
      "   - 'planes': 92 / 400 rows\n",
      "   - 'routes': 92 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #24 ---\n",
      "‚úÖ Received and parsed chunk #24.\n",
      "   - 'airlines': 96 / 400 rows\n",
      "   - 'airports': 96 / 400 rows\n",
      "   - 'countries': 96 / 400 rows\n",
      "   - 'planes': 96 / 400 rows\n",
      "   - 'routes': 96 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #25 ---\n",
      "‚úÖ Received and parsed chunk #25.\n",
      "   - 'airlines': 100 / 400 rows\n",
      "   - 'airports': 100 / 400 rows\n",
      "   - 'countries': 100 / 400 rows\n",
      "   - 'planes': 100 / 400 rows\n",
      "   - 'routes': 100 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #26 ---\n",
      "‚úÖ Received and parsed chunk #26.\n",
      "   - 'airlines': 104 / 400 rows\n",
      "   - 'airports': 104 / 400 rows\n",
      "   - 'countries': 104 / 400 rows\n",
      "   - 'planes': 104 / 400 rows\n",
      "   - 'routes': 104 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #27 ---\n",
      "‚úÖ Received and parsed chunk #27.\n",
      "   - 'airlines': 108 / 400 rows\n",
      "   - 'airports': 108 / 400 rows\n",
      "   - 'countries': 108 / 400 rows\n",
      "   - 'planes': 108 / 400 rows\n",
      "   - 'routes': 108 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #28 ---\n",
      "‚úÖ Received and parsed chunk #28.\n",
      "   - 'airlines': 112 / 400 rows\n",
      "   - 'airports': 112 / 400 rows\n",
      "   - 'countries': 112 / 400 rows\n",
      "   - 'planes': 112 / 400 rows\n",
      "   - 'routes': 112 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #29 ---\n",
      "‚úÖ Received and parsed chunk #29.\n",
      "   - 'airlines': 116 / 400 rows\n",
      "   - 'airports': 116 / 400 rows\n",
      "   - 'countries': 116 / 400 rows\n",
      "   - 'planes': 116 / 400 rows\n",
      "   - 'routes': 116 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #30 ---\n",
      "‚úÖ Received and parsed chunk #30.\n",
      "   - 'airlines': 120 / 400 rows\n",
      "   - 'airports': 120 / 400 rows\n",
      "   - 'countries': 120 / 400 rows\n",
      "   - 'planes': 120 / 400 rows\n",
      "   - 'routes': 120 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #31 ---\n",
      "‚úÖ Received and parsed chunk #31.\n",
      "   - 'airlines': 124 / 400 rows\n",
      "   - 'airports': 124 / 400 rows\n",
      "   - 'countries': 124 / 400 rows\n",
      "   - 'planes': 124 / 400 rows\n",
      "   - 'routes': 124 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #32 ---\n",
      "‚úÖ Received and parsed chunk #32.\n",
      "   - 'airlines': 128 / 400 rows\n",
      "   - 'airports': 128 / 400 rows\n",
      "   - 'countries': 128 / 400 rows\n",
      "   - 'planes': 128 / 400 rows\n",
      "   - 'routes': 128 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #33 ---\n",
      "‚úÖ Received and parsed chunk #33.\n",
      "   - 'airlines': 132 / 400 rows\n",
      "   - 'airports': 132 / 400 rows\n",
      "   - 'countries': 132 / 400 rows\n",
      "   - 'planes': 132 / 400 rows\n",
      "   - 'routes': 132 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #34 ---\n",
      "‚úÖ Received and parsed chunk #34.\n",
      "   - 'airlines': 136 / 400 rows\n",
      "   - 'airports': 136 / 400 rows\n",
      "   - 'countries': 136 / 400 rows\n",
      "   - 'planes': 136 / 400 rows\n",
      "   - 'routes': 136 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #35 ---\n",
      "‚úÖ Received and parsed chunk #35.\n",
      "   - 'airlines': 140 / 400 rows\n",
      "   - 'airports': 140 / 400 rows\n",
      "   - 'countries': 140 / 400 rows\n",
      "   - 'planes': 140 / 400 rows\n",
      "   - 'routes': 140 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #36 ---\n",
      "‚úÖ Received and parsed chunk #36.\n",
      "   - 'airlines': 144 / 400 rows\n",
      "   - 'airports': 144 / 400 rows\n",
      "   - 'countries': 144 / 400 rows\n",
      "   - 'planes': 144 / 400 rows\n",
      "   - 'routes': 144 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #37 ---\n",
      "‚úÖ Received and parsed chunk #37.\n",
      "   - 'airlines': 148 / 400 rows\n",
      "   - 'airports': 148 / 400 rows\n",
      "   - 'countries': 148 / 400 rows\n",
      "   - 'planes': 148 / 400 rows\n",
      "   - 'routes': 148 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #38 ---\n",
      "‚úÖ Received and parsed chunk #38.\n",
      "   - 'airlines': 152 / 400 rows\n",
      "   - 'airports': 152 / 400 rows\n",
      "   - 'countries': 152 / 400 rows\n",
      "   - 'planes': 152 / 400 rows\n",
      "   - 'routes': 152 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #39 ---\n",
      "‚úÖ Received and parsed chunk #39.\n",
      "   - 'airlines': 156 / 400 rows\n",
      "   - 'airports': 156 / 400 rows\n",
      "   - 'countries': 156 / 400 rows\n",
      "   - 'planes': 156 / 400 rows\n",
      "   - 'routes': 156 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #40 ---\n",
      "‚úÖ Received and parsed chunk #40.\n",
      "   - 'airlines': 160 / 400 rows\n",
      "   - 'airports': 160 / 400 rows\n",
      "   - 'countries': 160 / 400 rows\n",
      "   - 'planes': 160 / 400 rows\n",
      "   - 'routes': 160 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #41 ---\n",
      "‚úÖ Received and parsed chunk #41.\n",
      "   - 'airlines': 164 / 400 rows\n",
      "   - 'airports': 164 / 400 rows\n",
      "   - 'countries': 164 / 400 rows\n",
      "   - 'planes': 164 / 400 rows\n",
      "   - 'routes': 164 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #42 ---\n",
      "‚úÖ Received and parsed chunk #42.\n",
      "   - 'airlines': 168 / 400 rows\n",
      "   - 'airports': 168 / 400 rows\n",
      "   - 'countries': 168 / 400 rows\n",
      "   - 'planes': 168 / 400 rows\n",
      "   - 'routes': 168 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #43 ---\n",
      "‚úÖ Received and parsed chunk #43.\n",
      "   - 'airlines': 172 / 400 rows\n",
      "   - 'airports': 172 / 400 rows\n",
      "   - 'countries': 172 / 400 rows\n",
      "   - 'planes': 172 / 400 rows\n",
      "   - 'routes': 172 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #44 ---\n",
      "‚úÖ Received and parsed chunk #44.\n",
      "   - 'airlines': 176 / 400 rows\n",
      "   - 'airports': 176 / 400 rows\n",
      "   - 'countries': 176 / 400 rows\n",
      "   - 'planes': 176 / 400 rows\n",
      "   - 'routes': 176 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #45 ---\n",
      "‚úÖ Received and parsed chunk #45.\n",
      "   - 'airlines': 180 / 400 rows\n",
      "   - 'airports': 180 / 400 rows\n",
      "   - 'countries': 180 / 400 rows\n",
      "   - 'planes': 180 / 400 rows\n",
      "   - 'routes': 180 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #46 ---\n",
      "‚úÖ Received and parsed chunk #46.\n",
      "   - 'airlines': 184 / 400 rows\n",
      "   - 'airports': 184 / 400 rows\n",
      "   - 'countries': 184 / 400 rows\n",
      "   - 'planes': 184 / 400 rows\n",
      "   - 'routes': 184 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #47 ---\n",
      "‚úÖ Received and parsed chunk #47.\n",
      "   - 'airlines': 188 / 400 rows\n",
      "   - 'airports': 188 / 400 rows\n",
      "   - 'countries': 188 / 400 rows\n",
      "   - 'planes': 188 / 400 rows\n",
      "   - 'routes': 188 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #48 ---\n",
      "‚úÖ Received and parsed chunk #48.\n",
      "   - 'airlines': 192 / 400 rows\n",
      "   - 'airports': 192 / 400 rows\n",
      "   - 'countries': 192 / 400 rows\n",
      "   - 'planes': 192 / 400 rows\n",
      "   - 'routes': 192 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #49 ---\n",
      "‚úÖ Received and parsed chunk #49.\n",
      "   - 'airlines': 196 / 400 rows\n",
      "   - 'airports': 196 / 400 rows\n",
      "   - 'countries': 196 / 400 rows\n",
      "   - 'planes': 196 / 400 rows\n",
      "   - 'routes': 196 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #50 ---\n",
      "‚úÖ Received and parsed chunk #50.\n",
      "   - 'airlines': 200 / 400 rows\n",
      "   - 'airports': 200 / 400 rows\n",
      "   - 'countries': 200 / 400 rows\n",
      "   - 'planes': 200 / 400 rows\n",
      "   - 'routes': 200 / 400 rows\n",
      "\n",
      "üìû --- Generating data chunk #51 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pydantic import create_model, BaseModel\n",
    "from fireworks import LLM\n",
    "import duckdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Any, Dict, Type\n",
    "import datetime\n",
    "import decimal\n",
    "import uuid\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "TARGET_ROW_COUNT = 400  # The number of rows to generate for each table.\n",
    "\n",
    "# --- 1. Dynamically Create Pydantic Models from the SQL Schema ---\n",
    "def map_sql_type_to_python(sql_type: str) -> Type:\n",
    "    \"\"\"Maps SQL data types to Python types for Pydantic models.\"\"\"\n",
    "    sql_type_upper = str(sql_type).upper()\n",
    "    if 'DECIMAL' in sql_type_upper: return decimal.Decimal\n",
    "    if 'DOUBLE' in sql_type_upper or 'FLOAT' in sql_type_upper or 'REAL' in sql_type_upper: return float\n",
    "    if 'BIGINT' in sql_type_upper or 'INT' in sql_type_upper: return int\n",
    "    if 'VARCHAR' in sql_type_upper or 'TEXT' in sql_type_upper or 'STRING' in sql_type_upper: return str\n",
    "    if 'TIMESTAMP' in sql_type_upper: return datetime.datetime\n",
    "    if 'DATE' in sql_type_upper: return datetime.date\n",
    "    if 'TIME' in sql_type_upper: return datetime.time\n",
    "    if 'BOOLEAN' in sql_type_upper: return bool\n",
    "    if 'BLOB' in sql_type_upper or 'BYTEA' in sql_type_upper: return bytes\n",
    "    if 'UUID' in sql_type_upper: return uuid.UUID\n",
    "    return object\n",
    "\n",
    "pydantic_models: Dict[str, Type[BaseModel]] = {}\n",
    "table_names = schema_df['name'].unique()\n",
    "\n",
    "for table_name in table_names:\n",
    "    table_schema = schema_df[schema_df['name'] == table_name].iloc[0]\n",
    "    fields: Dict[str, Any] = {}\n",
    "    col_names = table_schema['column_names']\n",
    "    col_types = table_schema['column_types']\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        python_type = map_sql_type_to_python(col_types[i])\n",
    "        fields[col_name] = (Optional[python_type], None)\n",
    "    model_name = table_name.capitalize() + \"Model\"\n",
    "    pydantic_models[table_name] = create_model(model_name, **fields)\n",
    "\n",
    "dataset_fields: Dict[str, Any] = {\n",
    "    table_name: (List[model], ...) for table_name, model in pydantic_models.items()\n",
    "}\n",
    "SyntheticDataset = create_model('SyntheticDataset', **dataset_fields)\n",
    "print(\"‚úÖ Dynamically created Pydantic models for all tables.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Total Row Counts and Chunking Strategy ---\n",
    "TOTAL_ROW_COUNTS = {name: TARGET_ROW_COUNT for name in table_names}\n",
    "ROWS_PER_API_CALL = 4 # Ask for data in small, safe chunks\n",
    "print(\"\\n‚úÖ Data Generation Plan:\")\n",
    "print(f\" - Target rows per table: {list(TOTAL_ROW_COUNTS.values())[0]}\")\n",
    "print(f\" - Will make API calls asking for {ROWS_PER_API_CALL} rows/call until target is met.\")\n",
    "\n",
    "\n",
    "# --- 3. Setup LLM and Loop to Generate Data in Chunks ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "load_dotenv()\n",
    "llm = LLM(model=\"accounts/fireworks/models/deepseek-v3\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "\n",
    "all_synthetic_data: Dict[str, List[Dict]] = {name: [] for name in table_names}\n",
    "chunk_row_counts = {name: ROWS_PER_API_CALL for name in table_names}\n",
    "\n",
    "base_generation_prompt = f\"\"\"\n",
    "You are a highly intelligent AI data generator. Your task is to create a realistic, synthetic dataset based on the provided database schema.\n",
    "The data you generate must be internally consistent. For example, an `airline_id` in a `routes` table must correspond to an existing `airline_id` in an `airlines` table within this same generated chunk.\n",
    "This applies to any schema you might be working with, not just airline-related data.\n",
    "You must generate a single JSON object that strictly adheres to the provided JSON schema.\n",
    "\n",
    "The database schema is as follows:\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "call_count = 0\n",
    "# Loop until all tables have at least the desired number of rows\n",
    "while not all(len(rows) >= TOTAL_ROW_COUNTS[name] for name, rows in all_synthetic_data.items()):\n",
    "    call_count += 1\n",
    "    print(f\"\\nüìû --- Generating data chunk #{call_count} ---\")\n",
    "    \n",
    "    # --- Create a summary of existing data to guide the LLM ---\n",
    "    existing_data_summary = \"\"\n",
    "    if any(len(rows) > 0 for rows in all_synthetic_data.values()):\n",
    "        summary_parts = [\"\\nYou have already generated the following data. Do NOT generate rows that are substantially similar to these examples. Create new, unique data.\\n\"]\n",
    "        for table_name, rows in all_synthetic_data.items():\n",
    "            if rows:\n",
    "                summary_parts.append(f\"\\n--- Existing data in '{table_name}' table ---\")\n",
    "                df = pd.DataFrame(rows)\n",
    "                if len(df.columns) > 10:\n",
    "                    df = df.iloc[:, :10]\n",
    "                markdown_summary = df.to_markdown(index=False, tablefmt=\"grid\")\n",
    "                if markdown_summary:\n",
    "                    summary_parts.append(markdown_summary)\n",
    "        existing_data_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "    # --- Construct the final prompt for this iteration ---\n",
    "    final_prompt = (\n",
    "        base_generation_prompt +\n",
    "        existing_data_summary +\n",
    "        f\"\\n\\nNow, generate a NEW JSON object with a key for each table. The number of new rows for each table should be:\\n\" +\n",
    "        json.dumps(chunk_row_counts, indent=2)\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SyntheticDataset\", \"schema\": SyntheticDataset.model_json_schema()}},\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    choice = response.choices[0]\n",
    "    response_content = choice.message.content\n",
    "\n",
    "    if choice.finish_reason == \"length\":\n",
    "        print(f\"‚ö†Ô∏è WARNING: Chunk #{call_count} was truncated. Skipping.\")\n",
    "        continue\n",
    "    if not response_content:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Received empty content for chunk #{call_count}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chunk_data = json.loads(response_content)\n",
    "        print(f\"‚úÖ Received and parsed chunk #{call_count}.\")\n",
    "        for table_name, rows in chunk_data.items():\n",
    "            if table_name in all_synthetic_data and rows:\n",
    "                all_synthetic_data[table_name].extend(rows)\n",
    "        # Log progress\n",
    "        for name, rows in all_synthetic_data.items():\n",
    "             print(f\"   - '{name}': {len(rows)} / {TOTAL_ROW_COUNTS[name]} rows\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå ERROR: Failed to parse JSON for chunk #{call_count}. Reason: {e}. Skipping.\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# --- 4. Deduplicate and Write to DB ---\n",
    "print(\"\\n‚ú® Data generation complete. Aggregating, deduplicating, and saving to database...\")\n",
    "\n",
    "synthetic_data = all_synthetic_data\n",
    "print(\"\\n--- Deduplicating generated data ---\")\n",
    "for table_name, rows in synthetic_data.items():\n",
    "    if not rows: continue\n",
    "    initial_count = len(rows)\n",
    "    df = pd.DataFrame(rows).drop_duplicates()\n",
    "    final_count = len(df)\n",
    "    synthetic_data[table_name] = df.to_dict('records')\n",
    "    print(f\" - Table '{table_name}': Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Final trim to ensure exact counts\n",
    "for table_name, total_rows_needed in TOTAL_ROW_COUNTS.items():\n",
    "    if table_name in synthetic_data:\n",
    "        synthetic_data[table_name] = synthetic_data[table_name][:total_rows_needed]\n",
    "\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH) as con:\n",
    "    for table_name, rows in synthetic_data.items():\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            schema_cols = schema_df[schema_df['name'] == table_name].iloc[0]['column_names']\n",
    "            for col in schema_cols:\n",
    "                if col not in df.columns: df[col] = None\n",
    "            df = df[schema_cols]\n",
    "            con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Synthetic training sandbox created at: {SYNTHETIC_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 'Production' database simulated at: data/prod_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# --- Download the raw data files ---\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "BASE_URL = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/\"\n",
    "FILES_TO_DOWNLOAD = {\n",
    "    \"airports\": \"airports.dat\",\n",
    "    \"airlines\": \"airlines.dat\",\n",
    "    \"routes\": \"routes.dat\",\n",
    "    \"countries\": \"countries.dat\",\n",
    "    \"planes\": \"planes.dat\"\n",
    "}\n",
    "# Define column names as the files don't have headers\n",
    "COLUMN_NAMES = {\n",
    "    \"airports\": [\"airport_id\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"],\n",
    "    \"airlines\": [\"airline_id\", \"name\", \"alias\", \"iata\", \"icao\", \"callsign\", \"country\", \"active\"],\n",
    "    \"routes\": [\"airline\", \"airline_id\", \"source_airport\", \"source_airport_id\", \"destination_airport\", \"destination_airport_id\", \"codeshare\", \"stops\", \"equipment\"],\n",
    "    \"countries\": [\"name\", \"iso_code\", \"dafif_code\"],\n",
    "    \"planes\": [\"name\", \"iata\", \"icao\"]\n",
    "}\n",
    "\n",
    "PROD_DB_PATH = \"data/prod_openflights.db\"\n",
    "\n",
    "# --- Load the real data into our \"production\" DuckDB ---\n",
    "with duckdb.connect(PROD_DB_PATH) as con:\n",
    "    for name, filename in FILES_TO_DOWNLOAD.items():\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            print(f\"‚úÖ Downloaded: {path}\")\n",
    "\n",
    "        # Load data using pandas to handle missing headers and null values\n",
    "        df = pd.read_csv(path, header=None, names=COLUMN_NAMES[name], na_values=[\"\\\\N\"])\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE {name} AS SELECT * FROM df\")\n",
    "\n",
    "    print(f\"\\n‚úÖ 'Production' database simulated at: {PROD_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ‚úÖ Validate the Sandbox\n",
    "Let's run a few queries against our new synthetic database to ensure the LLM did a good job generating plausible, interconnected data.\n",
    "\n",
    "We expect to see non-empty, realistic-looking data that follows the schema constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validating the first few tables in the synthetic sandbox ---\n",
      "\n",
      "--- SELECT * FROM airlines LIMIT 10; ---\n",
      "+----+--------------+------------------+---------+--------+--------+----------------+----------------+----------+\n",
      "|    |   airline_id | name             | alias   | iata   | icao   | callsign       | country        | active   |\n",
      "|----+--------------+------------------+---------+--------+--------+----------------+----------------+----------|\n",
      "|  0 |            1 | SkyHigh Airlines | SHA     | SH     | SKY    | SKYHIGH        | United States  | Y        |\n",
      "|  1 |            2 | Oceanic Airways  | OA      | OC     | OCN    | OCEANIC        | United Kingdom | Y        |\n",
      "|  2 |            3 | Global Wings     | GW      | GL     | GLB    | GLOBALWINGS    | Canada         | Y        |\n",
      "|  3 |            4 | Pacific Horizon  | PH      | PA     | PHZ    | PACIFICHORIZON | Australia      | Y        |\n",
      "|  4 |            5 | Arctic Air       | AA      | AR     | ARC    | ARCTIC         | Canada         | Y        |\n",
      "|  5 |            6 | Sahara Sky       | SS      | SA     | SHR    | SAHARASKY      | Egypt          | Y        |\n",
      "|  6 |            7 | Nordic Air       | NA      | ND     | NRC    | NORDICAIR      | Norway         | Y        |\n",
      "|  7 |            8 | Sunrise Airlines | SA      | SR     | SUN    | SUNRISE        | Japan          | Y        |\n",
      "|  8 |            9 | Alpine Air       | AL      | AP     | ALP    | ALPINE         | Switzerland    | Y        |\n",
      "|  9 |           10 | Island Hopper    | IH      | IH     | ISL    | ISLANDER       | Fiji           | Y        |\n",
      "+----+--------------+------------------+---------+--------+--------+----------------+----------------+----------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM airports LIMIT 10; ---\n",
      "+----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------+\n",
      "|    |   airport_id | name                          | city      | country        | iata   | icao   |   latitude |   longitude |   altitude |   timezone | dst   | tz_db            | type    | source      |\n",
      "|----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------|\n",
      "|  0 |            1 | SkyHigh International Airport | New York  | United States  | SKY    | KSKY   |    40.7128 |    -74.006  |         10 |         -5 | A     | America/New_York | airport | OurAirports |\n",
      "|  1 |            2 | Oceanic Gateway Airport       | London    | United Kingdom | OCN    | EGLC   |    51.5074 |     -0.1278 |         15 |          0 | E     | Europe/London    | airport | OurAirports |\n",
      "|  2 |            3 | Global Wings Hub              | Toronto   | Canada         | GLH    | CYYZ   |    43.6532 |    -79.3832 |        173 |         -5 | A     | America/Toronto  | airport | OurAirports |\n",
      "|  3 |            4 | Pacific Horizon Terminal      | Sydney    | Australia      | PHT    | YSSY   |   -33.8688 |    151.209  |         21 |         10 | O     | Australia/Sydney | airport | OurAirports |\n",
      "|  4 |            5 | Arctic Air Terminal           | Vancouver | Canada         | ARC    | CYVR   |    49.1936 |   -123.184  |          4 |         -8 |       |                  |         |             |\n",
      "|  5 |            6 | Sahara Sky Hub                | Cairo     | Egypt          | SHR    | HECA   |    30.1092 |     31.4022 |        116 |          2 |       |                  |         |             |\n",
      "|  6 |            7 | Nordic Air Hub                | Oslo      | Norway         | NRD    | ENOA   |    59.9139 |     10.7522 |         23 |          1 |       |                  |         |             |\n",
      "|  7 |            8 | Sunrise International Airport | Tokyo     | Japan          | SRN    | RJTT   |    35.5494 |    139.78   |         35 |          9 |       |                  |         |             |\n",
      "|  8 |            9 | Alpine Air Hub                | Zurich    | Switzerland    | ALP    | LSZH   |    47.4647 |      8.5492 |       1416 |          1 | E     | Europe/Zurich    | airport | OurAirports |\n",
      "|  9 |           10 | Island Hopper Terminal        | Nadi      | Fiji           | IHB    | NFFN   |   -17.7554 |    177.443  |         18 |         12 | U     | Pacific/Fiji     | airport | OurAirports |\n",
      "+----+--------------+-------------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+------------------+---------+-------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM countries LIMIT 10; ---\n",
      "+----+----------------+------------+--------------+\n",
      "|    | name           | iso_code   | dafif_code   |\n",
      "|----+----------------+------------+--------------|\n",
      "|  0 | United States  | US         | US           |\n",
      "|  1 | United Kingdom | GB         | UK           |\n",
      "|  2 | Canada         | CA         | CA           |\n",
      "|  3 | Australia      | AU         | AS           |\n",
      "|  4 | Egypt          | EG         | EG           |\n",
      "|  5 | Japan          | JP         | JA           |\n",
      "|  6 | Norway         | NO         | NO           |\n",
      "|  7 | China          | CN         | CH           |\n",
      "|  8 | Switzerland    | CH         | SZ           |\n",
      "|  9 | Fiji           | FJ         | FJ           |\n",
      "+----+----------------+------------+--------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM planes LIMIT 10; ---\n",
      "+----+-------------------+--------+--------+\n",
      "|    | name              | iata   | icao   |\n",
      "|----+-------------------+--------+--------|\n",
      "|  0 | Boeing 747        | 74     | B744   |\n",
      "|  1 | Airbus A380       | 38     | A388   |\n",
      "|  2 | Boeing 777        | 77     | B777   |\n",
      "|  3 | Airbus A320       | 32     | A320   |\n",
      "|  4 | Boeing 737        | 73     | B737   |\n",
      "|  5 | Airbus A350       | 35     | A350   |\n",
      "|  6 | Boeing 787        | 78     | B788   |\n",
      "|  7 | Embraer E195      | E9     | E195   |\n",
      "|  8 | Bombardier CRJ900 | CR     | CRJ9   |\n",
      "|  9 | ATR 72            | AT     | AT72   |\n",
      "+----+-------------------+--------+--------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM routes LIMIT 10; ---\n",
      "+----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------+\n",
      "|    | airline          |   airline_id | source_airport                |   source_airport_id | destination_airport           |   destination_airport_id | codeshare   |   stops | equipment         |\n",
      "|----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------|\n",
      "|  0 | SkyHigh Airlines |            1 | SkyHigh International Airport |                   1 | Oceanic Gateway Airport       |                        2 |             |       0 | Boeing 747        |\n",
      "|  1 | Oceanic Airways  |            2 | Oceanic Gateway Airport       |                   2 | SkyHigh International Airport |                        1 |             |       0 | Airbus A380       |\n",
      "|  2 | Global Wings     |            3 | Global Wings Hub              |                   3 | Pacific Horizon Terminal      |                        4 |             |       0 | Boeing 777        |\n",
      "|  3 | Pacific Horizon  |            4 | Pacific Horizon Terminal      |                   4 | Global Wings Hub              |                        3 |             |       0 | Airbus A320       |\n",
      "|  4 | Arctic Air       |            5 | Arctic Air Terminal           |                   5 | Sahara Sky Hub                |                        6 |             |       0 | Boeing 737        |\n",
      "|  5 | Sahara Sky       |            6 | Sahara Sky Hub                |                   6 | Arctic Air Terminal           |                        5 |             |       0 | Airbus A350       |\n",
      "|  6 | Nordic Air       |            7 | Nordic Air Hub                |                   7 | Sunrise International Airport |                        8 |             |       0 | Boeing 787        |\n",
      "|  7 | Sunrise Airlines |            8 | Sunrise International Airport |                   8 | Nordic Air Hub                |                        7 |             |       0 | Embraer E195      |\n",
      "|  8 | Alpine Air       |            9 | Alpine Air Hub                |                   9 | Island Hopper Terminal        |                       10 |             |       0 | Bombardier CRJ900 |\n",
      "|  9 | Island Hopper    |           10 | Island Hopper Terminal        |                  10 | Alpine Air Hub                |                        9 |             |       0 | ATR 72            |\n",
      "+----+------------------+--------------+-------------------------------+---------------------+-------------------------------+--------------------------+-------------+---------+-------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from tabulate import tabulate\n",
    "\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "\n",
    "# Connect to the synthetic database\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    \n",
    "    # Get the list of all tables created\n",
    "    all_tables = [table[0] for table in con.sql(\"SHOW TABLES;\").fetchall()]\n",
    "    \n",
    "    # Select the first 5 tables to display (or all if fewer than 5)\n",
    "    tables_to_validate = all_tables[:5]\n",
    "\n",
    "    print(\"--- Validating the first few tables in the synthetic sandbox ---\\n\")\n",
    "\n",
    "    # Execute and print results for the selected tables\n",
    "    for table_name in tables_to_validate:\n",
    "        print(f\"--- SELECT * FROM {table_name} LIMIT 10; ---\")\n",
    "        try:\n",
    "            result_df = con.sql(f\"SELECT * FROM {table_name} LIMIT 10;\").df()\n",
    "            if not result_df.empty:\n",
    "                print(tabulate(result_df, headers='keys', tablefmt='psql'))\n",
    "            else:\n",
    "                print(f\"(Table '{table_name}' is empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed for table '{table_name}': {e}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. üìù Generate Example SQL Queries\n",
    "With our synthetic database in place, the next step is to create a set of synthetic SQL queries. These SQL queries will be executed against our database of synthetic data to get the ground truth labels for RFT. Furthermore, these same SQL queries will be used as input to an LLM to generate queries in natural language. This will enable us to form our final RFT dataset, which pairs natural language queries with ground truth results from the database.\n",
    "\n",
    "#### Query Generation Strategy:\n",
    "- **Diversity**: We want queries covering different SQL features (JOINs, GROUP BY, aggregates)\n",
    "- **Complexity Range**: From simple SELECT statements to complex multi-table joins\n",
    "- **Deterministic Results**: Queries include ORDER BY clauses where necessary to break ties and ensure consistent results\n",
    "- **MECE Principle**: Mutually Exclusive, Collectively Exhaustive - covering all major query patterns\n",
    "\n",
    "> **Real World üåç**: You would use a historical log of real SQL queries that have been run against your production database. These logs are the most valuable source of training data because they represent the *actual* way your users query your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Goal: Generate 1200 unique queries in batches of 20.\n",
      "\n",
      "üìû --- Generating batch #1 ---\n",
      "   - Received 20 new queries. Total now: 20 / 1200\n",
      "\n",
      "üìû --- Generating batch #2 ---\n",
      "   - Received 20 new queries. Total now: 40 / 1200\n",
      "\n",
      "üìû --- Generating batch #3 ---\n",
      "   - Received 20 new queries. Total now: 60 / 1200\n",
      "\n",
      "üìû --- Generating batch #4 ---\n",
      "   - Received 20 new queries. Total now: 80 / 1200\n",
      "\n",
      "üìû --- Generating batch #5 ---\n",
      "   - Received 20 new queries. Total now: 100 / 1200\n",
      "\n",
      "üìû --- Generating batch #6 ---\n",
      "   - Received 20 new queries. Total now: 120 / 1200\n",
      "\n",
      "üìû --- Generating batch #7 ---\n",
      "   - Received 20 new queries. Total now: 140 / 1200\n",
      "\n",
      "üìû --- Generating batch #8 ---\n",
      "   - Received 20 new queries. Total now: 160 / 1200\n",
      "\n",
      "üìû --- Generating batch #9 ---\n",
      "   - Received 20 new queries. Total now: 180 / 1200\n",
      "\n",
      "üìû --- Generating batch #10 ---\n",
      "   - Received 20 new queries. Total now: 200 / 1200\n",
      "\n",
      "üìû --- Generating batch #11 ---\n",
      "   - Received 20 new queries. Total now: 220 / 1200\n",
      "\n",
      "üìû --- Generating batch #12 ---\n",
      "   - Received 20 new queries. Total now: 240 / 1200\n",
      "\n",
      "üìû --- Generating batch #13 ---\n",
      "   - Received 20 new queries. Total now: 260 / 1200\n",
      "\n",
      "üìû --- Generating batch #14 ---\n",
      "   - Received 20 new queries. Total now: 280 / 1200\n",
      "\n",
      "üìû --- Generating batch #15 ---\n",
      "   - Received 20 new queries. Total now: 300 / 1200\n",
      "\n",
      "üìû --- Generating batch #16 ---\n",
      "   - Received 20 new queries. Total now: 320 / 1200\n",
      "\n",
      "üìû --- Generating batch #17 ---\n",
      "   - Received 20 new queries. Total now: 340 / 1200\n",
      "\n",
      "üìû --- Generating batch #18 ---\n",
      "   - Received 20 new queries. Total now: 360 / 1200\n",
      "\n",
      "üìû --- Generating batch #19 ---\n",
      "   - Received 20 new queries. Total now: 380 / 1200\n",
      "\n",
      "üìû --- Generating batch #20 ---\n",
      "   - Received 20 new queries. Total now: 400 / 1200\n",
      "\n",
      "üìû --- Generating batch #21 ---\n",
      "   - Received 20 new queries. Total now: 420 / 1200\n",
      "\n",
      "üìû --- Generating batch #22 ---\n",
      "   - Received 20 new queries. Total now: 440 / 1200\n",
      "\n",
      "üìû --- Generating batch #23 ---\n",
      "   - Received 20 new queries. Total now: 460 / 1200\n",
      "\n",
      "üìû --- Generating batch #24 ---\n",
      "   - Received 20 new queries. Total now: 480 / 1200\n",
      "\n",
      "üìû --- Generating batch #25 ---\n",
      "   - Received 20 new queries. Total now: 500 / 1200\n",
      "\n",
      "üìû --- Generating batch #26 ---\n",
      "   - Received 20 new queries. Total now: 520 / 1200\n",
      "\n",
      "üìû --- Generating batch #27 ---\n",
      "   - Received 20 new queries. Total now: 540 / 1200\n",
      "\n",
      "üìû --- Generating batch #28 ---\n",
      "   - Received 20 new queries. Total now: 560 / 1200\n",
      "\n",
      "üìû --- Generating batch #29 ---\n",
      "   - Received 20 new queries. Total now: 580 / 1200\n",
      "\n",
      "üìû --- Generating batch #30 ---\n",
      "   - Received 20 new queries. Total now: 600 / 1200\n",
      "\n",
      "üìû --- Generating batch #31 ---\n",
      "   - Received 20 new queries. Total now: 620 / 1200\n",
      "\n",
      "üìû --- Generating batch #32 ---\n",
      "   - Received 20 new queries. Total now: 640 / 1200\n",
      "\n",
      "üìû --- Generating batch #33 ---\n",
      "   - Received 20 new queries. Total now: 660 / 1200\n",
      "\n",
      "üìû --- Generating batch #34 ---\n",
      "   - Received 20 new queries. Total now: 680 / 1200\n",
      "\n",
      "üìû --- Generating batch #35 ---\n",
      "   - Received 20 new queries. Total now: 700 / 1200\n",
      "\n",
      "üìû --- Generating batch #36 ---\n",
      "   - Received 20 new queries. Total now: 720 / 1200\n",
      "\n",
      "üìû --- Generating batch #37 ---\n",
      "   - Received 20 new queries. Total now: 740 / 1200\n",
      "\n",
      "üìû --- Generating batch #38 ---\n",
      "   - Received 20 new queries. Total now: 760 / 1200\n",
      "\n",
      "üìû --- Generating batch #39 ---\n",
      "   - Received 20 new queries. Total now: 780 / 1200\n",
      "\n",
      "üìû --- Generating batch #40 ---\n",
      "   - Received 20 new queries. Total now: 800 / 1200\n",
      "\n",
      "üìû --- Generating batch #41 ---\n",
      "   - Received 20 new queries. Total now: 820 / 1200\n",
      "\n",
      "üìû --- Generating batch #42 ---\n",
      "   - Received 20 new queries. Total now: 840 / 1200\n",
      "\n",
      "üìû --- Generating batch #43 ---\n",
      "   - Received 20 new queries. Total now: 860 / 1200\n",
      "\n",
      "üìû --- Generating batch #44 ---\n",
      "   - Received 20 new queries. Total now: 880 / 1200\n",
      "\n",
      "üìû --- Generating batch #45 ---\n",
      "   - Received 20 new queries. Total now: 900 / 1200\n",
      "\n",
      "üìû --- Generating batch #46 ---\n",
      "   - Received 20 new queries. Total now: 920 / 1200\n",
      "\n",
      "üìû --- Generating batch #47 ---\n",
      "   - Received 20 new queries. Total now: 940 / 1200\n",
      "\n",
      "üìû --- Generating batch #48 ---\n",
      "   - Received 20 new queries. Total now: 960 / 1200\n",
      "\n",
      "üìû --- Generating batch #49 ---\n",
      "   - Received 20 new queries. Total now: 980 / 1200\n",
      "\n",
      "üìû --- Generating batch #50 ---\n",
      "   - Received 20 new queries. Total now: 1000 / 1200\n",
      "\n",
      "üìû --- Generating batch #51 ---\n",
      "   - Received 20 new queries. Total now: 1020 / 1200\n",
      "\n",
      "üìû --- Generating batch #52 ---\n",
      "   - Received 20 new queries. Total now: 1040 / 1200\n",
      "\n",
      "üìû --- Generating batch #53 ---\n",
      "   - Received 20 new queries. Total now: 1060 / 1200\n",
      "\n",
      "üìû --- Generating batch #54 ---\n",
      "   - Received 20 new queries. Total now: 1080 / 1200\n",
      "\n",
      "üìû --- Generating batch #55 ---\n",
      "   - Received 20 new queries. Total now: 1100 / 1200\n",
      "\n",
      "üìû --- Generating batch #56 ---\n",
      "   - Received 20 new queries. Total now: 1120 / 1200\n",
      "\n",
      "üìû --- Generating batch #57 ---\n",
      "   - Received 20 new queries. Total now: 1140 / 1200\n",
      "\n",
      "üìû --- Generating batch #58 ---\n",
      "   - Received 20 new queries. Total now: 1160 / 1200\n",
      "\n",
      "üìû --- Generating batch #59 ---\n",
      "   - Received 20 new queries. Total now: 1180 / 1200\n",
      "\n",
      "üìû --- Generating batch #60 ---\n",
      "   - Received 20 new queries. Total now: 1200 / 1200\n",
      "\n",
      "‚ú® Generation complete. Deduplicating and saving...\n",
      " - Removed 513 duplicates (1200 -> 687).\n",
      "\n",
      "‚úÖ Successfully saved 687 unique queries to `data/generated_queries.json`.\n",
      "\n",
      "--- Here are a few examples: ---\n",
      "- SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\n",
      "- SELECT a.name AS airport_name, a.city, a.country, COUNT(r.airline_id) AS route_count FROM airports a LEFT JOIN routes r ON a.airport_id = r.source_airport_id GROUP BY a.airport_id, a.name, a.city, a.country ORDER BY route_count DESC, a.name ASC\n",
      "- SELECT r.airline, COUNT(*) AS route_frequency FROM routes r WHERE r.stops = 0 GROUP BY r.airline ORDER BY route_frequency DESC, r.airline ASC\n",
      "- SELECT c.name AS country_name, COUNT(a.airport_id) AS airport_count FROM countries c JOIN airports a ON c.name = a.country GROUP BY c.name ORDER BY airport_count DESC, c.name ASC\n",
      "- SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AS destination_count FROM airlines a JOIN routes r ON a.airline_id = r.airline_id GROUP BY a.airline_id, a.name ORDER BY destination_count DESC, a.name ASC LIMIT 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from fireworks import LLM\n",
    "import os\n",
    "import duckdb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define Generation Parameters and Pydantic Model ---\n",
    "llm = LLM(model=\"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))  # Use Qwen3-coder for SQL queries\n",
    "TOTAL_QUERIES_TO_GENERATE = 1200  # Note, some of these queries will likely be duplicates or invalid, reducing the final number used for fine-tuning\n",
    "QUERIES_PER_API_CALL = 30\n",
    "\n",
    "class SqlQueryBatch(BaseModel):\n",
    "    queries: List[str] = Field(description=f\"A list of exactly {QUERIES_PER_API_CALL} unique and diverse SQL queries.\")\n",
    "\n",
    "print(f\"üéØ Goal: Generate {TOTAL_QUERIES_TO_GENERATE} unique queries in batches of {QUERIES_PER_API_CALL}.\")\n",
    "\n",
    "# --- 2. Get Clean Schema From Synthetic DB ---\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    schema_df = con.sql(\"DESCRIBE;\").df()\n",
    "    schema_for_prompt = schema_df.to_markdown(index=False)\n",
    "\n",
    "# --- 3. Setup Base Prompt and Generation Loop ---\n",
    "base_query_generation_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to generate unique and diverse SQL queries based on the database schema provided.\n",
    "The queries should be realistic and cover a range of complexities and SQL features (JOINS, GROUP BY, aggregates, etc.).\n",
    "Ensure you break ties with ORDER BY clauses so that the same queries produce the same results when executed against the database.\n",
    "Write on the SQL query and nothing else.\n",
    "Ensure the generated SQL is valid for DuckDB.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "all_generated_queries = []\n",
    "# Loop until we have enough queries\n",
    "while len(all_generated_queries) < TOTAL_QUERIES_TO_GENERATE:\n",
    "    print(f\"\\nüìû --- Generating batch #{len(all_generated_queries) // QUERIES_PER_API_CALL + 1} ---\")\n",
    "\n",
    "    # Create a summary of queries generated so far to prevent duplicates\n",
    "    existing_queries_summary = \"\"\n",
    "    if all_generated_queries:\n",
    "        summary_parts = [\"\\nYou have already generated the following queries. Generate new, unique queries that are not on this list and cover different analytic scenarios.\\n\"]\n",
    "        for i, q in enumerate(all_generated_queries):\n",
    "            summary_parts.append(f\"{i+1}. {q}\")\n",
    "        existing_queries_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "    # Construct the final prompt for this iteration\n",
    "    final_prompt = (\n",
    "        base_query_generation_prompt +\n",
    "        existing_queries_summary +\n",
    "        f\"\\n\\nNow, generate {QUERIES_PER_API_CALL} new and unique SQL queries. Return your response as a single JSON object adhering to the specified schema.\"\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SqlQueryBatch\", \"schema\": SqlQueryBatch.model_json_schema()}},\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    if response_content:\n",
    "        try:\n",
    "            new_queries = json.loads(response_content).get(\"queries\", [])\n",
    "            all_generated_queries.extend(new_queries)\n",
    "            print(f\"   - Received {len(new_queries)} new queries. Total now: {len(all_generated_queries)} / {TOTAL_QUERIES_TO_GENERATE}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå ERROR: Failed to parse generated queries in this batch: {e}\")\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 4. Deduplicate, Trim, and Save --- \n",
    "print(\"\\n‚ú® Generation complete. Deduplicating and saving...\")\n",
    "initial_count = len(all_generated_queries)\n",
    "# Simple, fast deduplication preserving order\n",
    "unique_queries = list(dict.fromkeys(all_generated_queries))\n",
    "final_count = len(unique_queries)\n",
    "print(f\" - Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Trim to the exact number we need\n",
    "final_queries = unique_queries[:TOTAL_QUERIES_TO_GENERATE]\n",
    "\n",
    "# Save the final list to a file\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "with open(QUERIES_FILE_PATH, 'w') as f:\n",
    "    json.dump({\"queries\": final_queries}, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {len(final_queries)} unique queries to `{QUERIES_FILE_PATH}`.\")\n",
    "print(\"\\n--- Here are a few examples: ---\")\n",
    "for query in final_queries[:5]:\n",
    "    print(f\"- {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. üéØ Execute Queries to Get Ground-Truth Answers\n",
    "Now we will act as the \"system\" and run the queries we just generated against our synthetic sandbox. The output of each query is the **ground-truth result**. During Reinforcement Fine-Tuning, our model will be rewarded if the SQL it writes produces this exact same result.\n",
    "\n",
    "#### Why RFT is a good choice for a text-to-SQL use-case:\n",
    "In RFT, the model explores the space of possible SQL queries during fine-tuning; the reward signal comes from comparing the result of executing the model's output SQL queries against the ground truth expected results. This is fundamentally different from SFT, where the model learns to mimic the exact SQL syntax. With RFT:\n",
    "- Multiple SQL queries can be \"correct\" if they produce the same result\n",
    "- The model learns to reason about the problem rather than memorize solutions\n",
    "- Edge cases and query optimization patterns can emerge naturally\n",
    "\n",
    "> **Real World üåç**: You would run your real historical queries against the synthetic database we previously created. The correctness of the data is not a concern here, as our aim is to see what a correct query would have generated, so we can compare it to our LLM's generations during the RFT process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 687 queries to execute.\n",
      "Executing queries against the synthetic database...\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.country, AVG(ar.altitude) AS avg_altitude FROM airports a JOIN airlines ar ON a.country = ar.country GROUP BY a.country ORDER BY avg_altitude DESC, a.country ASC\n",
      "   Error: Binder Error: Table \"ar\" does not have a column named \"altitude\"\n",
      "\n",
      "Candidate bindings: : \"active\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, AVG(ar.altitude) AS avg_altitude, COUNT(a.airport_id) AS airport_count FROM airports ar JOIN airlines a ON ar.country = a.country WHERE ar.type = 'airport' GROUP BY ar.city ORDER BY avg_altitude DESC, ar.city ASC\n",
      "   Error: Binder Error: Table \"a\" does not have a column named \"airport_id\"\n",
      "\n",
      "Candidate bindings: : \"airline_id\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count ASC, a.city ASC LIMIT 10\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AS.total_routes FROM airlines a JOIN routes r ON a.airline_id = r.airline_id WHERE a.active = 'Y' GROUP BY a.airline_id, a.name HAVING COUNT(r.destination_airport_id) > 1000 ORDER BY.total_routes DESC, a.name ASC\n",
      "   Error: Parser Error: syntax error at or near \".\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count ASC, a.city ASC LIMIT 5\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) = 2 ORDER BY airport_count ASC, a.city ASC LIMIT 10\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count DESC, a.city ASC LIMIT 10\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count DESC, a.city ASC LIMIT 15\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) = 2 ORDER BY airport_count ASC, a.city ASC LIMIT 15\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count DESC, a.city ASC LIMIT 20\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT ar.city, COUNT(DISTINCT a.airport_id) AS airport_count FROM airports a GROUP BY a.city HAVING COUNT(DISTINCT a.airport_id) > 1 ORDER BY airport_count DESC, a.city ASC LIMIT 5\n",
      "   Error: Binder Error: Referenced table \"ar\" not found!\n",
      "Candidate tables: \"a\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AStotal_routes FROM airlines a JOIN routes r ON a.airline_id = r.airline_id WHERE a.active = 'Y' GROUP BY a.airline_id, a.name HAVING COUNT(r.destination_airport_id) > 1000 ORDER BY total_routes DESC, a.name ASC\n",
      "   Error: Binder Error: Referenced column \"total_routes\" not found in FROM clause!\n",
      "Candidate bindings: \"destination_airport_id\", \"source_airport\", \"codeshare\", \"source_airport_id\", \"airline\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AS50plus_routes FROM airlines a JOIN routes r ON a.airline_id = r.airline_id WHERE a.active = 'Y' GROUP BY a.airline_id, a.name HAVING COUNT(r.destination_airport_id) >= 50 ORDER BY50plus_routes DESC, a.name ASC\n",
      "   Error: Parser Error: syntax error at or near \"BY50plus_routes\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AS(route_count) FROM airlines a JOIN routes r ON a.airline_id = r.airline_id WHERE a.active = 'Y' GROUP BY a.airline_id, a.name ORDER BY route_count DESC, a.name ASC LIMIT 25\n",
      "   Error: Parser Error: syntax error at or near \"(\"\n",
      "\n",
      "\n",
      "Execution complete. Success: 673, Failed: 14.\n",
      "\n",
      "‚úÖ Successfully saved 673 ground-truth results to `data/ground_truth_results.jsonl`.\n",
      "\n",
      "--- Example ground_truth_results dataset entry ---\n",
      "{\n",
      "  \"query\": \"SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\",\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"country\": \"Canada\",\n",
      "      \"airline_count\": 10\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Sweden\",\n",
      "      \"airline_count\": 10\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Kenya\",\n",
      "      \"airline_count\": 9\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United States\",\n",
      "      \"airline_count\": 9\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Australia\",\n",
      "      \"airline_count\": 8\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Spain\",\n",
      "      \"airline_count\": 6\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Italy\",\n",
      "      \"airline_count\": 4\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Switzerland\",\n",
      "      \"airline_count\": 4\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Finland\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"France\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Mexico\",\n",
      "      \"airline_count\": 3\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Costa Rica\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Germany\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Iceland\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ireland\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Japan\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Norway\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Singapore\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Kingdom\",\n",
      "      \"airline_count\": 2\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Argentina\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Brazil\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"China\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Egypt\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Fiji\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Greece\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"India\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Jordan\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Netherlands\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"New Zealand\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Portugal\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Saudi Arabia\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"South Africa\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Thailand\",\n",
      "      \"airline_count\": 1\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Arab Emirates\",\n",
      "      \"airline_count\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "\n",
    "# --- 2. Load Generated Queries ---\n",
    "with open(QUERIES_FILE_PATH, 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "    queries_to_execute = queries_data.get(\"queries\", [])\n",
    "\n",
    "print(f\"Loaded {len(queries_to_execute)} queries to execute.\")\n",
    "\n",
    "# --- 3. Execute Queries and Store Results ---\n",
    "ground_truth_results = []\n",
    "successful_executions = 0\n",
    "failed_executions = 0\n",
    "\n",
    "print(\"Executing queries against the synthetic database...\")\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    for query in queries_to_execute:\n",
    "        try:\n",
    "            # Execute the query and convert the result to a pandas DataFrame\n",
    "            result_df = con.sql(query).df()\n",
    "\n",
    "            # Replace any NaN/NaT values with None, which serializes to JSON `null`\n",
    "            result_df = result_df.astype(object).where(pd.notna(result_df), None)\n",
    "            \n",
    "            result_records = result_df.to_dict('records')\n",
    "            \n",
    "            # Pair the query with its result\n",
    "            ground_truth_results.append({\n",
    "                \"query\": query,\n",
    "                \"result\": result_records\n",
    "            })\n",
    "            successful_executions += 1\n",
    "        except Exception as e:\n",
    "            # The LLM might have occasionally generated a slightly invalid query\n",
    "            print(f\"‚ö†Ô∏è  Skipping query due to execution error: {query}\\n   Error: {e}\\n\")\n",
    "            failed_executions += 1\n",
    "\n",
    "print(f\"\\nExecution complete. Success: {successful_executions}, Failed: {failed_executions}.\")\n",
    "\n",
    "# --- 4. Save the Ground-Truth Data ---\n",
    "with open(GROUND_TRUTH_FILE_PATH, 'w') as f:\n",
    "    for entry in ground_truth_results:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {len(ground_truth_results)} ground-truth results to `{GROUND_TRUTH_FILE_PATH}`.\")\n",
    "\n",
    "# --- 5. Print an Example ---\n",
    "if ground_truth_results:\n",
    "    print(\"\\n--- Example ground_truth_results dataset entry ---\")\n",
    "    print(json.dumps(ground_truth_results[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. üí¨ Generate Natural Language Questions for Final RFT Training Data\n",
    "We now have pairs of `(SQL Query, Ground-Truth Result)`. The final piece missing from our training data is the user's input: a question in natural language. This is because our final goal is to use RFT to tune an LLM to map from a natural language question to a SQL query, having the reward signal be the actual result of the query, rather than just the query itself. This is important because there are many ways to write the same SQL query that yield the same, correct result.\n",
    "\n",
    "#### Thus, the complete training loop will look like this:\n",
    "1. User asks: *\"Which countries have the most airlines?\"*\n",
    "2. Model generates: *SQL query*\n",
    "3. System executes: *Query against database*\n",
    "4. Reward calculation: *Does result match ground truth?*\n",
    "5. Model update: *Reinforce successful strategies*\n",
    "\n",
    "Thus, we will use an LLM once again to translate our \"historical\" SQL queries into plausible questions a business user might ask, corresponding to that query. This will yield our final training dataset in the format: `(Natural Language Question, SQL Query, Ground-Truth Result)`. Note that the SQL queries themselves will not be used as part of the RFT job itself, but are useful for debugging our evaluation function (more details in a later section).\n",
    "\n",
    "> **Real World üåç**: You might not need this step! If you have logs that already link user questions to the queries they ran (e.g., from a BI tool's search bar), you can use those directly. If not, this LLM-based translation is a powerful technique to bootstrap your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 673 query-result pairs.\n",
      "Generating natural language questions and formatting for RFT for 673 queries...\n",
      " - Processing query 1/673...\n",
      " - Processing query 2/673...\n",
      " - Processing query 3/673...\n",
      " - Processing query 4/673...\n",
      " - Processing query 5/673...\n",
      " - Processing query 6/673...\n",
      " - Processing query 7/673...\n",
      " - Processing query 8/673...\n",
      " - Processing query 9/673...\n",
      " - Processing query 10/673...\n",
      " - Processing query 11/673...\n",
      " - Processing query 12/673...\n",
      " - Processing query 13/673...\n",
      " - Processing query 14/673...\n",
      " - Processing query 15/673...\n",
      " - Processing query 16/673...\n",
      " - Processing query 17/673...\n",
      " - Processing query 18/673...\n",
      " - Processing query 19/673...\n",
      " - Processing query 20/673...\n",
      " - Processing query 21/673...\n",
      " - Processing query 22/673...\n",
      " - Processing query 23/673...\n",
      " - Processing query 24/673...\n",
      " - Processing query 25/673...\n",
      " - Processing query 26/673...\n",
      " - Processing query 27/673...\n",
      " - Processing query 28/673...\n",
      " - Processing query 29/673...\n",
      " - Processing query 30/673...\n",
      " - Processing query 31/673...\n",
      " - Processing query 32/673...\n",
      " - Processing query 33/673...\n",
      " - Processing query 34/673...\n",
      " - Processing query 35/673...\n",
      " - Processing query 36/673...\n",
      " - Processing query 37/673...\n",
      " - Processing query 38/673...\n",
      " - Processing query 39/673...\n",
      " - Processing query 40/673...\n",
      " - Processing query 41/673...\n",
      " - Processing query 42/673...\n",
      " - Processing query 43/673...\n",
      " - Processing query 44/673...\n",
      " - Processing query 45/673...\n",
      " - Processing query 46/673...\n",
      " - Processing query 47/673...\n",
      " - Processing query 48/673...\n",
      " - Processing query 49/673...\n",
      " - Processing query 50/673...\n",
      " - Processing query 51/673...\n",
      " - Processing query 52/673...\n",
      " - Processing query 53/673...\n",
      " - Processing query 54/673...\n",
      " - Processing query 55/673...\n",
      " - Processing query 56/673...\n",
      " - Processing query 57/673...\n",
      " - Processing query 58/673...\n",
      " - Processing query 59/673...\n",
      " - Processing query 60/673...\n",
      " - Processing query 61/673...\n",
      " - Processing query 62/673...\n",
      " - Processing query 63/673...\n",
      " - Processing query 64/673...\n",
      " - Processing query 65/673...\n",
      " - Processing query 66/673...\n",
      " - Processing query 67/673...\n",
      " - Processing query 68/673...\n",
      " - Processing query 69/673...\n",
      " - Processing query 70/673...\n",
      " - Processing query 71/673...\n",
      " - Processing query 72/673...\n",
      " - Processing query 73/673...\n",
      " - Processing query 74/673...\n",
      " - Processing query 75/673...\n",
      " - Processing query 76/673...\n",
      " - Processing query 77/673...\n",
      " - Processing query 78/673...\n",
      " - Processing query 79/673...\n",
      " - Processing query 80/673...\n",
      " - Processing query 81/673...\n",
      " - Processing query 82/673...\n",
      " - Processing query 83/673...\n",
      " - Processing query 84/673...\n",
      " - Processing query 85/673...\n",
      " - Processing query 86/673...\n",
      " - Processing query 87/673...\n",
      " - Processing query 88/673...\n",
      " - Processing query 89/673...\n",
      " - Processing query 90/673...\n",
      " - Processing query 91/673...\n",
      " - Processing query 92/673...\n",
      " - Processing query 93/673...\n",
      " - Processing query 94/673...\n",
      " - Processing query 95/673...\n",
      " - Processing query 96/673...\n",
      " - Processing query 97/673...\n",
      " - Processing query 98/673...\n",
      " - Processing query 99/673...\n",
      " - Processing query 100/673...\n",
      " - Processing query 101/673...\n",
      " - Processing query 102/673...\n",
      " - Processing query 103/673...\n",
      " - Processing query 104/673...\n",
      " - Processing query 105/673...\n",
      " - Processing query 106/673...\n",
      " - Processing query 107/673...\n",
      " - Processing query 108/673...\n",
      " - Processing query 109/673...\n",
      " - Processing query 110/673...\n",
      " - Processing query 111/673...\n",
      " - Processing query 112/673...\n",
      " - Processing query 113/673...\n",
      " - Processing query 114/673...\n",
      " - Processing query 115/673...\n",
      " - Processing query 116/673...\n",
      " - Processing query 117/673...\n",
      " - Processing query 118/673...\n",
      " - Processing query 119/673...\n",
      " - Processing query 120/673...\n",
      " - Processing query 121/673...\n",
      " - Processing query 122/673...\n",
      " - Processing query 123/673...\n",
      " - Processing query 124/673...\n",
      " - Processing query 125/673...\n",
      " - Processing query 126/673...\n",
      " - Processing query 127/673...\n",
      " - Processing query 128/673...\n",
      " - Processing query 129/673...\n",
      " - Processing query 130/673...\n",
      " - Processing query 131/673...\n",
      " - Processing query 132/673...\n",
      " - Processing query 133/673...\n",
      " - Processing query 134/673...\n",
      " - Processing query 135/673...\n",
      " - Processing query 136/673...\n",
      " - Processing query 137/673...\n",
      " - Processing query 138/673...\n",
      " - Processing query 139/673...\n",
      " - Processing query 140/673...\n",
      " - Processing query 141/673...\n",
      " - Processing query 142/673...\n",
      " - Processing query 143/673...\n",
      " - Processing query 144/673...\n",
      " - Processing query 145/673...\n",
      " - Processing query 146/673...\n",
      " - Processing query 147/673...\n",
      " - Processing query 148/673...\n",
      " - Processing query 149/673...\n",
      " - Processing query 150/673...\n",
      " - Processing query 151/673...\n",
      " - Processing query 152/673...\n",
      " - Processing query 153/673...\n",
      " - Processing query 154/673...\n",
      " - Processing query 155/673...\n",
      " - Processing query 156/673...\n",
      " - Processing query 157/673...\n",
      " - Processing query 158/673...\n",
      " - Processing query 159/673...\n",
      " - Processing query 160/673...\n",
      " - Processing query 161/673...\n",
      " - Processing query 162/673...\n",
      " - Processing query 163/673...\n",
      " - Processing query 164/673...\n",
      " - Processing query 165/673...\n",
      " - Processing query 166/673...\n",
      " - Processing query 167/673...\n",
      " - Processing query 168/673...\n",
      " - Processing query 169/673...\n",
      " - Processing query 170/673...\n",
      " - Processing query 171/673...\n",
      " - Processing query 172/673...\n",
      " - Processing query 173/673...\n",
      " - Processing query 174/673...\n",
      " - Processing query 175/673...\n",
      " - Processing query 176/673...\n",
      " - Processing query 177/673...\n",
      " - Processing query 178/673...\n",
      " - Processing query 179/673...\n",
      " - Processing query 180/673...\n",
      " - Processing query 181/673...\n",
      " - Processing query 182/673...\n",
      " - Processing query 183/673...\n",
      " - Processing query 184/673...\n",
      " - Processing query 185/673...\n",
      " - Processing query 186/673...\n",
      " - Processing query 187/673...\n",
      " - Processing query 188/673...\n",
      " - Processing query 189/673...\n",
      " - Processing query 190/673...\n",
      " - Processing query 191/673...\n",
      " - Processing query 192/673...\n",
      " - Processing query 193/673...\n",
      " - Processing query 194/673...\n",
      " - Processing query 195/673...\n",
      " - Processing query 196/673...\n",
      " - Processing query 197/673...\n",
      " - Processing query 198/673...\n",
      " - Processing query 199/673...\n",
      " - Processing query 200/673...\n",
      " - Processing query 201/673...\n",
      " - Processing query 202/673...\n",
      " - Processing query 203/673...\n",
      " - Processing query 204/673...\n",
      " - Processing query 205/673...\n",
      " - Processing query 206/673...\n",
      " - Processing query 207/673...\n",
      " - Processing query 208/673...\n",
      " - Processing query 209/673...\n",
      " - Processing query 210/673...\n",
      " - Processing query 211/673...\n",
      " - Processing query 212/673...\n",
      " - Processing query 213/673...\n",
      " - Processing query 214/673...\n",
      " - Processing query 215/673...\n",
      " - Processing query 216/673...\n",
      " - Processing query 217/673...\n",
      " - Processing query 218/673...\n",
      " - Processing query 219/673...\n",
      " - Processing query 220/673...\n",
      " - Processing query 221/673...\n",
      " - Processing query 222/673...\n",
      " - Processing query 223/673...\n",
      " - Processing query 224/673...\n",
      " - Processing query 225/673...\n",
      " - Processing query 226/673...\n",
      " - Processing query 227/673...\n",
      " - Processing query 228/673...\n",
      " - Processing query 229/673...\n",
      " - Processing query 230/673...\n",
      " - Processing query 231/673...\n",
      " - Processing query 232/673...\n",
      " - Processing query 233/673...\n",
      " - Processing query 234/673...\n",
      " - Processing query 235/673...\n",
      " - Processing query 236/673...\n",
      " - Processing query 237/673...\n",
      " - Processing query 238/673...\n",
      " - Processing query 239/673...\n",
      " - Processing query 240/673...\n",
      " - Processing query 241/673...\n",
      " - Processing query 242/673...\n",
      " - Processing query 243/673...\n",
      " - Processing query 244/673...\n",
      " - Processing query 245/673...\n",
      " - Processing query 246/673...\n",
      " - Processing query 247/673...\n",
      " - Processing query 248/673...\n",
      " - Processing query 249/673...\n",
      " - Processing query 250/673...\n",
      " - Processing query 251/673...\n",
      " - Processing query 252/673...\n",
      " - Processing query 253/673...\n",
      " - Processing query 254/673...\n",
      " - Processing query 255/673...\n",
      " - Processing query 256/673...\n",
      " - Processing query 257/673...\n",
      " - Processing query 258/673...\n",
      " - Processing query 259/673...\n",
      " - Processing query 260/673...\n",
      " - Processing query 261/673...\n",
      " - Processing query 262/673...\n",
      " - Processing query 263/673...\n",
      " - Processing query 264/673...\n",
      " - Processing query 265/673...\n",
      " - Processing query 266/673...\n",
      " - Processing query 267/673...\n",
      " - Processing query 268/673...\n",
      " - Processing query 269/673...\n",
      " - Processing query 270/673...\n",
      " - Processing query 271/673...\n",
      " - Processing query 272/673...\n",
      " - Processing query 273/673...\n",
      " - Processing query 274/673...\n",
      " - Processing query 275/673...\n",
      " - Processing query 276/673...\n",
      " - Processing query 277/673...\n",
      " - Processing query 278/673...\n",
      " - Processing query 279/673...\n",
      " - Processing query 280/673...\n",
      " - Processing query 281/673...\n",
      " - Processing query 282/673...\n",
      " - Processing query 283/673...\n",
      " - Processing query 284/673...\n",
      " - Processing query 285/673...\n",
      " - Processing query 286/673...\n",
      " - Processing query 287/673...\n",
      " - Processing query 288/673...\n",
      " - Processing query 289/673...\n",
      " - Processing query 290/673...\n",
      " - Processing query 291/673...\n",
      " - Processing query 292/673...\n",
      " - Processing query 293/673...\n",
      " - Processing query 294/673...\n",
      " - Processing query 295/673...\n",
      " - Processing query 296/673...\n",
      " - Processing query 297/673...\n",
      " - Processing query 298/673...\n",
      " - Processing query 299/673...\n",
      " - Processing query 300/673...\n",
      " - Processing query 301/673...\n",
      " - Processing query 302/673...\n",
      " - Processing query 303/673...\n",
      " - Processing query 304/673...\n",
      " - Processing query 305/673...\n",
      " - Processing query 306/673...\n",
      " - Processing query 307/673...\n",
      " - Processing query 308/673...\n",
      " - Processing query 309/673...\n",
      " - Processing query 310/673...\n",
      " - Processing query 311/673...\n",
      " - Processing query 312/673...\n",
      " - Processing query 313/673...\n",
      " - Processing query 314/673...\n",
      " - Processing query 315/673...\n",
      " - Processing query 316/673...\n",
      " - Processing query 317/673...\n",
      " - Processing query 318/673...\n",
      " - Processing query 319/673...\n",
      " - Processing query 320/673...\n",
      " - Processing query 321/673...\n",
      " - Processing query 322/673...\n",
      " - Processing query 323/673...\n",
      " - Processing query 324/673...\n",
      " - Processing query 325/673...\n",
      " - Processing query 326/673...\n",
      " - Processing query 327/673...\n",
      " - Processing query 328/673...\n",
      " - Processing query 329/673...\n",
      " - Processing query 330/673...\n",
      " - Processing query 331/673...\n",
      " - Processing query 332/673...\n",
      " - Processing query 333/673...\n",
      " - Processing query 334/673...\n",
      " - Processing query 335/673...\n",
      " - Processing query 336/673...\n",
      " - Processing query 337/673...\n",
      " - Processing query 338/673...\n",
      " - Processing query 339/673...\n",
      " - Processing query 340/673...\n",
      " - Processing query 341/673...\n",
      " - Processing query 342/673...\n",
      " - Processing query 343/673...\n",
      " - Processing query 344/673...\n",
      " - Processing query 345/673...\n",
      " - Processing query 346/673...\n",
      " - Processing query 347/673...\n",
      " - Processing query 348/673...\n",
      " - Processing query 349/673...\n",
      " - Processing query 350/673...\n",
      " - Processing query 351/673...\n",
      " - Processing query 352/673...\n",
      " - Processing query 353/673...\n",
      " - Processing query 354/673...\n",
      " - Processing query 355/673...\n",
      " - Processing query 356/673...\n",
      " - Processing query 357/673...\n",
      " - Processing query 358/673...\n",
      " - Processing query 359/673...\n",
      " - Processing query 360/673...\n",
      " - Processing query 361/673...\n",
      " - Processing query 362/673...\n",
      " - Processing query 363/673...\n",
      " - Processing query 364/673...\n",
      " - Processing query 365/673...\n",
      " - Processing query 366/673...\n",
      " - Processing query 367/673...\n",
      " - Processing query 368/673...\n",
      " - Processing query 369/673...\n",
      " - Processing query 370/673...\n",
      " - Processing query 371/673...\n",
      " - Processing query 372/673...\n",
      " - Processing query 373/673...\n",
      " - Processing query 374/673...\n",
      " - Processing query 375/673...\n",
      " - Processing query 376/673...\n",
      " - Processing query 377/673...\n",
      " - Processing query 378/673...\n",
      " - Processing query 379/673...\n",
      " - Processing query 380/673...\n",
      " - Processing query 381/673...\n",
      " - Processing query 382/673...\n",
      " - Processing query 383/673...\n",
      " - Processing query 384/673...\n",
      " - Processing query 385/673...\n",
      " - Processing query 386/673...\n",
      " - Processing query 387/673...\n",
      " - Processing query 388/673...\n",
      " - Processing query 389/673...\n",
      " - Processing query 390/673...\n",
      " - Processing query 391/673...\n",
      " - Processing query 392/673...\n",
      " - Processing query 393/673...\n",
      " - Processing query 394/673...\n",
      " - Processing query 395/673...\n",
      " - Processing query 396/673...\n",
      " - Processing query 397/673...\n",
      " - Processing query 398/673...\n",
      " - Processing query 399/673...\n",
      " - Processing query 400/673...\n",
      " - Processing query 401/673...\n",
      " - Processing query 402/673...\n",
      " - Processing query 403/673...\n",
      " - Processing query 404/673...\n",
      " - Processing query 405/673...\n",
      " - Processing query 406/673...\n",
      " - Processing query 407/673...\n",
      " - Processing query 408/673...\n",
      " - Processing query 409/673...\n",
      " - Processing query 410/673...\n",
      " - Processing query 411/673...\n",
      " - Processing query 412/673...\n",
      " - Processing query 413/673...\n",
      " - Processing query 414/673...\n",
      " - Processing query 415/673...\n",
      " - Processing query 416/673...\n",
      " - Processing query 417/673...\n",
      " - Processing query 418/673...\n",
      " - Processing query 419/673...\n",
      " - Processing query 420/673...\n",
      " - Processing query 421/673...\n",
      " - Processing query 422/673...\n",
      " - Processing query 423/673...\n",
      " - Processing query 424/673...\n",
      " - Processing query 425/673...\n",
      " - Processing query 426/673...\n",
      " - Processing query 427/673...\n",
      " - Processing query 428/673...\n",
      " - Processing query 429/673...\n",
      " - Processing query 430/673...\n",
      " - Processing query 431/673...\n",
      " - Processing query 432/673...\n",
      " - Processing query 433/673...\n",
      " - Processing query 434/673...\n",
      " - Processing query 435/673...\n",
      " - Processing query 436/673...\n",
      " - Processing query 437/673...\n",
      " - Processing query 438/673...\n",
      " - Processing query 439/673...\n",
      " - Processing query 440/673...\n",
      " - Processing query 441/673...\n",
      " - Processing query 442/673...\n",
      " - Processing query 443/673...\n",
      " - Processing query 444/673...\n",
      " - Processing query 445/673...\n",
      " - Processing query 446/673...\n",
      " - Processing query 447/673...\n",
      " - Processing query 448/673...\n",
      " - Processing query 449/673...\n",
      " - Processing query 450/673...\n",
      " - Processing query 451/673...\n",
      " - Processing query 452/673...\n",
      " - Processing query 453/673...\n",
      " - Processing query 454/673...\n",
      " - Processing query 455/673...\n",
      " - Processing query 456/673...\n",
      " - Processing query 457/673...\n",
      " - Processing query 458/673...\n",
      " - Processing query 459/673...\n",
      " - Processing query 460/673...\n",
      " - Processing query 461/673...\n",
      " - Processing query 462/673...\n",
      " - Processing query 463/673...\n",
      " - Processing query 464/673...\n",
      " - Processing query 465/673...\n",
      " - Processing query 466/673...\n",
      " - Processing query 467/673...\n",
      " - Processing query 468/673...\n",
      " - Processing query 469/673...\n",
      " - Processing query 470/673...\n",
      " - Processing query 471/673...\n",
      " - Processing query 472/673...\n",
      " - Processing query 473/673...\n",
      " - Processing query 474/673...\n",
      " - Processing query 475/673...\n",
      " - Processing query 476/673...\n",
      " - Processing query 477/673...\n",
      " - Processing query 478/673...\n",
      " - Processing query 479/673...\n",
      " - Processing query 480/673...\n",
      " - Processing query 481/673...\n",
      " - Processing query 482/673...\n",
      " - Processing query 483/673...\n",
      " - Processing query 484/673...\n",
      " - Processing query 485/673...\n",
      " - Processing query 486/673...\n",
      " - Processing query 487/673...\n",
      " - Processing query 488/673...\n",
      " - Processing query 489/673...\n",
      " - Processing query 490/673...\n",
      " - Processing query 491/673...\n",
      " - Processing query 492/673...\n",
      " - Processing query 493/673...\n",
      " - Processing query 494/673...\n",
      " - Processing query 495/673...\n",
      " - Processing query 496/673...\n",
      " - Processing query 497/673...\n",
      " - Processing query 498/673...\n",
      " - Processing query 499/673...\n",
      " - Processing query 500/673...\n",
      " - Processing query 501/673...\n",
      " - Processing query 502/673...\n",
      " - Processing query 503/673...\n",
      " - Processing query 504/673...\n",
      " - Processing query 505/673...\n",
      " - Processing query 506/673...\n",
      " - Processing query 507/673...\n",
      " - Processing query 508/673...\n",
      " - Processing query 509/673...\n",
      " - Processing query 510/673...\n",
      " - Processing query 511/673...\n",
      " - Processing query 512/673...\n",
      " - Processing query 513/673...\n",
      " - Processing query 514/673...\n",
      " - Processing query 515/673...\n",
      " - Processing query 516/673...\n",
      " - Processing query 517/673...\n",
      " - Processing query 518/673...\n",
      " - Processing query 519/673...\n",
      " - Processing query 520/673...\n",
      " - Processing query 521/673...\n",
      " - Processing query 522/673...\n",
      " - Processing query 523/673...\n",
      " - Processing query 524/673...\n",
      " - Processing query 525/673...\n",
      " - Processing query 526/673...\n",
      " - Processing query 527/673...\n",
      " - Processing query 528/673...\n",
      " - Processing query 529/673...\n",
      " - Processing query 530/673...\n",
      " - Processing query 531/673...\n",
      " - Processing query 532/673...\n",
      " - Processing query 533/673...\n",
      " - Processing query 534/673...\n",
      " - Processing query 535/673...\n",
      " - Processing query 536/673...\n",
      " - Processing query 537/673...\n",
      " - Processing query 538/673...\n",
      " - Processing query 539/673...\n",
      " - Processing query 540/673...\n",
      " - Processing query 541/673...\n",
      " - Processing query 542/673...\n",
      " - Processing query 543/673...\n",
      " - Processing query 544/673...\n",
      " - Processing query 545/673...\n",
      " - Processing query 546/673...\n",
      " - Processing query 547/673...\n",
      " - Processing query 548/673...\n",
      " - Processing query 549/673...\n",
      " - Processing query 550/673...\n",
      " - Processing query 551/673...\n",
      " - Processing query 552/673...\n",
      " - Processing query 553/673...\n",
      " - Processing query 554/673...\n",
      " - Processing query 555/673...\n",
      " - Processing query 556/673...\n",
      " - Processing query 557/673...\n",
      " - Processing query 558/673...\n",
      " - Processing query 559/673...\n",
      " - Processing query 560/673...\n",
      " - Processing query 561/673...\n",
      " - Processing query 562/673...\n",
      " - Processing query 563/673...\n",
      " - Processing query 564/673...\n",
      " - Processing query 565/673...\n",
      " - Processing query 566/673...\n",
      " - Processing query 567/673...\n",
      " - Processing query 568/673...\n",
      " - Processing query 569/673...\n",
      " - Processing query 570/673...\n",
      " - Processing query 571/673...\n",
      " - Processing query 572/673...\n",
      " - Processing query 573/673...\n",
      " - Processing query 574/673...\n",
      " - Processing query 575/673...\n",
      " - Processing query 576/673...\n",
      " - Processing query 577/673...\n",
      " - Processing query 578/673...\n",
      " - Processing query 579/673...\n",
      " - Processing query 580/673...\n",
      " - Processing query 581/673...\n",
      " - Processing query 582/673...\n",
      " - Processing query 583/673...\n",
      " - Processing query 584/673...\n",
      " - Processing query 585/673...\n",
      " - Processing query 586/673...\n",
      " - Processing query 587/673...\n",
      " - Processing query 588/673...\n",
      " - Processing query 589/673...\n",
      " - Processing query 590/673...\n",
      " - Processing query 591/673...\n",
      " - Processing query 592/673...\n",
      " - Processing query 593/673...\n",
      " - Processing query 594/673...\n",
      " - Processing query 595/673...\n",
      " - Processing query 596/673...\n",
      " - Processing query 597/673...\n",
      " - Processing query 598/673...\n",
      " - Processing query 599/673...\n",
      " - Processing query 600/673...\n",
      " - Processing query 601/673...\n",
      " - Processing query 602/673...\n",
      " - Processing query 603/673...\n",
      " - Processing query 604/673...\n",
      " - Processing query 605/673...\n",
      " - Processing query 606/673...\n",
      " - Processing query 607/673...\n",
      " - Processing query 608/673...\n",
      " - Processing query 609/673...\n",
      " - Processing query 610/673...\n",
      " - Processing query 611/673...\n",
      " - Processing query 612/673...\n",
      " - Processing query 613/673...\n",
      " - Processing query 614/673...\n",
      " - Processing query 615/673...\n",
      " - Processing query 616/673...\n",
      " - Processing query 617/673...\n",
      " - Processing query 618/673...\n",
      " - Processing query 619/673...\n",
      " - Processing query 620/673...\n",
      " - Processing query 621/673...\n",
      " - Processing query 622/673...\n",
      " - Processing query 623/673...\n",
      " - Processing query 624/673...\n",
      " - Processing query 625/673...\n",
      " - Processing query 626/673...\n",
      " - Processing query 627/673...\n",
      " - Processing query 628/673...\n",
      " - Processing query 629/673...\n",
      " - Processing query 630/673...\n",
      " - Processing query 631/673...\n",
      " - Processing query 632/673...\n",
      " - Processing query 633/673...\n",
      " - Processing query 634/673...\n",
      " - Processing query 635/673...\n",
      " - Processing query 636/673...\n",
      " - Processing query 637/673...\n",
      " - Processing query 638/673...\n",
      " - Processing query 639/673...\n",
      " - Processing query 640/673...\n",
      " - Processing query 641/673...\n",
      " - Processing query 642/673...\n",
      " - Processing query 643/673...\n",
      " - Processing query 644/673...\n",
      " - Processing query 645/673...\n",
      " - Processing query 646/673...\n",
      " - Processing query 647/673...\n",
      " - Processing query 648/673...\n",
      " - Processing query 649/673...\n",
      " - Processing query 650/673...\n",
      " - Processing query 651/673...\n",
      " - Processing query 652/673...\n",
      " - Processing query 653/673...\n",
      " - Processing query 654/673...\n",
      " - Processing query 655/673...\n",
      " - Processing query 656/673...\n",
      " - Processing query 657/673...\n",
      " - Processing query 658/673...\n",
      " - Processing query 659/673...\n",
      " - Processing query 660/673...\n",
      " - Processing query 661/673...\n",
      " - Processing query 662/673...\n",
      " - Processing query 663/673...\n",
      " - Processing query 664/673...\n",
      " - Processing query 665/673...\n",
      " - Processing query 666/673...\n",
      " - Processing query 667/673...\n",
      " - Processing query 668/673...\n",
      " - Processing query 669/673...\n",
      " - Processing query 670/673...\n",
      " - Processing query 671/673...\n",
      " - Processing query 672/673...\n",
      " - Processing query 673/673...\n",
      "\n",
      "Generated 673 total examples. Now splitting into train and test sets.\n",
      "Train set size: 538\n",
      "Test set size: 135\n",
      "\n",
      "‚úÖ Successfully saved training dataset to `data/final_rft_sql_train_data.jsonl`.\n",
      "‚úÖ Successfully saved test dataset to `data/final_rft_sql_test_data.jsonl`.\n",
      "\n",
      "--- Example RFT training entry ---\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nYou are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\\n\\n**Database Schema:**\\n| database              | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\\n|:----------------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\\n| synthetic_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\\n|                       |          |           |                                                                            |  'VARCHAR']                                                           |             |\\n| synthetic_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\\n|                       |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\\n| synthetic_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR'   | False       |\\n|                       |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\\n|                       |          |           |  'equipment']                                                              |                                                                       |             |\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What are the top 10 active airlines from the United States, Canada, or Mexico with the most international routes, where international routes are defined as routes where the source and destination airports are in different countries?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"SELECT a.name AS airline_name, COUNT(r.destination_airport_id) AS international_routes FROM airlines a JOIN routes r ON a.airline_id = r.airline_id JOIN airports s ON r.source_airport_id = s.airport_id JOIN airports d ON r.destination_airport_id = d.airport_id WHERE s.country != d.country AND a.active = 'Y' AND a.country IN ('United States', 'Canada', 'Mexico') GROUP BY a.name ORDER BY international_routes DESC, a.name ASC LIMIT 10\"\n",
      "    }\n",
      "  ],\n",
      "  \"ground_truth\": [\n",
      "    {\n",
      "      \"airline_name\": \"AirQuest\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Arctic Air\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Blue Horizon Express\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"EagleWing Airlines\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"EagleWings International\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Global Voyager Airways\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Global Wings\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Golden Globe Airways\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"Golden Wings Airlines\",\n",
      "      \"international_routes\": 1\n",
      "    },\n",
      "    {\n",
      "      \"airline_name\": \"HighFly Airlines\",\n",
      "      \"international_routes\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import jsonlines\n",
    "from typing import List\n",
    "import random\n",
    "from fireworks import LLM\n",
    "\n",
    "# --- 1. Define File Paths and Parameters ---\n",
    "llm = LLM(model=\"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct\", deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "FINAL_TRAINING_DATA_PATH = \"data/final_rft_sql_train_data.jsonl\"\n",
    "FINAL_TEST_DATA_PATH = \"data/final_rft_sql_test_data.jsonl\"\n",
    "\n",
    "# --- 2. Load Ground-Truth Data ---\n",
    "query_result_pairs = []\n",
    "with jsonlines.open(GROUND_TRUTH_FILE_PATH) as reader:\n",
    "    for obj in reader:\n",
    "        query_result_pairs.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(query_result_pairs)} query-result pairs.\")\n",
    "\n",
    "# --- 3. Use LLM to Generate Natural Language Questions ---\n",
    "nl_generation_prompt_template = f\"\"\"\n",
    "You are an expert data analyst who is great at translating SQL queries into plain English.\n",
    "Based on the database schema and the provided SQL query, what is a natural language question a business user would ask to get this information?\n",
    "Ensure that the question is precise enough to accurately map to the corresponding SQL query.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\n",
    "**SQL Query:**\n",
    "```sql\n",
    "{{query}}\n",
    "```\n",
    "\n",
    "Provide only the user's question, without any preamble or explanation.\n",
    "\"\"\"\n",
    "\n",
    "# The system prompt that will be included in the final training data for the RFT job.\n",
    "# It gives the model its instructions at inference time.\n",
    "rft_system_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "final_generated_data = []\n",
    "print(f\"Generating natural language questions and formatting for RFT for {len(query_result_pairs)} queries...\")\n",
    "\n",
    "for i, pair in enumerate(query_result_pairs):\n",
    "    print(f\" - Processing query {i+1}/{len(query_result_pairs)}...\")\n",
    "    query = pair['query']\n",
    "    ground_truth = pair['result']\n",
    "    nl_generation_prompt = nl_generation_prompt_template.format(query=query)\n",
    "    \n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": nl_generation_prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    nl_question = response.choices[0].message.content\n",
    "    if nl_question:  # Only include the entry if the LLM generated a question\n",
    "        # Assemble the final data structure\n",
    "        rft_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": rft_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": nl_question.strip()},\n",
    "                {\"role\": \"assistant\", \"content\": query}\n",
    "            ],\n",
    "            \"ground_truth\": ground_truth  # The ground-truth result for the evaluator\n",
    "        }\n",
    "        final_generated_data.append(rft_entry)\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 4. Shuffle and Split the Dataset ---\n",
    "print(f\"\\nGenerated {len(final_generated_data)} total examples. Now splitting into train and test sets.\")\n",
    "random.seed(42)\n",
    "random.shuffle(final_generated_data)\n",
    "\n",
    "split_index = int(len(final_generated_data) * 0.8)\n",
    "train_data = final_generated_data[:split_index]\n",
    "test_data = final_generated_data[split_index:]\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# --- 5. Save the Final RFT-Ready Datasets ---\n",
    "with jsonlines.open(FINAL_TRAINING_DATA_PATH, mode='w') as writer:\n",
    "    writer.write_all(train_data)\n",
    "print(f\"\\n‚úÖ Successfully saved training dataset to `{FINAL_TRAINING_DATA_PATH}`.\")\n",
    "\n",
    "with jsonlines.open(FINAL_TEST_DATA_PATH, mode='w') as writer:\n",
    "    writer.write_all(test_data)\n",
    "print(f\"‚úÖ Successfully saved test dataset to `{FINAL_TEST_DATA_PATH}`.\")\n",
    "\n",
    "# --- 6. Print an Example ---\n",
    "if train_data:\n",
    "    print(\"\\n--- Example RFT training entry ---\")\n",
    "    print(json.dumps(train_data[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. üõ∞Ô∏è Deploy an MCP Server for the Synthetic Data\n",
    "Now, we'll start a remote server that speaks the Model Context Protocol (MCP). This server will wrap our synthetic DuckDB database, providing a standardized way for any external tool‚Äîin our case, the Fireworks RFT evaluator‚Äîto interact with it.\n",
    "\n",
    "#### What is MCP?\n",
    "The Model Context Protocol is an open standard that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals, MCP provides a standardized way to connect AI models to various data sources and tools.\n",
    "\n",
    "Key benefits:\n",
    "- **Flexibility**: Works with any data source or tool\n",
    "- **Standardization**: One protocol for all integrations instead of custom APIs for each tool; MCP servers for many applications are readily available\n",
    "\n",
    "> Real World üåç: This pattern is directly applicable. You would run a similar MCP server to provide a secure, read-only interface to a production database replica or a data warehouse, allowing the fine-tuning process to happen without granting direct database credentials to the training environment.\n",
    "\n",
    "9. a) Create a server script in this project's root directory (`run_mcp_server.py`). This Python script starts our database server. It is configured to be read-only.\n",
    "\n",
    "```python\n",
    "    import os, contextlib, uvicorn\n",
    "    from starlette.applications import Starlette\n",
    "    from starlette.routing import Mount\n",
    "    from mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n",
    "    from mcp_server_motherduck import build_application\n",
    "\n",
    "    DB = \"data/synthetic_openflights.db\"          # ‚Üê path from previous steps\n",
    "    PORT = int(os.environ.get(\"PORT\", 8080))        # Cloud Run injects $PORT\n",
    "\n",
    "    # 1Ô∏è‚É£ Build the core SQL-aware MCP server (read-only for safety).\n",
    "    server, _ = build_application(db_path=DB, read_only=True)\n",
    "\n",
    "    # 2Ô∏è‚É£ Wrap it so HTTP clients can talk to it (ASGI handler).\n",
    "    sess = StreamableHTTPSessionManager(app=server, event_store=None, stateless=True)\n",
    "\n",
    "    async def handler(scope, receive, send):\n",
    "        await sess.handle_request(scope, receive, send)\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def lifespan(app):\n",
    "        async with sess.run():\n",
    "            yield                                        # keep sessions alive\n",
    "\n",
    "    # 3Ô∏è‚É£ Starlette turns that handler into a full ASGI app Uvicorn can serve.\n",
    "    app = Starlette(routes=[Mount(\"/mcp\", app=handler)], lifespan=lifespan)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        print(f\"üî• MCP endpoint ‚Üí http://0.0.0.0:{PORT}/mcp\")\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. ‚òÅÔ∏è Set Up Google Cloud CLI & .gcloudignore\n",
    "We'll first set up the Google Cloud CLI and authenticate. Google Cloud Run provides an easy way to deploy containerized applications without managing infrastructure.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You would follow along here in the same way. Cloud Run is ideal for MCP servers because it auto-scales based on demand (down to zero when not in use, thus charging only for actual usage).\n",
    "\n",
    "10. a) **Install** the SDK (macOS/Linux):\n",
    "\n",
    "      ```bash\n",
    "      curl -sSL https://sdk.cloud.google.com | bash\n",
    "      exec -l $SHELL  # reload shell so 'gcloud' is available\n",
    "      ```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. b) **Log in** (creates local access token):\n",
    "      ```bash\n",
    "      gcloud auth login\n",
    "      ```\n",
    "\n",
    "<br>\n",
    "\n",
    "10. c) **Set your active project desired gcloud project**:\n",
    "      ```bash\n",
    "      gcloud config set project < YOUR_PROJECT_ID >  # set up project in gcloud console before running this if not already done\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. üì¶ Containerize & Deploy the MCP Server\n",
    "We‚Äôll build a Docker image and push it straight to Cloud Run.  \n",
    "Remember to replace **`YOUR_PROJECT_ID`** with the project you actually want to bill.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You would follow along in the same way here.\n",
    "\n",
    "11. a) Create `mcp_requirements.txt` containing the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "mcp\n",
    "mcp-server-motherduck\n",
    "duckdb\n",
    "uvicorn\n",
    "starlette\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "11. b) Create a `Dockerfile` (no extension) containing the following\n",
    "```bash\n",
    "base\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "\n",
    "COPY mcp_requirements.txt .\n",
    "RUN pip install --no-cache-dir -r mcp_requirements.txt\n",
    "\n",
    "COPY run_mcp_server.py .\n",
    "COPY data/synthetic_openflights.db ./data/\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"python\", \"run_mcp_server.py\"]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "11. c) Create a .gcloudignore file in your root dir (to only deploy files needed for MCP server) containing:\n",
    "```bash\n",
    "# .gcloudignore\n",
    "\n",
    "# 1. Ignore EVERYTHING in the directory by default.\n",
    "*\n",
    "\n",
    "# 2. Now, create exceptions for ONLY the files needed by the Dockerfile.\n",
    "# The \"!\" character means \"do not ignore this file\".\n",
    "\n",
    "# The Dockerfile itself is needed for the build process.\n",
    "!Dockerfile\n",
    "\n",
    "# The files explicitly copied by your Dockerfile:\n",
    "!mcp_requirements.txt\n",
    "!run_mcp_server.py\n",
    "\n",
    "# 3. To include a specific file in a subdirectory, use this\n",
    "#    three-line pattern to un-ignore the directory, re-ignore its\n",
    "#    contents, and then un-ignore the specific file.\n",
    "!data/\n",
    "data/*\n",
    "!data/synthetic_openflights.db\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "11. d) Deploy your MCP server as a Cloud Run app by running (from your project root):\n",
    "```bash\n",
    "FIREWORKS_API_KEY=$(grep FIREWORKS_API_KEY .env | cut -d '=' -f2) reward-kit deploy-mcp \\\n",
    "--id mcp-sql-rft-server \\\n",
    "--dockerfile Dockerfile \\\n",
    "--port 8080 \\\n",
    "--gcp-project < YOUR_GCP_PROJECT_ID > \\\n",
    "--gcp-region < YOUR_GCP_REGION >\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "11. e) Test that your MCP server is working as expected by running the following from your terminal:\n",
    "11. e) i. To get your MCP server's URL:\n",
    "```bash\n",
    "gcloud run services describe mcp-sql-rft-server \\\n",
    "--project < YOUR_GCP_PROJECT_ID > \\\n",
    "--region < YOUR_GCP_REGION > \\\n",
    "--format=\"value(status.url)\"\n",
    "```\n",
    "\n",
    "11. e) ii. (optional) To check the names of the MCP server's available tools:\n",
    "```bash\n",
    "curl -X POST \"< YOUR_MCP_SERVER_URL_FROM_STEP_i >/mcp/\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Accept: application/json, text/event-stream\" \\\n",
    "-d '{\n",
    "    \"id\": \"list-tools-1\",\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"params\": {\n",
    "        \"session\": {\"id\": \"test-from-my-laptop\"}\n",
    "    }\n",
    "}'\n",
    "```\n",
    ">Note that the above is a generally useful way to check an MCP server's tools.\n",
    ">In this case, the tool of interest is the \"query\" tool.\n",
    "\n",
    "11. e) iii. To send a test request to the MCP server:\n",
    "```bash\n",
    "curl -X POST \"< YOUR_MCP_SERVER_URL_FROM_STEP_i >/mcp/\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Accept: application/json, text/event-stream\" \\\n",
    "-d '{\n",
    "    \"id\": \"query-1\",\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"session\": {\"id\": \"test-from-my-laptop\"},\n",
    "        \"name\": \"query\",\n",
    "        \"arguments\": {\n",
    "            \"query\": \"SELECT COUNT(*) FROM airlines;\"\n",
    "        }\n",
    "    }\n",
    "}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. üîç Define an evaluation function for RFT\n",
    "Here, we define an `evaluate` function for RFT, which will interface with our MCP server. Note that you will not directly execute the function here, but will use it as part of the Fireworks Evaluations UI.\n",
    "\n",
    "#### Understanding the Evaluation Function:\n",
    "The evaluation function is the heart of RFT. It:\n",
    "1. Receives the model's generated SQL query\n",
    "2. Executes it against the real database (via MCP)\n",
    "3. Compares the result with ground truth\n",
    "4. Returns a reward score (0 or 1)\n",
    "\n",
    "This binary reward signal drives the reinforcement learning process. The model learns through trial and error which SQL patterns lead to correct results.\n",
    "\n",
    "Key design decisions:\n",
    "- **Exact match comparison**: We normalize values and sort rows to handle different but equivalent result orderings\n",
    "- **Robust error handling**: SQL syntax errors or execution failures return a score of 0\n",
    "- **Detailed reasoning**: The function returns explanatory messages for debugging\n",
    "\n",
    "Ensure that you set MCP_SERVER_URL to be your actual MCP server URL from step 11. e) i.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You would follow along in the same way here. The evaluation function could also be further customized, with, for example:\n",
    "> - Partial credit for near-correct answers\n",
    "> - Performance-based rewards (faster queries get higher scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "MCP_SERVER_URL = None  # <--- PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "\n",
    "def evaluate(messages: list[dict], ground_truth: list[dict], **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates the model's generated SQL query by executing it against a live\n",
    "    MCP server and comparing the result with the ground_truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "        This version robustly handles 'NULL' values and empty strings.\n",
    "        \"\"\"\n",
    "        lines = table_string.strip().split('\\n')\n",
    "        content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "        if len(content_lines) < 2:\n",
    "            return []\n",
    "        \n",
    "        header_raw = [h.strip() for h in content_lines[0].split('|')[1:-1]]\n",
    "        data_lines = content_lines[1:]\n",
    "        \n",
    "        if len(data_lines) > 0:\n",
    "            try:\n",
    "                first_data_values = [v.strip() for v in data_lines[0].split('|')[1:-1]]\n",
    "                if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "                    data_lines = data_lines[1:]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        rows = []\n",
    "        for line in data_lines:\n",
    "            try:\n",
    "                values_raw = [v.strip() for v in line.split('|')[1:-1]]\n",
    "                if len(values_raw) == len(header_raw):\n",
    "                    row_dict = {}\n",
    "                    for i, header in enumerate(header_raw):\n",
    "                        value_str = values_raw[i]\n",
    "                        if value_str.upper() == 'NULL' or value_str == '':\n",
    "                            row_dict[header] = None\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            if '.' in value_str:\n",
    "                                row_dict[header] = float(value_str)\n",
    "                            else:\n",
    "                                row_dict[header] = int(value_str)\n",
    "                        except (ValueError, TypeError):\n",
    "                            row_dict[header] = value_str\n",
    "                    rows.append(row_dict)\n",
    "            except IndexError:\n",
    "                continue\n",
    "        return rows\n",
    "\n",
    "    # --- 1. Get MCP Server URL from Environment Variables ---\n",
    "    mcp_server_url = MCP_SERVER_URL\n",
    "    if not mcp_server_url:\n",
    "        return {\"score\": 0, \"is_score_valid\": False, \"reason\": \"FATAL: MCP_SERVER_URL environment variable is not set.\"}\n",
    "\n",
    "    # --- 2. Get the SQL query from the model's response ---\n",
    "    sql_query = messages[-1]['content'].strip()\n",
    "    if not sql_query:\n",
    "        return {\"score\": 0, \"reason\": \"Model returned an empty response.\"}\n",
    "\n",
    "    # --- 3. Execute the Query against the MCP Server ---\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json, text/event-stream\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\",\n",
    "        \"params\": {\"session\": {\"id\": \"stateless-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": sql_query}}\n",
    "    }\n",
    "    try:\n",
    "        with requests.post(f\"{mcp_server_url}/mcp/\", headers=headers, json=payload, timeout=15, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            response_data = None\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    decoded_line = line.decode('utf-8')\n",
    "                    if decoded_line.startswith('data:'):\n",
    "                        json_part = decoded_line[len('data:'):].strip()\n",
    "                        if json_part:\n",
    "                            response_data = json.loads(json_part)\n",
    "                            break\n",
    "            if response_data is None:\n",
    "                return {\"score\": 0, \"reason\": \"Could not find JSON data in event stream response from MCP server.\"}\n",
    "\n",
    "        if \"error\" in response_data:\n",
    "            return {\"score\": 0, \"reason\": f\"SQL execution failed. Error: {response_data['error'].get('message', 'Unknown')}\"}\n",
    "\n",
    "        ascii_table = response_data['result']['content'][0]['text']\n",
    "        predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"score\": 0, \"reason\": f\"Network error calling MCP server: {e}\"}\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"score\": 0, \"reason\": f\"JSON decode error from server response: {e}\"}\n",
    "    except (KeyError, IndexError):\n",
    "        return {\"score\": 0, \"reason\": f\"Failed to parse predicted result from MCP server response structure. Data found: {json.dumps(response_data)}\"}\n",
    "    except Exception as e:\n",
    "        return {\"score\": 0, \"reason\": f\"An unexpected error occurred during query execution: {e}\"}\n",
    "\n",
    "    # --- 4. Process Ground Truth ---\n",
    "    if not isinstance(ground_truth, list):\n",
    "        return {\"score\": 0, \"is_score_valid\": False, \"reason\": f\"FATAL: ground_truth is not a list as expected. Got type: {type(ground_truth)}\"}\n",
    "    ground_truth_rows = ground_truth\n",
    "\n",
    "\n",
    "    # --- 5. Comparison Logic ---\n",
    "    def normalize_and_stringify(v):\n",
    "        \"\"\"\n",
    "        Normalizes numbers and None before string conversion.\n",
    "        \"\"\"\n",
    "        if v is None:\n",
    "            return str(v)\n",
    "        \n",
    "        if isinstance(v, float) and not math.isinf(v) and not math.isnan(v) and v == int(v):\n",
    "            v = int(v)\n",
    "        return str(v)\n",
    "\n",
    "    try:\n",
    "        gt_values = sorted([sorted(map(normalize_and_stringify, row.values())) for row in ground_truth_rows])\n",
    "        predicted_values = sorted([sorted(map(normalize_and_stringify, row.values())) for row in predicted_rows])\n",
    "\n",
    "        if gt_values == predicted_values:\n",
    "            score = 1\n",
    "            reason = \"Success: The SQL query produced the exact expected result.\"\n",
    "        else:\n",
    "            score = 0\n",
    "            gt_json = json.dumps(ground_truth_rows)\n",
    "            pred_json = json.dumps(predicted_rows)\n",
    "            reason = f\"Incorrect result. Expected (from ground_truth): {gt_json}. Got (from query): {pred_json}.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"score\": 0, \"reason\": f\"Error during result comparison: {e}\"}\n",
    "\n",
    "    return {\"score\": score, \"reason\": reason}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. üß™ Test English -> SQL of a base model without fine-tuning\n",
    "Here, we test a base model's ability to generate SQL from a natural language question on a single example from our training data.\n",
    "\n",
    "This is a quick sanity check that:\n",
    "1. **Verifies your MCP server is working**: Ensures the server is accessible and can execute queries\n",
    "2. **Tests the full pipeline**: Confirms that the flow from natural language ‚Üí SQL generation ‚Üí execution ‚Üí result parsing works end-to-end\n",
    "3. **Shows a concrete example**: Demonstrates what happens when an off-the-shelf model tries to answer a question about your specific database\n",
    "\n",
    "The test process:\n",
    "1. Load one example from your training data (by default, the first row)\n",
    "2. Feed the natural language question to a base model (e.g., Llama 3.1 8B)\n",
    "3. Execute whatever SQL the model generates against your MCP server\n",
    "4. Compare the result to the ground truth\n",
    "5. Print whether it succeeded or failed\n",
    "\n",
    "What to expect:\n",
    "- The base model might get it right! Simple queries often work.\n",
    "- Or, you'll see some kind of failure: wrong column names, missing aliases, incorrect syntax, etc.\n",
    "- Either outcome is fine; this is just a quick test to see the model in action before fine-tuning.\n",
    "\n",
    "To try different examples, change `ROW_INDEX_TO_TEST` to test other rows from your dataset.\n",
    "\n",
    "Ensure that you set MCP_SERVER_URL to be your actual MCP server URL from step 11. e) i.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You can follow along in the same way here. This single-example test is just a quick way to verify everything is wired up correctly before launching the more expensive fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load from file. Reason: [Errno 2] No such file or directory: 'data/final_rft_sql_train_data_v3.jsonl'\n",
      "Using hardcoded fallback EXAMPLE_DATA.\n",
      "\n",
      "====================\n",
      "LLM QUERY GENERATION\n",
      "====================\n",
      "Calling model 'accounts/fireworks/models/llama-v3p1-8b-instruct' to generate SQL query...\n",
      "\n",
      "Model Generated SQL Query:\n",
      "SELECT country, COUNT(*) FROM airlines GROUP BY country ORDER BY COUNT(*) DESC, country ASC\n",
      "\n",
      "====================\n",
      "MCP SERVER EXECUTION\n",
      "====================\n",
      "Sending query to MCP server...\n",
      "\n",
      "Parsed Result from Server:\n",
      "[\n",
      "  {\n",
      "    \"country\": \"Canada\",\n",
      "    \"count_star()\": 10\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Sweden\",\n",
      "    \"count_star()\": 10\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Kenya\",\n",
      "    \"count_star()\": 9\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United States\",\n",
      "    \"count_star()\": 9\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Australia\",\n",
      "    \"count_star()\": 8\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Spain\",\n",
      "    \"count_star()\": 6\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Italy\",\n",
      "    \"count_star()\": 4\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Switzerland\",\n",
      "    \"count_star()\": 4\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Finland\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"France\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Mexico\",\n",
      "    \"count_star()\": 3\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Costa Rica\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Germany\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Iceland\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Ireland\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Japan\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Norway\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Singapore\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United Kingdom\",\n",
      "    \"count_star()\": 2\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Argentina\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Brazil\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"China\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Egypt\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Fiji\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Greece\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"India\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Jordan\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Netherlands\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"New Zealand\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Portugal\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Saudi Arabia\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"South Africa\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"Thailand\",\n",
      "    \"count_star()\": 1\n",
      "  },\n",
      "  {\n",
      "    \"country\": \"United Arab Emirates\",\n",
      "    \"count_star()\": 1\n",
      "  }\n",
      "]\n",
      "\n",
      "====================\n",
      "COMPARISON\n",
      "====================\n",
      "\n",
      "‚úÖ GOOD RESULT: The base model generated SQL that produced the correct data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from fireworks import LLM\n",
    "\n",
    "# --- 1. SETUP: Define API keys, server URLs, and the model to use ---\n",
    "\n",
    "# IMPORTANT: Make sure your FIREWORKS_API_KEY is set as an environment variable.\n",
    "# You can get one from https://fireworks.ai\n",
    "if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "    print(\"FATAL: FIREWORKS_API_KEY environment variable not set.\")\n",
    "    # If not set, you can hardcode it here for testing, but this is not recommended:\n",
    "    # os.environ[\"FIREWORKS_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# The model we'll use to generate the SQL. This acts as our \"base\" model.\n",
    "LLM_MODEL = \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n",
    "llm = LLM(model=LLM_MODEL, deployment_type=\"serverless\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "\n",
    "# The URL for your running MCP server.\n",
    "MCP_SERVER_URL = None  # PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "\n",
    "\n",
    "# --- 2. LOAD THE EXAMPLE DATA ---\n",
    "\n",
    "# This is the example data you provided.\n",
    "DATASET_FILE_PATH = \"data/final_rft_sql_train_data_v3.jsonl\"\n",
    "ROW_INDEX_TO_TEST = 0  # 0 is the first row, 1 is the second row, etc.\n",
    "\n",
    "EXAMPLE_DATA = None\n",
    "try:\n",
    "    with open(DATASET_FILE_PATH, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == ROW_INDEX_TO_TEST:\n",
    "                EXAMPLE_DATA = json.loads(line)\n",
    "                break\n",
    "    \n",
    "    if EXAMPLE_DATA is None:\n",
    "        with open(DATASET_FILE_PATH, 'r') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "        raise IndexError(f\"row index {ROW_INDEX_TO_TEST} is out of bounds for file with {line_count} rows.\")\n",
    "\n",
    "    print(f\"Successfully loaded row {ROW_INDEX_TO_TEST} from '{DATASET_FILE_PATH}'.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load from file. Reason: {e}\")\n",
    "\n",
    "# If loading from file failed for any reason, use the hardcoded fallback data.\n",
    "if EXAMPLE_DATA is None:\n",
    "    print(\"Using hardcoded fallback EXAMPLE_DATA.\\n\")\n",
    "    EXAMPLE_DATA = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\\nYou are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\\n\\n**Database Schema:**\\n| database              | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\\n|:----------------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\\n| synthetic_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\\n|                       |          |           |                                                                            |  'VARCHAR']                                                           |             |\\n| synthetic_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\\n|                       |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\\n| synthetic_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| synthetic_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR' 'BIGINT' 'VARCHAR'   | False       |\\n|                       |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\\n|                       |          |           |  'equipment']                                                              |                                                                       |             |\\n\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which countries have the most airlines, and how many airlines are there in each country, listed in descending order by the number of airlines and then alphabetically by country name?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"SELECT country, COUNT(*) AS airline_count FROM airlines GROUP BY country ORDER BY airline_count DESC, country ASC\"}\n",
    "        ],\n",
    "        \"ground_truth\": [{\"country\": \"Canada\", \"airline_count\": 10}, {\"country\": \"Sweden\", \"airline_count\": 10}, {\"country\": \"Kenya\", \"airline_count\": 9}, {\"country\": \"United States\", \"airline_count\": 9}, {\"country\": \"Australia\", \"airline_count\": 8}, {\"country\": \"Spain\", \"airline_count\": 6}, {\"country\": \"Italy\", \"airline_count\": 4}, {\"country\": \"Switzerland\", \"airline_count\": 4}, {\"country\": \"Finland\", \"airline_count\": 3}, {\"country\": \"France\", \"airline_count\": 3}, {\"country\": \"Mexico\", \"airline_count\": 3}, {\"country\": \"Costa Rica\", \"airline_count\": 2}, {\"country\": \"Germany\", \"airline_count\": 2}, {\"country\": \"Iceland\", \"airline_count\": 2}, {\"country\": \"Ireland\", \"airline_count\": 2}, {\"country\": \"Japan\", \"airline_count\": 2}, {\"country\": \"Norway\", \"airline_count\": 2}, {\"country\": \"Singapore\", \"airline_count\": 2}, {\"country\": \"United Kingdom\", \"airline_count\": 2}, {\"country\": \"Argentina\", \"airline_count\": 1}, {\"country\": \"Brazil\", \"airline_count\": 1}, {\"country\": \"China\", \"airline_count\": 1}, {\"country\": \"Egypt\", \"airline_count\": 1}, {\"country\": \"Fiji\", \"airline_count\": 1}, {\"country\": \"Greece\", \"airline_count\": 1}, {\"country\": \"India\", \"airline_count\": 1}, {\"country\": \"Jordan\", \"airline_count\": 1}, {\"country\": \"Netherlands\", \"airline_count\": 1}, {\"country\": \"New Zealand\", \"airline_count\": 1}, {\"country\": \"Portugal\", \"airline_count\": 1}, {\"country\": \"Saudi Arabia\", \"airline_count\": 1}, {\"country\": \"South Africa\", \"airline_count\": 1}, {\"country\": \"Thailand\", \"airline_count\": 1}, {\"country\": \"United Arab Emirates\", \"airline_count\": 1}]\n",
    "    }\n",
    "\n",
    "# Extract the prompts and ground truth from the data\n",
    "system_prompt = EXAMPLE_DATA[\"messages\"][0][\"content\"]\n",
    "user_prompt = EXAMPLE_DATA[\"messages\"][1][\"content\"]\n",
    "GROUND_TRUTH_ROWS = EXAMPLE_DATA[\"ground_truth\"]\n",
    "\n",
    "# --- 3. HELPER FUNCTION: To parse the server's ASCII table response ---\n",
    "\n",
    "def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "    This version robustly handles 'NULL' values and empty strings.\n",
    "    \"\"\"\n",
    "    lines = table_string.strip().split('\\n')\n",
    "    content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "    if len(content_lines) < 2:\n",
    "        return []\n",
    "    \n",
    "    header_raw = [h.strip() for h in content_lines[0].split('|')[1:-1]]\n",
    "    data_lines = content_lines[1:]\n",
    "    \n",
    "    if len(data_lines) > 0:\n",
    "        try:\n",
    "            first_data_values = [v.strip() for v in data_lines[0].split('|')[1:-1]]\n",
    "            if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "                data_lines = data_lines[1:]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    rows = []\n",
    "    for line in data_lines:\n",
    "        try:\n",
    "            values_raw = [v.strip() for v in line.split('|')[1:-1]]\n",
    "            if len(values_raw) == len(header_raw):\n",
    "                row_dict = {}\n",
    "                for i, header in enumerate(header_raw):\n",
    "                    value_str = values_raw[i]\n",
    "                    if value_str.upper() == 'NULL' or value_str == '':\n",
    "                        row_dict[header] = None\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        if '.' in value_str:\n",
    "                            row_dict[header] = float(value_str)\n",
    "                        else:\n",
    "                            row_dict[header] = int(value_str)\n",
    "                    except (ValueError, TypeError):\n",
    "                        row_dict[header] = value_str\n",
    "                rows.append(row_dict)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# --- 4. GENERATE SQL QUERY USING THE LLM ---\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"LLM QUERY GENERATION\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "model_generated_sql = \"\"\n",
    "try:\n",
    "    print(f\"Calling model '{LLM_MODEL}' to generate SQL query...\")\n",
    "    \n",
    "    messages_for_llm = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages_for_llm,\n",
    "        temperature=0.0  # Set to 0 for deterministic output\n",
    "    )\n",
    "    \n",
    "    model_generated_sql = response.choices[0].message.content.strip()\n",
    "    print(\"\\nModel Generated SQL Query:\")\n",
    "    print(model_generated_sql)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nAN ERROR OCCURRED during LLM call: {e}\")\n",
    "\n",
    "\n",
    "# --- 5. EXECUTE GENERATED QUERY ON MCP SERVER ---\n",
    "\n",
    "predicted_rows = []\n",
    "if model_generated_sql:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*20)\n",
    "        print(\"MCP SERVER EXECUTION\")\n",
    "        print(\"=\"*20)\n",
    "        print(f\"Sending query to MCP server...\")\n",
    "        \n",
    "        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json, text/event-stream\"}\n",
    "        payload = {\n",
    "            \"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\",\n",
    "            \"params\": {\"session\": {\"id\": \"stateless-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": model_generated_sql}}\n",
    "        }\n",
    "\n",
    "        with requests.post(f\"{MCP_SERVER_URL}/mcp/\", headers=headers, json=payload, timeout=20, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            response_data = None\n",
    "            for line in response.iter_lines():\n",
    "                if line and line.decode('utf-8').startswith('data:'):\n",
    "                    json_part = line.decode('utf-8')[len('data:'):].strip()\n",
    "                    if json_part:\n",
    "                        response_data = json.loads(json_part)\n",
    "                        break\n",
    "            \n",
    "            if response_data is None: raise RuntimeError(\"No JSON data in event stream.\")\n",
    "            if \"error\" in response_data: raise RuntimeError(f\"SQL Error: {response_data['error'].get('message', 'Unknown')}\")\n",
    "\n",
    "            ascii_table = response_data['result']['content'][0]['text']\n",
    "            predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "            print(\"\\nParsed Result from Server:\")\n",
    "            print(json.dumps(predicted_rows, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED during MCP call: {e}\")\n",
    "\n",
    "# --- 6. FINAL COMPARISON ---\n",
    "print(\"\\n\" + \"=\"*20)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "if not predicted_rows:\n",
    "    print(\"Skipping comparison: no rows returned from query or an error occurred.\")\n",
    "else:\n",
    "    gt_values = sorted([sorted(map(str, row.values())) for row in GROUND_TRUTH_ROWS])\n",
    "    predicted_values = sorted([sorted(map(str, row.values())) for row in predicted_rows])\n",
    "\n",
    "    if gt_values == predicted_values:\n",
    "        print(\"\\n‚úÖ GOOD RESULT: The base model generated SQL that produced the correct data.\\n\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå BAD RESULT: The base model's SQL produced different data than expected.\\n\")\n",
    "        print(\"This is often the intended outcome when testing a base model, as it highlights what fine-tuning needs to correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. üöÄ Launch the Fine-Tuning Job & Deploy via the UI\n",
    "Now we'll use the Fireworks AI web interface to take our prepared dataset and fine-tune a model. This process uses your custom `evaluate` function to teach a base model how to generate SQL correctly.\n",
    "\n",
    "#### RFT vs Traditional Fine-Tuning:\n",
    "Traditional supervised fine-tuning (SFT) would:\n",
    "- Require thousands of examples\n",
    "- Teach the model to mimic exact SQL syntax\n",
    "- Often overfit to specific query patterns\n",
    "\n",
    "Reinforcement fine-tuning (RFT) instead:\n",
    "- Works with just hundreds of examples\n",
    "- Rewards correct results regardless of SQL syntax\n",
    "- Discovers novel solutions through exploration\n",
    "- Generalizes better to unseen queries\n",
    "\n",
    "> **Real World üåç**  \n",
    "> This is the core of the RFT process. You're teaching a general-purpose model a very specific and valuable new skill using a powerful, UI-driven workflow. You may follow along as described below\n",
    "\n",
    "As described in the [Fireworks RFT documentation](https://fireworks.ai/docs/fine-tuning/reinforcement-fine-tuning-models), the process involves uploading your data, creating an evaluator, running the job, and deploying.\n",
    "\n",
    "<br>\n",
    "\n",
    "**14. a) Upload Your Dataset**\n",
    "\n",
    "1.  Navigate to the **Datasets** tab in your [Fireworks AI dashboard](https://fireworks.ai/account/datasets).\n",
    "2.  Click **\"Upload Dataset\"**.\n",
    "3.  Upload your training file: `data/final_rft_sql_train_data.jsonl`.\n",
    "4.  Give it a memorable name, like `rft-sql-train-data-v1`, and save it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**14. b) Create the Evaluator**\n",
    "\n",
    "1.  Navigate to the **Evaluations** tab in the dashboard.\n",
    "2.  Click **\"Create Evaluator\"**. This will open the web IDE.\n",
    "3.  In the editor on the left, replace the template code with your full `evaluate` function from step 12 above. This function already contains the logic to connect to your MCP server and compare the results. You just need to add your MCP server URL to the MCP_SERVER_URL line.\n",
    "4.  Save the evaluator with a name like `rft-sql-mcp-evaluator`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**14. c) Launch the Fine-Tuning Job**\n",
    "\n",
    "1.  Navigate to the **Fine-Tuning** tab.\n",
    "2.  Click **\"Fine-Tune a Model\"** and select **Reinforcement**.\n",
    "3.  Configure the job:\n",
    "    *   **Model Selection:** Select a model, for example `accounts/fireworks/models/llama-v3p1-8b-instruct`.\n",
    "    *   **Dataset:** Select the `rft-sql-train-data-v1` you uploaded.\n",
    "    *   **Evaluator:** Select the `rft-sql-mcp-evaluator` you just created.\n",
    "    *   **Rollout:** You can leave these as the default values.\n",
    "    *   **Optional Settings:** You can leave the Model Output Name blank and get the default name, or enter a name of your choosing.\n",
    "4.  You can leave the other hyperparameters as their defaults for the first run.\n",
    "5.  Click **\"Create Job\"**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**14. d) Monitor and Deploy**\n",
    "\n",
    "1.  You can monitor the progress of your job in the **Fine-tuning** tab.\n",
    "2.  Once the job status is `Completed`, you can deploy your model. To deploy, click \"Deploy\" on the top right of your fine-tuning job's page. Please note:\n",
    "    -  The Model under \"Select base model*\" should be the one from your Reinforcement Fine-Tuning job (this should be populated automatically)\n",
    "    -  Speculative decoding is an advanced technique that can improve latency, but is not needed for this use-case\n",
    "    -  Feel free to make the other selections (Performance, Scaling, and Metadata) as needed; enabling autoscaling is recommended to reduce costs\n",
    "3.  Find this new model and click the **Deploy** button to create an API endpoint.\n",
    "\n",
    "<br>\n",
    "\n",
    "**14. e) Test Your New Model!**\n",
    "Once deployed, copy your new model's ID and paste it into the `LLM_MODEL` variable in the testing cell (step #12) to make sure it works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. ‚öñÔ∏è Evaluate Model Performance\n",
    "Now for the moment of truth. We will systematically compare the performance of the original base model against our newly fine-tuned model, as well as a much larger base model, to quantify the improvement and general accuracy.\n",
    "\n",
    "We'll run both models against every entry in our test dataset (final_rft_sql_test_data.jsonl). For each entry, we will:\n",
    "1. Provide the same system and user prompt to both the base model and the fine-tuned model.\n",
    "2. Capture the SQL query generated by each.\n",
    "3. Execute each query against our live MCP server.\n",
    "4. Compare the query result to the ground_truth from our dataset.\n",
    "5. Keep a running score for each model.\n",
    "\n",
    "This process will give us a clear, data-driven view of how much more accurate our model became after reinforcement fine-tuning.\n",
    "> **Real World üåç**\n",
    "> This is a critical step in any MLOps loop. Evaluating a model on a consistent test set is the only way to prove that your efforts have resulted in a tangible improvement. In production, you'd also want to:\n",
    "> - Track latency and cost metrics\n",
    "> - Monitor for drift over time\n",
    "> - A/B test against your current solution\n",
    "> - Collect user feedback on query quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM objects for all three models created successfully.\n",
      "Loaded 135 evaluation examples from 'data/final_rft_sql_test_data.jsonl'.\n",
      "\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:  16%|‚ñà‚ñã        | 22/135 [09:45<12:44,  6.77s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Error during evaluation for model accounts/fireworks/models/qwen2p5-7b: HTTPSConnectionPool(host='mcp-sql-rft-server-gz5z3egp4a-uw.a.run.app', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [23:32<00:00, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "  EVALUATION COMPLETE\n",
      "=========================\n",
      "Total Examples: 135\n",
      "\n",
      "--- BASE MODEL ---\n",
      "Model ID: accounts/fireworks/models/qwen2p5-7b\n",
      "Correct: 75/135\n",
      "Accuracy: 55.56%\n",
      "\n",
      "--- LARGE BASE MODEL ---\n",
      "Model ID: accounts/fireworks/models/qwen3-235b-a22b-instruct-2507\n",
      "Correct: 95/135\n",
      "Accuracy: 70.37%\n",
      "\n",
      "--- FINE-TUNED MODEL ---\n",
      "Model ID: accounts/pyroworks/models/qwen2p5-mcp-sql-rft-tune-bigger\n",
      "Correct: 95/135\n",
      "Accuracy: 70.37%\n",
      "\n",
      "=========================\n",
      "  PERFORMANCE LIFT\n",
      "=========================\n",
      "Fine-Tuned vs. Base: +14.81%\n",
      "Fine-Tuned vs. Large Base: +0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from fireworks import LLM\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. SETUP: Define the models to compare, server URL, and dataset path ---\n",
    "\n",
    "# IMPORTANT: Make sure your FIREWORKS_API_KEY is set as an environment variable.\n",
    "if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "    print(\"FATAL: FIREWORKS_API_KEY environment variable not set.\")\n",
    "\n",
    "# The base model you used for the fine-tuning job.\n",
    "BASE_MODEL_ID = \"accounts/fireworks/models/qwen2p5-7b\"  # <--- Replace if you used a different base model\n",
    "LARGE_BASE_MODEL_ID = \"accounts/fireworks/models/qwen3-235b-a22b-instruct-2507\"\n",
    "\n",
    "# IMPORTANT: Replace this with the model ID of your new fine-tuned model.\n",
    "#FINE_TUNED_MODEL_ID = \"accounts/<your-account-id>/models/<your-base-model-id>\"  # <--- Replace with your fine-tuned model ID\n",
    "FINE_TUNED_MODEL_ID = \"accounts/<your-account-id>/models/<your-base-model-id>\"\n",
    "\n",
    "MCP_SERVER_URL = None  # <--- PUT MCP SERVER URL HERE without the /mcp/ suffix at the end\n",
    "DATASET_FILE_PATH = \"data/final_rft_sql_test_data.jsonl\"\n",
    "\n",
    "# --- 2. Create LLM Objects ---\n",
    "base_model_llm = None\n",
    "large_base_model_llm = None\n",
    "fine_tuned_model_llm = None\n",
    "try:\n",
    "    base_model_llm = LLM(model=BASE_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    large_base_model_llm = LLM(model=LARGE_BASE_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    fine_tuned_model_llm = LLM(model=FINE_TUNED_MODEL_ID, deployment_type=\"auto\", api_key=os.getenv(\"FIREWORKS_API_KEY\"))\n",
    "    print(\"LLM objects for all three models created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not create LLM objects. Error: {e}\")\n",
    "\n",
    "# --- 3. Load Dataset ---\n",
    "dataset = []\n",
    "if all([base_model_llm, large_base_model_llm, fine_tuned_model_llm]):\n",
    "    try:\n",
    "        with open(DATASET_FILE_PATH, 'r') as f:\n",
    "            dataset = [json.loads(line) for line in f]\n",
    "        print(f\"Loaded {len(dataset)} evaluation examples from '{DATASET_FILE_PATH}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not load dataset. Error: {e}\")\n",
    "        dataset = []\n",
    "\n",
    "# --- 4. HELPER AND EVALUATION FUNCTIONS ---\n",
    "\n",
    "def parse_duckdb_ascii_table(table_string: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parses a DuckDB-style ASCII table string into a list of dictionaries.\n",
    "    This version robustly handles 'NULL' values and empty strings.\n",
    "    \"\"\"\n",
    "    lines = table_string.strip().split('\\n')\n",
    "    content_lines = [line for line in lines if line.strip() and not line.startswith('+')]\n",
    "    if len(content_lines) < 2:\n",
    "        return []\n",
    "    \n",
    "    header_raw = [h.strip() for h in content_lines[0].split('|')[1:-1]]\n",
    "    data_lines = content_lines[1:]\n",
    "    \n",
    "    if len(data_lines) > 0:\n",
    "        try:\n",
    "            first_data_values = [v.strip() for v in data_lines[0].split('|')[1:-1]]\n",
    "            if len(first_data_values) == len(header_raw) and all(v.isupper() for v in first_data_values):\n",
    "                data_lines = data_lines[1:]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    rows = []\n",
    "    for line in data_lines:\n",
    "        try:\n",
    "            values_raw = [v.strip() for v in line.split('|')[1:-1]]\n",
    "            if len(values_raw) == len(header_raw):\n",
    "                row_dict = {}\n",
    "                for i, header in enumerate(header_raw):\n",
    "                    value_str = values_raw[i]\n",
    "                    if value_str.upper() == 'NULL' or value_str == '':\n",
    "                        row_dict[header] = None\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        if '.' in value_str:\n",
    "                            row_dict[header] = float(value_str)\n",
    "                        else:\n",
    "                            row_dict[header] = int(value_str)\n",
    "                    except (ValueError, TypeError):\n",
    "                        row_dict[header] = value_str\n",
    "                rows.append(row_dict)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "def are_results_equal(predicted_rows: list[dict], ground_truth_rows: list[dict]) -> bool:\n",
    "    \"\"\"\n",
    "    Compares datasets by converting all values to strings and sorting them,\n",
    "    which ignores row order, column order, and data types (e.g., int vs float).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gt_values = sorted([sorted(map(str, row.values())) for row in ground_truth_rows])\n",
    "        predicted_values = sorted([sorted(map(str, row.values())) for row in predicted_rows])\n",
    "        return gt_values == predicted_values\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_sql_and_evaluate(llm_obj, system_prompt: str, user_prompt: str, ground_truth: list[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Calls a pre-configured LLM object to get a SQL query, executes it, and compares to ground truth.\n",
    "    Returns 1 for a correct result, 0 for an incorrect one.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get SQL from the model\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        response = llm_obj.chat.completions.create(messages=messages, temperature=0.0)\n",
    "        sql_query = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Step 2: Execute SQL on MCP server\n",
    "        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json, text/event-stream\"}\n",
    "        payload = {\"id\": \"eval-query-1\", \"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"session\": {\"id\": \"full-eval-session\"}, \"name\": \"query\", \"arguments\": {\"query\": sql_query}}}\n",
    "\n",
    "        response_data = None\n",
    "        with requests.post(f\"{MCP_SERVER_URL}/mcp/\", headers=headers, json=payload, timeout=30, stream=True) as mcp_response:\n",
    "            mcp_response.raise_for_status()\n",
    "            for line in mcp_response.iter_lines():\n",
    "                if line and line.decode('utf-8').startswith('data:'):\n",
    "                    json_part = line.decode('utf-8')[len('data:'):].strip()\n",
    "                    if json_part:\n",
    "                        response_data = json.loads(json_part)\n",
    "                        break\n",
    "\n",
    "        if response_data is None or \"error\" in response_data:\n",
    "            return 0\n",
    "\n",
    "        # Step 3: Parse and compare results\n",
    "        ascii_table = response_data['result']['content'][0]['text']\n",
    "        predicted_rows = parse_duckdb_ascii_table(ascii_table)\n",
    "\n",
    "        return 1 if are_results_equal(predicted_rows, ground_truth) else 0\n",
    "    except Exception as e:\n",
    "        print(f\"--> Error during evaluation for model {llm_obj.model}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 5. RUN THE FULL EVALUATION ---\n",
    "\n",
    "base_model_score = 0\n",
    "large_base_model_score = 0\n",
    "fine_tuned_model_score = 0\n",
    "\n",
    "if dataset:\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    for item in tqdm(dataset, desc=\"Evaluating models\"):\n",
    "        system_prompt = item[\"messages\"][0][\"content\"]\n",
    "        user_prompt = item[\"messages\"][1][\"content\"]\n",
    "        ground_truth = item[\"ground_truth\"]\n",
    "\n",
    "        # Evaluate base model\n",
    "        base_model_score += get_sql_and_evaluate(base_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)  # Be nice to the API\n",
    "\n",
    "        # Evaluate large base model\n",
    "        large_base_model_score += get_sql_and_evaluate(large_base_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Evaluate fine-tuned model\n",
    "        fine_tuned_model_score += get_sql_and_evaluate(fine_tuned_model_llm, system_prompt, user_prompt, ground_truth)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 6. REPORT RESULTS ---\n",
    "\n",
    "if dataset:\n",
    "    total = len(dataset)\n",
    "    base_accuracy = (base_model_score / total) * 100\n",
    "    large_base_accuracy = (large_base_model_score / total) * 100\n",
    "    tuned_accuracy = (fine_tuned_model_score / total) * 100\n",
    "\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"  EVALUATION COMPLETE\")\n",
    "    print(\"=\"*25)\n",
    "    print(f\"Total Examples: {total}\\n\")\n",
    "    print(\"--- BASE MODEL ---\")\n",
    "    print(f\"Model ID: {BASE_MODEL_ID}\")\n",
    "    print(f\"Correct: {base_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {base_accuracy:.2f}%\\n\")\n",
    "\n",
    "    print(\"--- LARGE BASE MODEL ---\")\n",
    "    print(f\"Model ID: {LARGE_BASE_MODEL_ID}\")\n",
    "    print(f\"Correct: {large_base_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {large_base_accuracy:.2f}%\\n\")\n",
    "\n",
    "    print(\"--- FINE-TUNED MODEL ---\")\n",
    "    print(f\"Model ID: {FINE_TUNED_MODEL_ID}\")\n",
    "    print(f\"Correct: {fine_tuned_model_score}/{total}\")\n",
    "    print(f\"Accuracy: {tuned_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    print(\"=\"*25)\n",
    "    print(\"  PERFORMANCE LIFT\")\n",
    "    print(\"=\"*25)\n",
    "    print(f\"Fine-Tuned vs. Base: {tuned_accuracy - base_accuracy:+.2f}%\")\n",
    "    print(f\"Fine-Tuned vs. Large Base: {tuned_accuracy - large_base_accuracy:+.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Evaluation skipped because the dataset or LLM objects could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. ‚ú® Cleanup & Conclusion\n",
    "Congratulations! You've successfully completed the entire Reinforcement Fine-Tuning loop. You started with just a database schema and ended with a highly specialized, performant, and data-aware AI model.\n",
    "\n",
    "#### Cleanup\n",
    "Cloud resources and model deployments can incur costs, so it's good practice to clean up any resources you no longer need.\n",
    "\n",
    "*   **Check your Deployments:** Navigate to the [Deployments tab](https://app.fireworks.ai/dashboard/deployments) in your Fireworks AI dashboard. Here you can monitor and manage all your deployed models.\n",
    "*   **Delete Unneeded Models:** Feel free to delete any deployments you no longer need. For example, you might have deployed the base or large-base models during the evaluation step to compare against your fine-tuned model. These can now be safely removed to save costs.\n",
    "*   **Delete Cloud Run service and container image:** Feel free to delete your MCP server Cloud Run service and container image to avoid stray storage costs.\n",
    "\n",
    "You can, of course, continue using your new fine-tuned SQL generation model for any application you see fit!\n",
    "\n",
    "#### Conclusions\n",
    "The evaluation results from the previous step highlight the power of this approach.\n",
    "\n",
    "*   **Performance on par with massive models:** Our fine-tuned 7B parameter model performs on par with a much larger model like `qwen3-coder-480b-a35b-instruct` on this specific dataset. This is because it has been fine-tuned to understand the data schema via real query generation and execution.\n",
    "*   **Efficiency Gains:** This specialized 7B model is significantly faster and cheaper to run than its 480B counterpart, offering production-grade performance at a fraction of the cost and latency.\n",
    "*   **High-Level Capability on Complex Tasks:** The queries in this dataset are relatively complex, which is reflected in the final accuracy score of around 60%. This is a strong result, demonstrating that for a specialized domain, a smaller model can be tuned to achieve a level of performance comparable to elite, state-of-the-art coding models like Claude 4 Sonnet.\n",
    "\n",
    "---\n",
    "\n",
    "Throughout this tutorial, we demonstrated a complete, end-to-end workflow for creating a fine-tuned text-to-SQL model. We began with the absolute minimum requirement, a database schema, and used a series of LLM-driven steps to generate a safe, synthetic data sandbox. From there, we generated a rich dataset of queries and answers, which we used to fine-tune a model using the Fireworks RFT platform. The final result is a small, efficient model that can accurately query data it has never seen, a task that was previously only possible with vastly larger and more expensive models.\n",
    "\n",
    "This pattern of **schema ‚Üí synthetic data ‚Üí RFT** is a secure, effective, and repeatable methodology for teaching language models to become expert users of your private data and custom APIs, without ever exposing the underlying sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
