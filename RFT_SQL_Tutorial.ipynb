{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úàÔ∏è Natural-Language ‚Üí SQL with Reinforcement-Fine-Tuning (RFT)\n",
    "A full, reproducible demo that trains a 3-B parameter model to answer English questions by writing SQL, without touching real production data.\n",
    "\n",
    "### 0. Preface ‚Äì Why this matters\n",
    "Large-language-model copilots are great, but even the best models don't know your private data, and can hallucinate a column that doesn‚Äôt exist.\n",
    "Reinforcement-Fine-Tuning (RFT) fixes this by teaching your language model about your data and how to write accurate queries. In this tutorial you will:\n",
    "\n",
    "| You will learn to üìö                               | By the end you‚Äôll have üéÅ                          |\n",
    "| -------------------------------------------------- | -------------------------------------------------- |\n",
    "| ‚úì create a synthetic DB that *mimics* your schema  | `openflights.db` (<20 MB) wrapped by an MCP server |\n",
    "| ‚úì generate a MECE query set & ground-truth answers | `queries.json`, `gt_rows.json`                     |\n",
    "| ‚úì build NL ‚Üî SQL result training pairs             | `train_pairs.jsonl`                                |\n",
    "| ‚úì run an RFT job on Fireworks AI                   | a tuned **Qwen-2.5-Coder-3B-RFT** model            |\n",
    "| ‚úì benchmark baseline vs. RFT accuracy              | >2√ó exact-match gain                               |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **A Note on Our Method: Demo vs. Real World üåç** <br>\n",
    "> Throughout this tutorial, we will be clear about what we're doing for the purpose of this self-contained demo versus what you would do in a real-world scenario.\n",
    "> - **Real World üåç**: Look for these notes to see the parallel step you would take in your own environment if you wanted to apply this workflow, typically by swapping in your own private business assets like schemas or query logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Development Environment Setup\n",
    "**Complete these steps once in your terminal, *outside* this notebook.**\n",
    "\n",
    "1.  **Get a Fireworks AI API Key**\n",
    "    - Go to [fireworks.ai](https://fireworks.ai) and sign up.\n",
    "    - Create an API key from your settings page.\n",
    "    - Create a file named `.env` in your project directory and add your key:\n",
    "      ```\n",
    "      FIREWORKS_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "      ```\n",
    "\n",
    "2.  **Install `uv`**\n",
    "    - `uv` is a fast Python package manager from Astral. Follow the official installation instructions at [docs.astral.sh/uv/](https://docs.astral.sh/uv/).\n",
    "\n",
    "3.  **Create a Virtual Environment and Install Packages**\n",
    "    - Once `uv` is installed, create and activate a virtual environment.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv venv .venv\n",
    "    source .venv/bin/activate  # On Windows PowerShell: .venv\\Scripts\\Activate.ps1\n",
    "    ```\n",
    "    - Install all required packages using `uv add`.\n",
    "    ```bash\n",
    "    # Run this in your terminal\n",
    "    uv add duckdb tabulate pandas pyarrow requests \\\n",
    "           faker python-dotenv \\\n",
    "           jsonlines fireworks-ai \\\n",
    "           mcp-sdk mcp-server-motherduck\n",
    "    ```\n",
    "After running these commands, your environment is ready. You can proceed with the cells inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulate the \"Production\" Database\n",
    "First, we'll create a database that represents your real, populated production database. We'll download the public OpenFlights dataset and load it into a DuckDB file.\n",
    "\n",
    "> **Real World üåç**: You already have this! It's your live production database (or a replica). You would skip this entire step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 'Production' database simulated at: data/prod_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# --- Download the raw data files ---\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "BASE_URL = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/\"\n",
    "FILES_TO_DOWNLOAD = {\n",
    "    \"airports\": \"airports.dat\",\n",
    "    \"airlines\": \"airlines.dat\",\n",
    "    \"routes\": \"routes.dat\",\n",
    "    \"countries\": \"countries.dat\",\n",
    "    \"planes\": \"planes.dat\"\n",
    "}\n",
    "# Define column names as the files don't have headers\n",
    "COLUMN_NAMES = {\n",
    "    \"airports\": [\"airport_id\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"],\n",
    "    \"airlines\": [\"airline_id\", \"name\", \"alias\", \"iata\", \"icao\", \"callsign\", \"country\", \"active\"],\n",
    "    \"routes\": [\"airline\", \"airline_id\", \"source_airport\", \"source_airport_id\", \"destination_airport\", \"destination_airport_id\", \"codeshare\", \"stops\", \"equipment\"],\n",
    "    \"countries\": [\"name\", \"iso_code\", \"dafif_code\"],\n",
    "    \"planes\": [\"name\", \"iata\", \"icao\"]\n",
    "}\n",
    "\n",
    "PROD_DB_PATH = \"data/prod_openflights.db\"\n",
    "\n",
    "# --- Load the real data into our \"production\" DuckDB ---\n",
    "with duckdb.connect(PROD_DB_PATH) as con:\n",
    "    for name, filename in FILES_TO_DOWNLOAD.items():\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "        path = DATA_DIR / filename\n",
    "        if not path.exists():\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            print(f\"‚úÖ Downloaded: {path}\")\n",
    "\n",
    "        # Load data using pandas to handle missing headers and null values\n",
    "        df = pd.read_csv(path, header=None, names=COLUMN_NAMES[name], na_values=[\"\\\\N\"])\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE {name} AS SELECT * FROM df\")\n",
    "\n",
    "    print(f\"\\n‚úÖ 'Production' database simulated at: {PROD_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Acquire the Schema (No Data!)\n",
    "This is a critical step. We connect to our \"production\" database and extract **only its schema** (the table structure, column names, and data types). We do not touch or read any of the data rows. This schema is the only artifact we need from the production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema successfully extracted from 'production' database:\n",
      "| database         | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\n",
      "|:-----------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\n",
      "| prod_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\n",
      "|                  |          |           |                                                                            |  'VARCHAR']                                                           |             |\n",
      "| prod_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\n",
      "|                  |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\n",
      "| prod_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\n",
      "| prod_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR'   | False       |\n",
      "|                  |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\n",
      "|                  |          |           |  'equipment']                                                              |                                                                       |             |\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the \"production\" database we just created\n",
    "with duckdb.connect(PROD_DB_PATH, read_only=True) as con:\n",
    "    # The DESCRIBE command gives us the schema information for all tables\n",
    "    schema_df = con.sql(\"DESCRIBE;\").df()\n",
    "\n",
    "print(\"‚úÖ Schema successfully extracted from 'production' database:\")\n",
    "print(schema_df.to_markdown(index=False))\n",
    "\n",
    "# We can also store this for later use in prompts\n",
    "schema_for_prompt = schema_df.to_markdown(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Synthetic Training Sandbox with an LLM\n",
    "Now that we have the schema, we will use a large language model to generate a complete, contextually-aware synthetic dataset.\n",
    "\n",
    "To ensure the LLM's output is structured and parseable, we will **dynamically generate a Pydantic schema** based on the `DESCRIBE` output from the previous step. This is a powerful, generic technique that adapts to any database schema.\n",
    "\n",
    "To fine-tune our model with RFT, **we will only interact with this synthetic database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically created Pydantic models for all tables.\n",
      "\n",
      "‚úÖ Data Generation Plan:\n",
      " - Target rows per table: 50\n",
      " - Will make API calls asking for 2 rows/call until target is met.\n",
      "\n",
      "üìû --- Generating data chunk #1 ---\n",
      "‚úÖ Received and parsed chunk #1.\n",
      "   - 'airlines': 2 / 50 rows\n",
      "   - 'airports': 2 / 50 rows\n",
      "   - 'countries': 2 / 50 rows\n",
      "   - 'planes': 2 / 50 rows\n",
      "   - 'routes': 2 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #2 ---\n",
      "‚úÖ Received and parsed chunk #2.\n",
      "   - 'airlines': 4 / 50 rows\n",
      "   - 'airports': 4 / 50 rows\n",
      "   - 'countries': 4 / 50 rows\n",
      "   - 'planes': 4 / 50 rows\n",
      "   - 'routes': 4 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #3 ---\n",
      "‚úÖ Received and parsed chunk #3.\n",
      "   - 'airlines': 6 / 50 rows\n",
      "   - 'airports': 6 / 50 rows\n",
      "   - 'countries': 6 / 50 rows\n",
      "   - 'planes': 6 / 50 rows\n",
      "   - 'routes': 6 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #4 ---\n",
      "‚úÖ Received and parsed chunk #4.\n",
      "   - 'airlines': 8 / 50 rows\n",
      "   - 'airports': 8 / 50 rows\n",
      "   - 'countries': 8 / 50 rows\n",
      "   - 'planes': 8 / 50 rows\n",
      "   - 'routes': 8 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #5 ---\n",
      "‚úÖ Received and parsed chunk #5.\n",
      "   - 'airlines': 10 / 50 rows\n",
      "   - 'airports': 10 / 50 rows\n",
      "   - 'countries': 10 / 50 rows\n",
      "   - 'planes': 10 / 50 rows\n",
      "   - 'routes': 10 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #6 ---\n",
      "‚úÖ Received and parsed chunk #6.\n",
      "   - 'airlines': 12 / 50 rows\n",
      "   - 'airports': 12 / 50 rows\n",
      "   - 'countries': 12 / 50 rows\n",
      "   - 'planes': 12 / 50 rows\n",
      "   - 'routes': 12 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #7 ---\n",
      "‚úÖ Received and parsed chunk #7.\n",
      "   - 'airlines': 14 / 50 rows\n",
      "   - 'airports': 14 / 50 rows\n",
      "   - 'countries': 14 / 50 rows\n",
      "   - 'planes': 14 / 50 rows\n",
      "   - 'routes': 14 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #8 ---\n",
      "‚úÖ Received and parsed chunk #8.\n",
      "   - 'airlines': 16 / 50 rows\n",
      "   - 'airports': 16 / 50 rows\n",
      "   - 'countries': 16 / 50 rows\n",
      "   - 'planes': 16 / 50 rows\n",
      "   - 'routes': 16 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #9 ---\n",
      "‚úÖ Received and parsed chunk #9.\n",
      "   - 'airlines': 18 / 50 rows\n",
      "   - 'airports': 18 / 50 rows\n",
      "   - 'countries': 18 / 50 rows\n",
      "   - 'planes': 18 / 50 rows\n",
      "   - 'routes': 18 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #10 ---\n",
      "‚úÖ Received and parsed chunk #10.\n",
      "   - 'airlines': 20 / 50 rows\n",
      "   - 'airports': 20 / 50 rows\n",
      "   - 'countries': 20 / 50 rows\n",
      "   - 'planes': 20 / 50 rows\n",
      "   - 'routes': 20 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #11 ---\n",
      "‚úÖ Received and parsed chunk #11.\n",
      "   - 'airlines': 22 / 50 rows\n",
      "   - 'airports': 22 / 50 rows\n",
      "   - 'countries': 22 / 50 rows\n",
      "   - 'planes': 22 / 50 rows\n",
      "   - 'routes': 22 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #12 ---\n",
      "‚úÖ Received and parsed chunk #12.\n",
      "   - 'airlines': 24 / 50 rows\n",
      "   - 'airports': 24 / 50 rows\n",
      "   - 'countries': 24 / 50 rows\n",
      "   - 'planes': 24 / 50 rows\n",
      "   - 'routes': 24 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #13 ---\n",
      "‚úÖ Received and parsed chunk #13.\n",
      "   - 'airlines': 26 / 50 rows\n",
      "   - 'airports': 26 / 50 rows\n",
      "   - 'countries': 26 / 50 rows\n",
      "   - 'planes': 26 / 50 rows\n",
      "   - 'routes': 26 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #14 ---\n",
      "‚úÖ Received and parsed chunk #14.\n",
      "   - 'airlines': 28 / 50 rows\n",
      "   - 'airports': 28 / 50 rows\n",
      "   - 'countries': 28 / 50 rows\n",
      "   - 'planes': 28 / 50 rows\n",
      "   - 'routes': 28 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #15 ---\n",
      "‚úÖ Received and parsed chunk #15.\n",
      "   - 'airlines': 30 / 50 rows\n",
      "   - 'airports': 30 / 50 rows\n",
      "   - 'countries': 30 / 50 rows\n",
      "   - 'planes': 30 / 50 rows\n",
      "   - 'routes': 30 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #16 ---\n",
      "‚úÖ Received and parsed chunk #16.\n",
      "   - 'airlines': 32 / 50 rows\n",
      "   - 'airports': 32 / 50 rows\n",
      "   - 'countries': 32 / 50 rows\n",
      "   - 'planes': 32 / 50 rows\n",
      "   - 'routes': 32 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #17 ---\n",
      "‚úÖ Received and parsed chunk #17.\n",
      "   - 'airlines': 34 / 50 rows\n",
      "   - 'airports': 34 / 50 rows\n",
      "   - 'countries': 34 / 50 rows\n",
      "   - 'planes': 34 / 50 rows\n",
      "   - 'routes': 34 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #18 ---\n",
      "‚úÖ Received and parsed chunk #18.\n",
      "   - 'airlines': 36 / 50 rows\n",
      "   - 'airports': 36 / 50 rows\n",
      "   - 'countries': 36 / 50 rows\n",
      "   - 'planes': 36 / 50 rows\n",
      "   - 'routes': 36 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #19 ---\n",
      "‚úÖ Received and parsed chunk #19.\n",
      "   - 'airlines': 38 / 50 rows\n",
      "   - 'airports': 38 / 50 rows\n",
      "   - 'countries': 38 / 50 rows\n",
      "   - 'planes': 38 / 50 rows\n",
      "   - 'routes': 38 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #20 ---\n",
      "‚úÖ Received and parsed chunk #20.\n",
      "   - 'airlines': 40 / 50 rows\n",
      "   - 'airports': 40 / 50 rows\n",
      "   - 'countries': 40 / 50 rows\n",
      "   - 'planes': 40 / 50 rows\n",
      "   - 'routes': 40 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #21 ---\n",
      "‚úÖ Received and parsed chunk #21.\n",
      "   - 'airlines': 42 / 50 rows\n",
      "   - 'airports': 42 / 50 rows\n",
      "   - 'countries': 42 / 50 rows\n",
      "   - 'planes': 42 / 50 rows\n",
      "   - 'routes': 42 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #22 ---\n",
      "‚úÖ Received and parsed chunk #22.\n",
      "   - 'airlines': 44 / 50 rows\n",
      "   - 'airports': 44 / 50 rows\n",
      "   - 'countries': 44 / 50 rows\n",
      "   - 'planes': 44 / 50 rows\n",
      "   - 'routes': 44 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #23 ---\n",
      "‚úÖ Received and parsed chunk #23.\n",
      "   - 'airlines': 46 / 50 rows\n",
      "   - 'airports': 46 / 50 rows\n",
      "   - 'countries': 46 / 50 rows\n",
      "   - 'planes': 46 / 50 rows\n",
      "   - 'routes': 46 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #24 ---\n",
      "‚úÖ Received and parsed chunk #24.\n",
      "   - 'airlines': 48 / 50 rows\n",
      "   - 'airports': 48 / 50 rows\n",
      "   - 'countries': 48 / 50 rows\n",
      "   - 'planes': 48 / 50 rows\n",
      "   - 'routes': 48 / 50 rows\n",
      "\n",
      "üìû --- Generating data chunk #25 ---\n",
      "‚úÖ Received and parsed chunk #25.\n",
      "   - 'airlines': 50 / 50 rows\n",
      "   - 'airports': 50 / 50 rows\n",
      "   - 'countries': 50 / 50 rows\n",
      "   - 'planes': 50 / 50 rows\n",
      "   - 'routes': 50 / 50 rows\n",
      "\n",
      "‚ú® Data generation complete. Aggregating, deduplicating, and saving to database...\n",
      "\n",
      "--- Deduplicating generated data ---\n",
      " - Table 'airlines': Removed 0 duplicates (50 -> 50).\n",
      " - Table 'airports': Removed 0 duplicates (50 -> 50).\n",
      " - Table 'countries': Removed 15 duplicates (50 -> 35).\n",
      " - Table 'planes': Removed 13 duplicates (50 -> 37).\n",
      " - Table 'routes': Removed 0 duplicates (50 -> 50).\n",
      "\n",
      "‚úÖ Synthetic training sandbox created at: data/synthetic_openflights.db\n",
      "Tables created: [('airlines',), ('airports',), ('countries',), ('planes',), ('routes',)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pydantic import create_model, BaseModel\n",
    "from fireworks import LLM\n",
    "import duckdb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Any, Dict, Type\n",
    "import datetime\n",
    "import decimal\n",
    "import uuid\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- 1. Dynamically Create Pydantic Models from the SQL Schema ---\n",
    "def map_sql_type_to_python(sql_type: str) -> Type:\n",
    "    \"\"\"Maps SQL data types to Python types for Pydantic models.\"\"\"\n",
    "    sql_type_upper = str(sql_type).upper()\n",
    "    if 'DECIMAL' in sql_type_upper: return decimal.Decimal\n",
    "    if 'DOUBLE' in sql_type_upper or 'FLOAT' in sql_type_upper or 'REAL' in sql_type_upper: return float\n",
    "    if 'BIGINT' in sql_type_upper or 'INT' in sql_type_upper: return int\n",
    "    if 'VARCHAR' in sql_type_upper or 'TEXT' in sql_type_upper or 'STRING' in sql_type_upper: return str\n",
    "    if 'TIMESTAMP' in sql_type_upper: return datetime.datetime\n",
    "    if 'DATE' in sql_type_upper: return datetime.date\n",
    "    if 'TIME' in sql_type_upper: return datetime.time\n",
    "    if 'BOOLEAN' in sql_type_upper: return bool\n",
    "    if 'BLOB' in sql_type_upper or 'BYTEA' in sql_type_upper: return bytes\n",
    "    if 'UUID' in sql_type_upper: return uuid.UUID\n",
    "    return object\n",
    "\n",
    "pydantic_models: Dict[str, Type[BaseModel]] = {}\n",
    "table_names = schema_df['name'].unique()\n",
    "\n",
    "for table_name in table_names:\n",
    "    table_schema = schema_df[schema_df['name'] == table_name].iloc[0]\n",
    "    fields: Dict[str, Any] = {}\n",
    "    col_names = table_schema['column_names']\n",
    "    col_types = table_schema['column_types']\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        python_type = map_sql_type_to_python(col_types[i])\n",
    "        fields[col_name] = (Optional[python_type], None)\n",
    "    model_name = table_name.capitalize() + \"Model\"\n",
    "    pydantic_models[table_name] = create_model(model_name, **fields)\n",
    "\n",
    "dataset_fields: Dict[str, Any] = {\n",
    "    table_name: (List[model], ...) for table_name, model in pydantic_models.items()\n",
    "}\n",
    "SyntheticDataset = create_model('SyntheticDataset', **dataset_fields)\n",
    "print(\"‚úÖ Dynamically created Pydantic models for all tables.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Total Row Counts and Chunking Strategy ---\n",
    "TOTAL_ROW_COUNTS = {name: 50 for name in table_names}\n",
    "ROWS_PER_API_CALL = 2 # Ask for data in small, safe chunks\n",
    "print(\"\\n‚úÖ Data Generation Plan:\")\n",
    "print(f\" - Target rows per table: {list(TOTAL_ROW_COUNTS.values())[0]}\")\n",
    "print(f\" - Will make API calls asking for {ROWS_PER_API_CALL} rows/call until target is met.\")\n",
    "\n",
    "\n",
    "# --- 3. Setup LLM and Loop to Generate Data in Chunks ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "load_dotenv()\n",
    "llm = LLM(model=\"accounts/fireworks/models/deepseek-v3\", deployment_type=\"serverless\", api_key=os.getenv(\"FW_API_KEY\"))\n",
    "\n",
    "all_synthetic_data: Dict[str, List[Dict]] = {name: [] for name in table_names}\n",
    "chunk_row_counts = {name: ROWS_PER_API_CALL for name in table_names}\n",
    "\n",
    "base_generation_prompt = f\"\"\"\n",
    "You are a highly intelligent AI data generator. Your task is to create a realistic, synthetic dataset based on the provided database schema.\n",
    "The data you generate must be internally consistent. For example, an `airline_id` in a `routes` table must correspond to an existing `airline_id` in an `airlines` table within this same generated chunk.\n",
    "This applies to any schema you might be working with, not just airlines.\n",
    "You must generate a single JSON object that strictly adheres to the provided JSON schema.\n",
    "\n",
    "The database schema is as follows:\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "call_count = 0\n",
    "# Loop until all tables have at least the desired number of rows\n",
    "while not all(len(rows) >= TOTAL_ROW_COUNTS[name] for name, rows in all_synthetic_data.items()):\n",
    "    call_count += 1\n",
    "    print(f\"\\nüìû --- Generating data chunk #{call_count} ---\")\n",
    "    \n",
    "    # --- Create a summary of existing data to guide the LLM ---\n",
    "    existing_data_summary = \"\"\n",
    "    if any(len(rows) > 0 for rows in all_synthetic_data.values()):\n",
    "        summary_parts = [\"\\nYou have already generated the following data. Do NOT generate rows that are substantially similar to these examples. Create new, distinct data.\\n\"]\n",
    "        for table_name, rows in all_synthetic_data.items():\n",
    "            if rows:\n",
    "                summary_parts.append(f\"\\n--- Existing data in '{table_name}' table ---\")\n",
    "                df = pd.DataFrame(rows)\n",
    "                if len(df.columns) > 10:\n",
    "                    df = df.iloc[:, :10]\n",
    "                markdown_summary = df.to_markdown(index=False, tablefmt=\"grid\")\n",
    "                if markdown_summary:\n",
    "                    summary_parts.append(markdown_summary)\n",
    "        existing_data_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "    # --- Construct the final prompt for this iteration ---\n",
    "    final_prompt = (\n",
    "        base_generation_prompt +\n",
    "        existing_data_summary +\n",
    "        f\"\\n\\nNow, generate a NEW JSON object with a key for each table. The number of new rows for each table should be:\\n\" +\n",
    "        json.dumps(chunk_row_counts, indent=2)\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SyntheticDataset\", \"schema\": SyntheticDataset.model_json_schema()}},\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    choice = response.choices[0]\n",
    "    response_content = choice.message.content\n",
    "\n",
    "    if choice.finish_reason == \"length\":\n",
    "        print(f\"‚ö†Ô∏è WARNING: Chunk #{call_count} was truncated. Skipping.\")\n",
    "        continue\n",
    "    if not response_content:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Received empty content for chunk #{call_count}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chunk_data = json.loads(response_content)\n",
    "        print(f\"‚úÖ Received and parsed chunk #{call_count}.\")\n",
    "        for table_name, rows in chunk_data.items():\n",
    "            if table_name in all_synthetic_data and rows:\n",
    "                all_synthetic_data[table_name].extend(rows)\n",
    "        # Log progress\n",
    "        for name, rows in all_synthetic_data.items():\n",
    "             print(f\"   - '{name}': {len(rows)} / {TOTAL_ROW_COUNTS[name]} rows\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå ERROR: Failed to parse JSON for chunk #{call_count}. Reason: {e}. Skipping.\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# --- 4. Deduplicate and Write to DB ---\n",
    "print(\"\\n‚ú® Data generation complete. Aggregating, deduplicating, and saving to database...\")\n",
    "\n",
    "synthetic_data = all_synthetic_data\n",
    "print(\"\\n--- Deduplicating generated data ---\")\n",
    "for table_name, rows in synthetic_data.items():\n",
    "    if not rows: continue\n",
    "    initial_count = len(rows)\n",
    "    df = pd.DataFrame(rows).drop_duplicates()\n",
    "    final_count = len(df)\n",
    "    synthetic_data[table_name] = df.to_dict('records')\n",
    "    print(f\" - Table '{table_name}': Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Final trim to ensure exact counts\n",
    "for table_name, total_rows_needed in TOTAL_ROW_COUNTS.items():\n",
    "    if table_name in synthetic_data:\n",
    "        synthetic_data[table_name] = synthetic_data[table_name][:total_rows_needed]\n",
    "\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH) as con:\n",
    "    for table_name, rows in synthetic_data.items():\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            schema_cols = schema_df[schema_df['name'] == table_name].iloc[0]['column_names']\n",
    "            for col in schema_cols:\n",
    "                if col not in df.columns: df[col] = None\n",
    "            df = df[schema_cols]\n",
    "            con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Synthetic training sandbox created at: {SYNTHETIC_DB_PATH}\")\n",
    "    print(\"Tables created:\", con.sql(\"SHOW TABLES;\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate the Synthetic Sandbox\n",
    "Let's run a few queries against our new synthetic database to ensure the LLM did a good job generating plausible, interconnected data. We expect to see non-empty results from these joins, which confirms that the referential integrity is holding up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validating the first few tables in the synthetic sandbox ---\n",
      "\n",
      "--- SELECT * FROM airlines LIMIT 10; ---\n",
      "+----+--------------+-------------------+---------+--------+--------+------------+----------------+----------+\n",
      "|    |   airline_id | name              | alias   | iata   | icao   | callsign   | country        | active   |\n",
      "|----+--------------+-------------------+---------+--------+--------+------------+----------------+----------|\n",
      "|  0 |            1 | Sky High Airlines | SHA     | SK     | SKY    | SKYHIGH    | United States  | Y        |\n",
      "|  1 |            2 | Oceanic Airways   | OA      | OC     | OCN    | OCEANIC    | United Kingdom | Y        |\n",
      "|  2 |            3 | Pacific Wings     | PW      | PW     | PWI    | PACWINGS   | Australia      | Y        |\n",
      "|  3 |            4 | Arctic Airlines   | AA      | AR     | ARC    | ARCTIC     | Canada         | Y        |\n",
      "|  4 |            5 | Global Airways    | GA      | GL     | GLO    | GLOBAL     | Germany        | Y        |\n",
      "|  5 |            6 | Nordic Airlines   | NA      | NR     | NRD    | NORDIC     | Sweden         | Y        |\n",
      "|  6 |            7 | Sahara Air        | SA      | SH     | SAH    | SAHARA     | Egypt          | Y        |\n",
      "|  7 |            8 | Alpine Air        | AL      | AP     | ALP    | ALPINE     | Switzerland    | Y        |\n",
      "|  8 |            9 | Eastern Airlines  | EA      | ET     | EAL    | EASTERN    | China          | Y        |\n",
      "|  9 |           10 | Southern Airways  | SA      | SO     | SAW    | SOUTHERN   | Brazil         | Y        |\n",
      "+----+--------------+-------------------+---------+--------+--------+------------+----------------+----------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM airports LIMIT 10; ---\n",
      "+----+--------------+------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+-------------------+---------+-------------+\n",
      "|    |   airport_id | name                   | city      | country        | iata   | icao   |   latitude |   longitude |   altitude |   timezone | dst   | tz_db             | type    | source      |\n",
      "|----+--------------+------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+-------------------+---------+-------------|\n",
      "|  0 |            1 | Sky High International | New York  | United States  | SKY    | SKHI   |    40.7128 |    -74.006  |         13 |         -5 | A     | America/New_York  | airport | OurAirports |\n",
      "|  1 |            2 | Oceanic Gateway        | London    | United Kingdom | OCN    | OCGW   |    51.5074 |     -0.1278 |         35 |          0 | B     | Europe/London     | airport | OurAirports |\n",
      "|  2 |            3 | Pacific Gateway        | Sydney    | Australia      | PWG    | SYPG   |   -33.8688 |    151.209  |         19 |         10 | A     | Australia/Sydney  | airport | OurAirports |\n",
      "|  3 |            4 | Arctic Hub             | Toronto   | Canada         | ARH    | YYZH   |    43.6532 |    -79.3832 |        112 |         -5 | A     | America/Toronto   | airport | OurAirports |\n",
      "|  4 |            5 | Global Hub             | Berlin    | Germany        | GLB    | BERH   |    52.52   |     13.405  |         34 |          1 | E     | Europe/Berlin     | airport | OurAirports |\n",
      "|  5 |            6 | Nordic Gateway         | Stockholm | Sweden         | NRD    | STKG   |    59.3293 |     18.0686 |         47 |          1 | E     | Europe/Stockholm  | airport | OurAirports |\n",
      "|  6 |            7 | Cairo International    | Cairo     | Egypt          | CAI    | HECA   |    30.1219 |     31.4056 |        116 |          2 | E     | Africa/Cairo      | airport | OurAirports |\n",
      "|  7 |            8 | Zurich Airport         | Zurich    | Switzerland    | ZRH    | LSZH   |    47.4647 |      8.5492 |       1416 |          1 | E     | Europe/Zurich     | airport | OurAirports |\n",
      "|  8 |            9 | Eastern Hub            | Shanghai  | China          | ETN    | SHHD   |    31.2304 |    121.474  |         10 |          8 | U     | Asia/Shanghai     | airport | OurAirports |\n",
      "|  9 |           10 | Southern Gateway       | Sao Paulo | Brazil         | SOG    | SBGR   |   -23.5505 |    -46.6333 |        750 |         -3 | S     | America/Sao_Paulo | airport | OurAirports |\n",
      "+----+--------------+------------------------+-----------+----------------+--------+--------+------------+-------------+------------+------------+-------+-------------------+---------+-------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM countries LIMIT 10; ---\n",
      "+----+----------------+------------+--------------+\n",
      "|    | name           | iso_code   | dafif_code   |\n",
      "|----+----------------+------------+--------------|\n",
      "|  0 | United States  | US         | US           |\n",
      "|  1 | United Kingdom | GB         | UK           |\n",
      "|  2 | Australia      | AU         | AS           |\n",
      "|  3 | Canada         | CA         | CA           |\n",
      "|  4 | Germany        | DE         | GM           |\n",
      "|  5 | Sweden         | SE         | SW           |\n",
      "|  6 | Egypt          | EG         | EG           |\n",
      "|  7 | Switzerland    | CH         | SZ           |\n",
      "|  8 | China          | CN         | CH           |\n",
      "|  9 | Brazil         | BR         | BR           |\n",
      "+----+----------------+------------+--------------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM planes LIMIT 10; ---\n",
      "+----+-------------------------+--------+--------+\n",
      "|    | name                    | iata   | icao   |\n",
      "|----+-------------------------+--------+--------|\n",
      "|  0 | Boeing 737              | B737   | B737   |\n",
      "|  1 | Airbus A320             | A320   | A320   |\n",
      "|  2 | Boeing 747              | B747   | B747   |\n",
      "|  3 | Airbus A380             | A380   | A380   |\n",
      "|  4 | Embraer E190            | E90    | E190   |\n",
      "|  5 | Bombardier CRJ900       | CR9    | CRJ9   |\n",
      "|  6 | McDonnell Douglas MD-11 | M11    | MD11   |\n",
      "|  7 | Boeing 777              | B777   | B777   |\n",
      "|  8 | Boeing 787              | B788   | B788   |\n",
      "|  9 | Airbus A350             | A359   | A359   |\n",
      "+----+-------------------------+--------+--------+\n",
      "\n",
      "\n",
      "--- SELECT * FROM routes LIMIT 10; ---\n",
      "+----+-----------+--------------+------------------+---------------------+-----------------------+--------------------------+-------------+---------+-------------+\n",
      "|    | airline   |   airline_id | source_airport   |   source_airport_id | destination_airport   |   destination_airport_id | codeshare   |   stops | equipment   |\n",
      "|----+-----------+--------------+------------------+---------------------+-----------------------+--------------------------+-------------+---------+-------------|\n",
      "|  0 | SK        |            1 | SKY              |                   1 | OCN                   |                        2 |             |       0 | B737        |\n",
      "|  1 | OC        |            2 | OCN              |                   2 | SKY                   |                        1 |             |       0 | A320        |\n",
      "|  2 | PW        |            3 | PWG              |                   3 | ARH                   |                        4 |             |       0 | B747        |\n",
      "|  3 | AR        |            4 | ARH              |                   4 | PWG                   |                        3 |             |       0 | A380        |\n",
      "|  4 | GL        |            5 | GLB              |                   5 | NRD                   |                        6 |             |       0 | E190        |\n",
      "|  5 | NR        |            6 | NRD              |                   6 | GLB                   |                        5 |             |       0 | CRJ9        |\n",
      "|  6 | SH        |            7 | CAI              |                   7 | ZRH                   |                        8 |             |       0 | MD11        |\n",
      "|  7 | AP        |            8 | ZRH              |                   8 | CAI                   |                        7 |             |       0 | B777        |\n",
      "|  8 | ET        |            9 | ETN              |                   9 | SOG                   |                       10 |             |       0 | B788        |\n",
      "|  9 | SO        |           10 | SOG              |                  10 | ETN                   |                        9 |             |       0 | A359        |\n",
      "+----+-----------+--------------+------------------+---------------------+-----------------------+--------------------------+-------------+---------+-------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Connect to the synthetic database\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    \n",
    "    # Get the list of all tables created\n",
    "    all_tables = [table[0] for table in con.sql(\"SHOW TABLES;\").fetchall()]\n",
    "    \n",
    "    # Select the first 5 tables to display (or all if fewer than 5)\n",
    "    tables_to_validate = all_tables[:5]\n",
    "\n",
    "    print(\"--- Validating the first few tables in the synthetic sandbox ---\\n\")\n",
    "\n",
    "    # Execute and print results for the selected tables\n",
    "    for table_name in tables_to_validate:\n",
    "        print(f\"--- SELECT * FROM {table_name} LIMIT 10; ---\")\n",
    "        try:\n",
    "            result_df = con.sql(f\"SELECT * FROM {table_name} LIMIT 10;\").df()\n",
    "            if not result_df.empty:\n",
    "                print(tabulate(result_df, headers='keys', tablefmt='psql'))\n",
    "            else:\n",
    "                print(f\"(Table '{table_name}' is empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed for table '{table_name}': {e}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate Example SQL Queries\n",
    "With our synthetic database in place, the next step is to create a set of synthetic SQL queries. These SQL queries will be executed against our database of synthetic data to get the ground truth labels for RFT. Furthermore, these same SQL queries will be used as input to an LLM to generate queries in natural language. This will enable us to form our final RFT dataset, which pairs natural language queries with ground truth results from the database.\n",
    "\n",
    "> **Real World üåç**: You would use a historical log of real SQL queries that have been run against your production database. These logs are the most valuable source of training data because they represent the *actual* way your users query your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Goal: Generate 20 unique queries in batches of 10.\n",
      "\n",
      "üìû --- Generating batch #1 ---\n",
      "   - Received 10 new queries. Total now: 10 / 20\n",
      "\n",
      "üìû --- Generating batch #2 ---\n",
      "   - Received 10 new queries. Total now: 20 / 20\n",
      "\n",
      "‚ú® Generation complete. Deduplicating and saving...\n",
      " - Removed 0 duplicates (20 -> 20).\n",
      "\n",
      "‚úÖ Successfully saved 20 unique queries to `data/generated_queries.json`.\n",
      "\n",
      "--- Here are a few examples: ---\n",
      "- SELECT a.name AS airline_name, COUNT(r.route_id) AS num_routes FROM airlines a LEFT JOIN routes r ON a.airline_id = r.airline_id GROUP BY a.name ORDER BY num_routes DESC LIMIT 10;\n",
      "- SELECT c.name AS country, COUNT(a.airport_id) AS num_airports FROM countries c LEFT JOIN airports a ON c.iso_code = a.country GROUP BY c.name ORDER BY num_airports DESC;\n",
      "- SELECT p.name AS plane, COUNT(r.route_id) AS num_routes FROM planes p LEFT JOIN routes r ON p.iata = r.equipment GROUP BY p.name ORDER BY num_routes DESC;\n",
      "- SELECT a.city, COUNT(*) AS num_airports FROM airports a GROUP BY a.city HAVING COUNT(*) > 1 ORDER BY num_airports DESC;\n",
      "- SELECT a.country, AVG(a.altitude) AS avg_altitude FROM airports a GROUP BY a.country ORDER BY avg_altitude DESC LIMIT 5;\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Define Generation Parameters and Pydantic Model ---\n",
    "TOTAL_QUERIES_TO_GENERATE = 20\n",
    "QUERIES_PER_API_CALL = 10\n",
    "\n",
    "class SqlQueryBatch(BaseModel):\n",
    "    queries: List[str] = Field(description=f\"A list of exactly {QUERIES_PER_API_CALL} unique and diverse SQL queries.\")\n",
    "\n",
    "print(f\"üéØ Goal: Generate {TOTAL_QUERIES_TO_GENERATE} unique queries in batches of {QUERIES_PER_API_CALL}.\")\n",
    "\n",
    "# --- 2. Setup Base Prompt and Generation Loop ---\n",
    "base_query_generation_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to generate unique and diverse SQL queries based on the database schema provided.\n",
    "The queries should be realistic and cover a range of complexities and SQL features (JOINS, GROUP BY, aggregates, etc.).\n",
    "Ensure the generated SQL is valid for DuckDB.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "all_generated_queries = []\n",
    "# Loop until we have enough queries\n",
    "while len(all_generated_queries) < TOTAL_QUERIES_TO_GENERATE:\n",
    "    print(f\"\\nüìû --- Generating batch #{len(all_generated_queries) // QUERIES_PER_API_CALL + 1} ---\")\n",
    "\n",
    "    # Create a summary of queries generated so far to prevent duplicates\n",
    "    existing_queries_summary = \"\"\n",
    "    if all_generated_queries:\n",
    "        summary_parts = [\"\\nYou have already generated the following queries. Generate NEW, DISTINCT queries that are not on this list and cover different analytic scenarios.\\n\"]\n",
    "        for i, q in enumerate(all_generated_queries):\n",
    "            summary_parts.append(f\"{i+1}. {q}\")\n",
    "        existing_queries_summary = \"\\n\".join(summary_parts)\n",
    "\n",
    "    # Construct the final prompt for this iteration\n",
    "    final_prompt = (\n",
    "        base_query_generation_prompt +\n",
    "        existing_queries_summary +\n",
    "        f\"\\n\\nNow, generate {QUERIES_PER_API_CALL} new and unique SQL queries. Return your response as a single JSON object adhering to the specified schema.\"\n",
    "    )\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"SqlQueryBatch\", \"schema\": SqlQueryBatch.model_json_schema()}},\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    if response_content:\n",
    "        try:\n",
    "            new_queries = json.loads(response_content).get(\"queries\", [])\n",
    "            all_generated_queries.extend(new_queries)\n",
    "            print(f\"   - Received {len(new_queries)} new queries. Total now: {len(all_generated_queries)} / {TOTAL_QUERIES_TO_GENERATE}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå ERROR: Failed to parse generated queries in this batch: {e}\")\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 3. Deduplicate, Trim, and Save --- \n",
    "print(\"\\n‚ú® Generation complete. Deduplicating and saving...\")\n",
    "initial_count = len(all_generated_queries)\n",
    "# Simple, fast deduplication preserving order\n",
    "unique_queries = list(dict.fromkeys(all_generated_queries))\n",
    "final_count = len(unique_queries)\n",
    "print(f\" - Removed {initial_count - final_count} duplicates ({initial_count} -> {final_count}).\")\n",
    "\n",
    "# Trim to the exact number we need\n",
    "final_queries = unique_queries[:TOTAL_QUERIES_TO_GENERATE]\n",
    "\n",
    "# Save the final list to a file\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "with open(QUERIES_FILE_PATH, 'w') as f:\n",
    "    json.dump({\"queries\": final_queries}, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {len(final_queries)} unique queries to `{QUERIES_FILE_PATH}`.\")\n",
    "print(\"\\n--- Here are a few examples: ---\")\n",
    "for query in final_queries[:5]:\n",
    "    print(f\"- {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Execute Queries to Get Ground-Truth Answers\n",
    "Now we will act as the \"system\" and run the queries we just generated against our synthetic sandbox. The output of each query is the **ground-truth result**. During Reinforcement Fine-Tuning, our model will be rewarded if the SQL it writes produces this exact same result.\n",
    "\n",
    "> **Real World üåç**: You would run your real historical queries against the synthetic database we previously created. The correctness of the data is not a concern here, as our aim is to see what a correct query would have generated, so we can compare it to our LLM's generations during the RFT process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 queries to execute.\n",
      "Executing queries against the synthetic database...\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT a.name AS airline_name, COUNT(r.route_id) AS num_routes FROM airlines a LEFT JOIN routes r ON a.airline_id = r.airline_id GROUP BY a.name ORDER BY num_routes DESC LIMIT 10;\n",
      "   Error: Binder Error: Table \"r\" does not have a column named \"route_id\"\n",
      "\n",
      "Candidate bindings: : \"source_airport\"\n",
      "\n",
      "‚ö†Ô∏è  Skipping query due to execution error: SELECT p.name AS plane, COUNT(r.route_id) AS num_routes FROM planes p LEFT JOIN routes r ON p.iata = r.equipment GROUP BY p.name ORDER BY num_routes DESC;\n",
      "   Error: Binder Error: Table \"r\" does not have a column named \"route_id\"\n",
      "\n",
      "Candidate bindings: : \"source_airport\"\n",
      "\n",
      "\n",
      "Execution complete. Success: 18, Failed: 2.\n",
      "\n",
      "‚úÖ Successfully saved 18 ground-truth results to `data/ground_truth_results.jsonl`.\n",
      "\n",
      "--- Example ground-truth entry ---\n",
      "{\n",
      "  \"query\": \"SELECT c.name AS country, COUNT(a.airport_id) AS num_airports FROM countries c LEFT JOIN airports a ON c.iso_code = a.country GROUP BY c.name ORDER BY num_airports DESC;\",\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"country\": \"New Zealand\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ecuador\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Norway\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"South Africa\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Kazakhstan\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Kingdom\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Arab Emirates\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Japan\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Spain\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Russia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Mexico\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Costa Rica\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Netherlands\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Belgium\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Australia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Brazil\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Italy\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Finland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"France\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Turkey\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United States\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Saudi Arabia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Germany\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ireland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Portugal\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Thailand\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Sweden\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Egypt\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Jamaica\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"China\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Indonesia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Canada\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Iceland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Greece\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Switzerland\",\n",
      "      \"num_airports\": 0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "SYNTHETIC_DB_PATH = \"data/synthetic_openflights.db\"\n",
    "QUERIES_FILE_PATH = \"data/generated_queries.json\"\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "\n",
    "# --- 2. Load Generated Queries ---\n",
    "with open(QUERIES_FILE_PATH, 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "    queries_to_execute = queries_data.get(\"queries\", [])\n",
    "\n",
    "print(f\"Loaded {len(queries_to_execute)} queries to execute.\")\n",
    "\n",
    "# --- 3. Execute Queries and Store Results ---\n",
    "ground_truth_results = []\n",
    "successful_executions = 0\n",
    "failed_executions = 0\n",
    "\n",
    "print(\"Executing queries against the synthetic database...\")\n",
    "with duckdb.connect(SYNTHETIC_DB_PATH, read_only=True) as con:\n",
    "    for query in queries_to_execute:\n",
    "        try:\n",
    "            # Execute the query and convert the result to a list of dictionaries\n",
    "            result_df = con.sql(query).df()\n",
    "            result_records = result_df.to_dict('records')\n",
    "            \n",
    "            # Pair the query with its result\n",
    "            ground_truth_results.append({\n",
    "                \"query\": query,\n",
    "                \"result\": result_records\n",
    "            })\n",
    "            successful_executions += 1\n",
    "        except Exception as e:\n",
    "            # The LLM might have occasionally generated a slightly invalid query (this should not apply in the case of a real-world sample of successful queries)\n",
    "            print(f\"‚ö†Ô∏è  Skipping query due to execution error: {query}\\n   Error: {e}\\n\")\n",
    "            failed_executions += 1\n",
    "\n",
    "print(f\"\\nExecution complete. Success: {successful_executions}, Failed: {failed_executions}.\")\n",
    "\n",
    "# --- 4. Save the Ground-Truth Data ---\n",
    "with open(GROUND_TRUTH_FILE_PATH, 'w') as f:\n",
    "    for entry in ground_truth_results:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {len(ground_truth_results)} ground-truth results to `{GROUND_TRUTH_FILE_PATH}`.\")\n",
    "\n",
    "# --- 5. Print an Example ---\n",
    "if ground_truth_results:\n",
    "    print(\"\\n--- Example ground-truth entry ---\")\n",
    "    print(json.dumps(ground_truth_results[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Natural Language Questions for Final RFT Training Data\n",
    "We now have pairs of `(SQL Query, Ground-Truth Result)`. The final piece missing from our training data is the user's input: a question in natural language. This is because our final goal is to use RFT to tune an LLM to map from a natural language question to a SQL query, having the reward signal be the actual result of the query, rather than just the query itself. This is important because there are many ways to write the same SQL query that yield the same, correct result.\n",
    "\n",
    "Thus, we will use an LLM once again to translate our \"historical\" SQL queries into plausible questions a business user might ask, corresponding to that query. This will yield our final training dataset in the format: `(Natural Language Question, Ground-Truth Result)`.\n",
    "\n",
    "> **Real World üåç**: You might not need this step! If you have logs that already link user questions to the queries they ran (e.g., from a BI tool's search bar), you can use those directly. If not, this LLM-based translation is a powerful technique to bootstrap your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 query-result pairs.\n",
      "Generating natural language questions and formatting for RFT for 18 queries...\n",
      " - Processing query 1/18...\n",
      " - Processing query 2/18...\n",
      " - Processing query 3/18...\n",
      " - Processing query 4/18...\n",
      " - Processing query 5/18...\n",
      " - Processing query 6/18...\n",
      " - Processing query 7/18...\n",
      " - Processing query 8/18...\n",
      " - Processing query 9/18...\n",
      " - Processing query 10/18...\n",
      " - Processing query 11/18...\n",
      " - Processing query 12/18...\n",
      " - Processing query 13/18...\n",
      " - Processing query 14/18...\n",
      " - Processing query 15/18...\n",
      " - Processing query 16/18...\n",
      " - Processing query 17/18...\n",
      " - Processing query 18/18...\n",
      "\n",
      "‚úÖ Successfully created final RFT training dataset with 18 entries at `data/training_data.jsonl`.\n",
      "\n",
      "--- Example RFT training entry ---\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nYou are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\\n\\n**Database Schema:**\\n| database         | schema   | name      | column_names                                                               | column_types                                                          | temporary   |\\n|:-----------------|:---------|:----------|:---------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------|\\n| prod_openflights | main     | airlines  | ['airline_id' 'name' 'alias' 'iata' 'icao' 'callsign' 'country' 'active']  | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' | False       |\\n|                  |          |           |                                                                            |  'VARCHAR']                                                           |             |\\n| prod_openflights | main     | airports  | ['airport_id' 'name' 'city' 'country' 'iata' 'icao' 'latitude' 'longitude' | ['BIGINT' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'DOUBLE'  | False       |\\n|                  |          |           |  'altitude' 'timezone' 'dst' 'tz_db' 'type' 'source']                      |  'DOUBLE' 'BIGINT' 'DOUBLE' 'VARCHAR' 'VARCHAR' 'VARCHAR' 'VARCHAR']  |             |\\n| prod_openflights | main     | countries | ['name' 'iso_code' 'dafif_code']                                           | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| prod_openflights | main     | planes    | ['name' 'iata' 'icao']                                                     | ['VARCHAR' 'VARCHAR' 'VARCHAR']                                       | False       |\\n| prod_openflights | main     | routes    | ['airline' 'airline_id' 'source_airport' 'source_airport_id'               | ['VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR' 'DOUBLE' 'VARCHAR'   | False       |\\n|                  |          |           |  'destination_airport' 'destination_airport_id' 'codeshare' 'stops'        |  'BIGINT' 'VARCHAR']                                                  |             |\\n|                  |          |           |  'equipment']                                                              |                                                                       |             |\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"\\\"Which countries have the most airports, and how many airports does each of those countries have?\\\"\"\n",
      "    }\n",
      "  ],\n",
      "  \"ground_truth\": [\n",
      "    {\n",
      "      \"country\": \"New Zealand\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ecuador\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Norway\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"South Africa\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Kazakhstan\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Kingdom\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United Arab Emirates\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Japan\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Spain\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Russia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Mexico\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Costa Rica\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Netherlands\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Belgium\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Australia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Brazil\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Italy\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Finland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"France\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Turkey\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United States\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Saudi Arabia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Germany\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Ireland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Portugal\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Thailand\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Sweden\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Egypt\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Jamaica\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"China\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Indonesia\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Canada\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Iceland\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Greece\",\n",
      "      \"num_airports\": 0\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Switzerland\",\n",
      "      \"num_airports\": 0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import jsonlines\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Define File Paths and Parameters ---\n",
    "GROUND_TRUTH_FILE_PATH = \"data/ground_truth_results.jsonl\"\n",
    "FINAL_TRAINING_DATA_PATH = \"data/training_data.jsonl\"\n",
    "\n",
    "# --- 2. Load Ground-Truth Data ---\n",
    "query_result_pairs = []\n",
    "with jsonlines.open(GROUND_TRUTH_FILE_PATH) as reader:\n",
    "    for obj in reader:\n",
    "        query_result_pairs.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(query_result_pairs)} query-result pairs.\")\n",
    "\n",
    "# --- 3. Use LLM to Generate Natural Language Questions ---\n",
    "nl_generation_prompt_template = f\"\"\"\n",
    "You are an expert data analyst who is great at translating SQL queries into plain English.\n",
    "Based on the database schema and the provided SQL query, what is a natural language question a business user would ask to get this information?\n",
    "Ensure that the question is precise enough to accurately map to the corresponding SQL query.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\n",
    "**SQL Query:**\n",
    "```sql\n",
    "{{query}}\n",
    "```\n",
    "\n",
    "Provide only the user's question, without any preamble or explanation.\n",
    "\"\"\"\n",
    "\n",
    "# The system prompt that will be included in the final training data for the RFT job.\n",
    "# It gives the model its instructions at inference time.\n",
    "rft_system_prompt = f\"\"\"\n",
    "You are an expert SQL data analyst. Your task is to write a single, valid DuckDB SQL query to answer the user's question, based on the provided database schema. Do not provide any explanation or text other than the SQL query itself.\n",
    "\n",
    "**Database Schema:**\n",
    "{schema_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "final_training_data = []\n",
    "print(f\"Generating natural language questions and formatting for RFT for {len(query_result_pairs)} queries...\")\n",
    "\n",
    "for i, pair in enumerate(query_result_pairs):\n",
    "    print(f\" - Processing query {i+1}/{len(query_result_pairs)}...\")\n",
    "    nl_generation_prompt = nl_generation_prompt_template.format(query=pair['query'])\n",
    "    \n",
    "    response = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": nl_generation_prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    nl_question = response.choices[0].message.content\n",
    "    if nl_question:\n",
    "        # Assemble the final data structure\n",
    "        rft_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": rft_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": nl_question.strip()}\n",
    "            ],\n",
    "            \"ground_truth\": pair['result'] # The ground-truth result for the evaluator\n",
    "        }\n",
    "        final_training_data.append(rft_entry)\n",
    "    \n",
    "    time.sleep(1) # Be nice to the API\n",
    "\n",
    "# --- 4. Save the Final RFT-Ready Dataset ---\n",
    "with jsonlines.open(FINAL_TRAINING_DATA_PATH, mode='w') as writer:\n",
    "    writer.write_all(final_training_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully created final RFT training dataset with {len(final_training_data)} entries at `{FINAL_TRAINING_DATA_PATH}`.\")\n",
    "\n",
    "# --- 5. Print an Example ---\n",
    "if final_training_data:\n",
    "    print(\"\\n--- Example RFT training entry ---\")\n",
    "    print(json.dumps(final_training_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. üõ∞Ô∏è Deploy an MCP Server for the Synthetic Data\n",
    "Now, we'll start a remote server that speaks the Model Context Protocol (MCP). This server will wrap our synthetic DuckDB database, providing a standardized way for any external tool‚Äîin our case, the Fireworks RFT evaluator‚Äîto interact with it.\n",
    "> Real World üåç: This pattern is directly applicable. You would run a similar MCP server to provide a secure, read-only interface to a production database replica or a data warehouse, allowing the fine-tuning process to happen without granting direct database credentials to the training environment.\n",
    "\n",
    "10. a) Create a server script in this project's root directory (`run_mcp_server.py`). This Python script starts our database server. It is configured to be read-only.\n",
    "\n",
    "```python\n",
    "    import os, contextlib, uvicorn\n",
    "    from starlette.applications import Starlette\n",
    "    from starlette.routing import Mount\n",
    "    from mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n",
    "    from mcp_server_motherduck import build_application\n",
    "\n",
    "    DB = \"data/synthetic_openflights.db\"          # ‚Üê path from previous steps\n",
    "    PORT = int(os.environ.get(\"PORT\", 8080))        # Cloud Run injects $PORT\n",
    "\n",
    "    # 1Ô∏è‚É£ Build the core SQL-aware MCP server (read-only for safety).\n",
    "    server, _ = build_application(db_path=DB, read_only=True)\n",
    "\n",
    "    # 2Ô∏è‚É£ Wrap it so HTTP clients can talk to it (ASGI handler).\n",
    "    sess = StreamableHTTPSessionManager(app=server, event_store=None, stateless=True)\n",
    "\n",
    "    async def handler(scope, receive, send):\n",
    "        await sess.handle_request(scope, receive, send)\n",
    "\n",
    "    @contextlib.asynccontextmanager\n",
    "    async def lifespan(app):\n",
    "        async with sess.run():\n",
    "            yield                                        # keep sessions alive\n",
    "\n",
    "    # 3Ô∏è‚É£ Starlette turns that handler into a full ASGI app Uvicorn can serve.\n",
    "    app = Starlette(routes=[Mount(\"/mcp\", app=handler)], lifespan=lifespan)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        print(f\"üî• MCP endpoint ‚Üí http://0.0.0.0:{PORT}/mcp\")\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. ‚òÅÔ∏è Set Up Google Cloud CLI\n",
    "We'll first set up the Google Cloud CLI and authenticate.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You would follow along here in the same way\n",
    "\n",
    "9. a) **Install** the SDK (macOS/Linux):\n",
    "\n",
    "      ```bash\n",
    "      curl -sSL https://sdk.cloud.google.com | bash\n",
    "      exec -l $SHELL  # reload shell so 'gcloud' is available\n",
    "      ```\n",
    "\n",
    "9. b) **Log in** (creates local access token)\n",
    "      ```bash\n",
    "      gcloud auth login\n",
    "      ```\n",
    "\n",
    "9. c) **Set your active project desired gcloud project**\n",
    "      ```bash\n",
    "      gcloud config set project < YOUR_PROJECT_ID >  # set up project in gcloud console before running this if not already done\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. üì¶ Containerize & Deploy the MCP Server  \n",
    "We‚Äôll build a Docker image and push it straight to Cloud Run.  \n",
    "Remember to replace **`YOUR_PROJECT_ID`** with the project you actually want to bill.\n",
    "\n",
    "> **Real World üåç**  \n",
    "> You would follow along in the same way here.\n",
    "\n",
    "10. a) Create `mcp_requirements.txt` containing the following:\n",
    "\n",
    "```bash\n",
    "        mcp\n",
    "        mcp-server-motherduck\n",
    "        duckdb\n",
    "        uvicorn\n",
    "        starlette\n",
    "```\n",
    "\n",
    "10. b) Create a `Dockerfile` (no extension) containing the following\n",
    "```bash\n",
    "        base\n",
    "        FROM python:3.11-slim\n",
    "        WORKDIR /app\n",
    "\n",
    "        COPY mcp_requirements.txt .\n",
    "        RUN pip install --no-cache-dir -r mcp_requirements.txt\n",
    "\n",
    "        COPY run_mcp_server.py .\n",
    "        COPY data/synthetic_openflights.db ./data/\n",
    "\n",
    "        EXPOSE 8080\n",
    "\n",
    "        CMD [\"python\", \"run_mcp_server.py\"]\n",
    "```\n",
    "\n",
    "10. c) Deploy your MCP server as a Cloud Run app by running (from your project root):\n",
    "```bash\n",
    "        reward-kit deploy-mcp \\\n",
    "        --id mcp-sql-rft-server \\\n",
    "        --dockerfile Dockerfile \\\n",
    "        --port 8080\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
